# Comparing `tmp/pytorch_ood-0.1.3-py3-none-any.whl.zip` & `tmp/pytorch_ood-0.1.4-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,76 +1,78 @@
-Zip file size: 105775 bytes, number of entries: 74
--rw-r--r--  2.0 unx      209 b- defN 23-Jul-07 08:32 pytorch_ood/__init__.py
--rw-r--r--  2.0 unx     2341 b- defN 23-Jun-29 07:52 pytorch_ood/api.py
--rw-r--r--  2.0 unx     1005 b- defN 23-Jul-06 08:56 pytorch_ood/benchmark/__init__.py
--rw-r--r--  2.0 unx      876 b- defN 23-Jul-06 08:56 pytorch_ood/benchmark/base.py
--rw-r--r--  2.0 unx       70 b- defN 23-Jul-06 08:56 pytorch_ood/benchmark/img/__init__.py
--rw-r--r--  2.0 unx     3573 b- defN 23-Jul-06 08:56 pytorch_ood/benchmark/img/cifar10.py
--rw-r--r--  2.0 unx     3578 b- defN 23-Jul-06 08:56 pytorch_ood/benchmark/img/cifar100.py
--rw-r--r--  2.0 unx       11 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/__init__.py
--rw-r--r--  2.0 unx      165 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/audio/__init__.py
--rw-r--r--  2.0 unx     3059 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/audio/fsdd.py
--rw-r--r--  2.0 unx     3141 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/__init__.py
--rw-r--r--  2.0 unx     2363 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/base.py
--rw-r--r--  2.0 unx     4329 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/chars74k.py
--rw-r--r--  2.0 unx     3076 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/cifar.py
--rw-r--r--  2.0 unx     1331 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/fooling.py
--rw-r--r--  2.0 unx     5793 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/imagenet.py
--rw-r--r--  2.0 unx     4270 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/mnistc.py
--rw-r--r--  2.0 unx     5382 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/mvtech.py
--rw-r--r--  2.0 unx     3936 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/noise.py
--rw-r--r--  2.0 unx     4891 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/odin.py
--rw-r--r--  2.0 unx     8477 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/pixmix.py
--rw-r--r--  2.0 unx     4242 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/streethazards.py
--rw-r--r--  2.0 unx     3001 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/textures.py
--rw-r--r--  2.0 unx     2641 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/tinyimagenet.py
--rw-r--r--  2.0 unx     4692 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/img/tinyimages.py
--rw-r--r--  2.0 unx      507 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/ossim/__init__.py
--rw-r--r--  2.0 unx     8194 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/ossim/ossim.py
--rw-r--r--  2.0 unx     1049 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/txt/__init__.py
--rw-r--r--  2.0 unx     2665 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/txt/multi30k.py
--rw-r--r--  2.0 unx     4002 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/txt/newsgroups.py
--rw-r--r--  2.0 unx     5954 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/txt/reuters.py
--rw-r--r--  2.0 unx     1983 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/txt/stop_words.py
--rw-r--r--  2.0 unx     3331 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/txt/wiki.py
--rw-r--r--  2.0 unx     2034 b- defN 23-Jun-29 07:52 pytorch_ood/dataset/txt/wmt16.py
--rw-r--r--  2.0 unx     2395 b- defN 23-Jun-29 07:52 pytorch_ood/detector/__init__.py
--rw-r--r--  2.0 unx     2352 b- defN 23-Jun-29 07:52 pytorch_ood/detector/energy.py
--rw-r--r--  2.0 unx     2046 b- defN 23-Jun-29 07:52 pytorch_ood/detector/entropy.py
--rw-r--r--  2.0 unx     4402 b- defN 23-Jun-29 07:52 pytorch_ood/detector/klmatching.py
--rw-r--r--  2.0 unx     7749 b- defN 23-Jun-29 07:52 pytorch_ood/detector/mahalanobis.py
--rw-r--r--  2.0 unx     1810 b- defN 23-Jun-29 07:52 pytorch_ood/detector/maxlogit.py
--rw-r--r--  2.0 unx     3619 b- defN 23-Jun-29 07:52 pytorch_ood/detector/mcd.py
--rw-r--r--  2.0 unx     5816 b- defN 23-Jun-29 07:52 pytorch_ood/detector/odin.py
--rw-r--r--  2.0 unx     2316 b- defN 23-Jun-29 07:52 pytorch_ood/detector/softmax.py
--rw-r--r--  2.0 unx     5267 b- defN 23-Jun-29 07:52 pytorch_ood/detector/vim.py
--rw-r--r--  2.0 unx      323 b- defN 23-Jun-29 07:52 pytorch_ood/detector/openmax/__init__.py
--rw-r--r--  2.0 unx     7041 b- defN 23-Jun-29 07:52 pytorch_ood/detector/openmax/numpy.py
--rw-r--r--  2.0 unx     3318 b- defN 23-Jun-29 07:52 pytorch_ood/detector/openmax/torch.py
--rw-r--r--  2.0 unx     5408 b- defN 23-Jun-29 07:52 pytorch_ood/loss/__init__.py
--rw-r--r--  2.0 unx     1589 b- defN 23-Jun-29 07:52 pytorch_ood/loss/background.py
--rw-r--r--  2.0 unx     3877 b- defN 23-Jun-29 07:52 pytorch_ood/loss/cac.py
--rw-r--r--  2.0 unx     4014 b- defN 23-Jun-29 07:52 pytorch_ood/loss/center.py
--rw-r--r--  2.0 unx     2378 b- defN 23-Jun-29 07:52 pytorch_ood/loss/conf.py
--rw-r--r--  2.0 unx     1188 b- defN 23-Jun-29 07:52 pytorch_ood/loss/crossentropy.py
--rw-r--r--  2.0 unx     2565 b- defN 23-Jun-29 07:52 pytorch_ood/loss/energy.py
--rw-r--r--  2.0 unx     2741 b- defN 23-Jun-29 07:52 pytorch_ood/loss/entropy.py
--rw-r--r--  2.0 unx     4599 b- defN 23-Jun-29 07:52 pytorch_ood/loss/ii.py
--rw-r--r--  2.0 unx     4875 b- defN 23-Jun-29 07:52 pytorch_ood/loss/mchad.py
--rw-r--r--  2.0 unx     2667 b- defN 23-Jun-29 07:52 pytorch_ood/loss/objectosphere.py
--rw-r--r--  2.0 unx     2199 b- defN 23-Jun-29 07:52 pytorch_ood/loss/oe.py
--rw-r--r--  2.0 unx     5289 b- defN 23-Jun-29 07:52 pytorch_ood/loss/svdd.py
--rw-r--r--  2.0 unx      723 b- defN 23-Jun-29 07:52 pytorch_ood/model/__init__.py
--rw-r--r--  2.0 unx     4684 b- defN 23-Jun-29 07:52 pytorch_ood/model/centers.py
--rw-r--r--  2.0 unx     1611 b- defN 23-Jul-06 08:56 pytorch_ood/model/gru.py
--rw-r--r--  2.0 unx    11843 b- defN 23-Jul-06 08:56 pytorch_ood/model/wrn.py
--rw-r--r--  2.0 unx       70 b- defN 23-Jun-29 07:52 pytorch_ood/utils/__init__.py
--rw-r--r--  2.0 unx    10960 b- defN 23-Jun-29 07:52 pytorch_ood/utils/gdown.py
--rw-r--r--  2.0 unx     7093 b- defN 23-Jun-29 07:52 pytorch_ood/utils/metrics.py
--rw-r--r--  2.0 unx     2111 b- defN 23-Jun-29 07:52 pytorch_ood/utils/transforms.py
--rw-r--r--  2.0 unx     8878 b- defN 23-Jun-29 07:52 pytorch_ood/utils/utils.py
--rw-r--r--  2.0 unx    11351 b- defN 23-Jul-07 08:34 pytorch_ood-0.1.3.dist-info/LICENSE
--rw-r--r--  2.0 unx    23666 b- defN 23-Jul-07 08:34 pytorch_ood-0.1.3.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jul-07 08:34 pytorch_ood-0.1.3.dist-info/WHEEL
--rw-r--r--  2.0 unx       12 b- defN 23-Jul-07 08:34 pytorch_ood-0.1.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6480 b- defN 23-Jul-07 08:34 pytorch_ood-0.1.3.dist-info/RECORD
-74 files, 287588 bytes uncompressed, 95473 bytes compressed:  66.8%
+Zip file size: 110404 bytes, number of entries: 76
+-rw-r--r--  2.0 unx      209 b- defN 23-Jul-19 08:13 pytorch_ood/__init__.py
+-rw-r--r--  2.0 unx     2341 b- defN 23-Jul-18 10:04 pytorch_ood/api.py
+-rw-r--r--  2.0 unx     1404 b- defN 23-Jul-19 08:12 pytorch_ood/benchmark/__init__.py
+-rw-r--r--  2.0 unx      876 b- defN 23-Jul-18 10:04 pytorch_ood/benchmark/base.py
+-rw-r--r--  2.0 unx      144 b- defN 23-Jul-19 08:12 pytorch_ood/benchmark/img/__init__.py
+-rw-r--r--  2.0 unx     7122 b- defN 23-Jul-19 08:12 pytorch_ood/benchmark/img/cifar10.py
+-rw-r--r--  2.0 unx     7157 b- defN 23-Jul-19 08:12 pytorch_ood/benchmark/img/cifar100.py
+-rw-r--r--  2.0 unx     3922 b- defN 23-Jul-19 08:12 pytorch_ood/benchmark/img/imagenet.py
+-rw-r--r--  2.0 unx       11 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/__init__.py
+-rw-r--r--  2.0 unx      165 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/audio/__init__.py
+-rw-r--r--  2.0 unx     3059 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/audio/fsdd.py
+-rw-r--r--  2.0 unx     3487 b- defN 23-Jul-19 08:12 pytorch_ood/dataset/img/__init__.py
+-rw-r--r--  2.0 unx     2457 b- defN 23-Jul-19 08:12 pytorch_ood/dataset/img/base.py
+-rw-r--r--  2.0 unx     4329 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/chars74k.py
+-rw-r--r--  2.0 unx     3076 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/cifar.py
+-rw-r--r--  2.0 unx     1331 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/fooling.py
+-rw-r--r--  2.0 unx     5793 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/imagenet.py
+-rw-r--r--  2.0 unx     4270 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/mnistc.py
+-rw-r--r--  2.0 unx     5382 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/mvtech.py
+-rw-r--r--  2.0 unx     3936 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/noise.py
+-rw-r--r--  2.0 unx     4891 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/odin.py
+-rw-r--r--  2.0 unx     4836 b- defN 23-Jul-19 08:12 pytorch_ood/dataset/img/openood.py
+-rw-r--r--  2.0 unx     8477 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/pixmix.py
+-rw-r--r--  2.0 unx     4242 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/streethazards.py
+-rw-r--r--  2.0 unx     3001 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/textures.py
+-rw-r--r--  2.0 unx     3766 b- defN 23-Jul-19 08:12 pytorch_ood/dataset/img/tinyimagenet.py
+-rw-r--r--  2.0 unx     4692 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/img/tinyimages.py
+-rw-r--r--  2.0 unx      507 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/ossim/__init__.py
+-rw-r--r--  2.0 unx     8194 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/ossim/ossim.py
+-rw-r--r--  2.0 unx     1049 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/__init__.py
+-rw-r--r--  2.0 unx     2665 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/multi30k.py
+-rw-r--r--  2.0 unx     4002 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/newsgroups.py
+-rw-r--r--  2.0 unx     5954 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/reuters.py
+-rw-r--r--  2.0 unx     1983 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/stop_words.py
+-rw-r--r--  2.0 unx     3331 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/wiki.py
+-rw-r--r--  2.0 unx     2034 b- defN 23-Jul-18 10:04 pytorch_ood/dataset/txt/wmt16.py
+-rw-r--r--  2.0 unx     2395 b- defN 23-Jul-18 10:04 pytorch_ood/detector/__init__.py
+-rw-r--r--  2.0 unx     2352 b- defN 23-Jul-18 10:04 pytorch_ood/detector/energy.py
+-rw-r--r--  2.0 unx     2046 b- defN 23-Jul-18 10:04 pytorch_ood/detector/entropy.py
+-rw-r--r--  2.0 unx     4402 b- defN 23-Jul-18 10:04 pytorch_ood/detector/klmatching.py
+-rw-r--r--  2.0 unx     7749 b- defN 23-Jul-18 10:04 pytorch_ood/detector/mahalanobis.py
+-rw-r--r--  2.0 unx     1810 b- defN 23-Jul-18 10:04 pytorch_ood/detector/maxlogit.py
+-rw-r--r--  2.0 unx     3619 b- defN 23-Jul-18 10:04 pytorch_ood/detector/mcd.py
+-rw-r--r--  2.0 unx     5816 b- defN 23-Jul-18 10:04 pytorch_ood/detector/odin.py
+-rw-r--r--  2.0 unx     2316 b- defN 23-Jul-18 10:04 pytorch_ood/detector/softmax.py
+-rw-r--r--  2.0 unx     5267 b- defN 23-Jul-18 10:04 pytorch_ood/detector/vim.py
+-rw-r--r--  2.0 unx      323 b- defN 23-Jul-18 10:04 pytorch_ood/detector/openmax/__init__.py
+-rw-r--r--  2.0 unx     7041 b- defN 23-Jul-18 10:04 pytorch_ood/detector/openmax/numpy.py
+-rw-r--r--  2.0 unx     3318 b- defN 23-Jul-18 10:04 pytorch_ood/detector/openmax/torch.py
+-rw-r--r--  2.0 unx     5408 b- defN 23-Jul-18 10:04 pytorch_ood/loss/__init__.py
+-rw-r--r--  2.0 unx     1589 b- defN 23-Jul-18 10:04 pytorch_ood/loss/background.py
+-rw-r--r--  2.0 unx     3877 b- defN 23-Jul-18 10:04 pytorch_ood/loss/cac.py
+-rw-r--r--  2.0 unx     4017 b- defN 23-Jul-18 10:04 pytorch_ood/loss/center.py
+-rw-r--r--  2.0 unx     2378 b- defN 23-Jul-18 10:04 pytorch_ood/loss/conf.py
+-rw-r--r--  2.0 unx     1188 b- defN 23-Jul-18 10:04 pytorch_ood/loss/crossentropy.py
+-rw-r--r--  2.0 unx     2565 b- defN 23-Jul-18 10:04 pytorch_ood/loss/energy.py
+-rw-r--r--  2.0 unx     2741 b- defN 23-Jul-18 10:04 pytorch_ood/loss/entropy.py
+-rw-r--r--  2.0 unx     4599 b- defN 23-Jul-18 10:04 pytorch_ood/loss/ii.py
+-rw-r--r--  2.0 unx     4875 b- defN 23-Jul-18 10:04 pytorch_ood/loss/mchad.py
+-rw-r--r--  2.0 unx     2667 b- defN 23-Jul-18 10:04 pytorch_ood/loss/objectosphere.py
+-rw-r--r--  2.0 unx     2199 b- defN 23-Jul-18 10:04 pytorch_ood/loss/oe.py
+-rw-r--r--  2.0 unx     5289 b- defN 23-Jul-18 10:04 pytorch_ood/loss/svdd.py
+-rw-r--r--  2.0 unx      723 b- defN 23-Jul-18 10:04 pytorch_ood/model/__init__.py
+-rw-r--r--  2.0 unx     4684 b- defN 23-Jul-18 10:04 pytorch_ood/model/centers.py
+-rw-r--r--  2.0 unx     1611 b- defN 23-Jul-18 10:04 pytorch_ood/model/gru.py
+-rw-r--r--  2.0 unx    12226 b- defN 23-Jul-18 10:04 pytorch_ood/model/wrn.py
+-rw-r--r--  2.0 unx       70 b- defN 23-Jul-18 10:04 pytorch_ood/utils/__init__.py
+-rw-r--r--  2.0 unx    10960 b- defN 23-Jul-18 10:04 pytorch_ood/utils/gdown.py
+-rw-r--r--  2.0 unx     7093 b- defN 23-Jul-18 10:04 pytorch_ood/utils/metrics.py
+-rw-r--r--  2.0 unx     2111 b- defN 23-Jul-18 10:04 pytorch_ood/utils/transforms.py
+-rw-r--r--  2.0 unx     8878 b- defN 23-Jul-18 10:04 pytorch_ood/utils/utils.py
+-rw-r--r--  2.0 unx    11351 b- defN 23-Jul-19 08:24 pytorch_ood-0.1.4.dist-info/LICENSE
+-rw-r--r--  2.0 unx    23728 b- defN 23-Jul-19 08:24 pytorch_ood-0.1.4.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-19 08:24 pytorch_ood-0.1.4.dist-info/WHEEL
+-rw-r--r--  2.0 unx       12 b- defN 23-Jul-19 08:24 pytorch_ood-0.1.4.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     6666 b- defN 23-Jul-19 08:24 pytorch_ood-0.1.4.dist-info/RECORD
+76 files, 306146 bytes uncompressed, 99808 bytes compressed:  67.4%
```

## zipnote {}

```diff
@@ -15,14 +15,17 @@
 
 Filename: pytorch_ood/benchmark/img/cifar10.py
 Comment: 
 
 Filename: pytorch_ood/benchmark/img/cifar100.py
 Comment: 
 
+Filename: pytorch_ood/benchmark/img/imagenet.py
+Comment: 
+
 Filename: pytorch_ood/dataset/__init__.py
 Comment: 
 
 Filename: pytorch_ood/dataset/audio/__init__.py
 Comment: 
 
 Filename: pytorch_ood/dataset/audio/fsdd.py
@@ -54,14 +57,17 @@
 
 Filename: pytorch_ood/dataset/img/noise.py
 Comment: 
 
 Filename: pytorch_ood/dataset/img/odin.py
 Comment: 
 
+Filename: pytorch_ood/dataset/img/openood.py
+Comment: 
+
 Filename: pytorch_ood/dataset/img/pixmix.py
 Comment: 
 
 Filename: pytorch_ood/dataset/img/streethazards.py
 Comment: 
 
 Filename: pytorch_ood/dataset/img/textures.py
@@ -201,23 +207,23 @@
 
 Filename: pytorch_ood/utils/transforms.py
 Comment: 
 
 Filename: pytorch_ood/utils/utils.py
 Comment: 
 
-Filename: pytorch_ood-0.1.3.dist-info/LICENSE
+Filename: pytorch_ood-0.1.4.dist-info/LICENSE
 Comment: 
 
-Filename: pytorch_ood-0.1.3.dist-info/METADATA
+Filename: pytorch_ood-0.1.4.dist-info/METADATA
 Comment: 
 
-Filename: pytorch_ood-0.1.3.dist-info/WHEEL
+Filename: pytorch_ood-0.1.4.dist-info/WHEEL
 Comment: 
 
-Filename: pytorch_ood-0.1.3.dist-info/top_level.txt
+Filename: pytorch_ood-0.1.4.dist-info/top_level.txt
 Comment: 
 
-Filename: pytorch_ood-0.1.3.dist-info/RECORD
+Filename: pytorch_ood-0.1.4.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pytorch_ood/__init__.py

```diff
@@ -1,8 +1,8 @@
 """
 PyTorch Out-of-Distribution Detection
 """
-__version__ = "0.1.3"
+__version__ = "0.1.4"
 
 from . import api, dataset, detector, loss, model, utils
 
 __all__ = ["dataset", "detector", "loss", "model", "utils", "api", "__version__"]
```

## pytorch_ood/benchmark/__init__.py

```diff
@@ -5,15 +5,15 @@
 Benchmark objects aim to provide a higher level interface to recreate the
 OOD detection benchmarks used in the literature.
 
 
 API
 ==================
 
-Each benchmark implements a common API.
+Each benchmark implements a common interface.
 
 .. note :: This is currently a draft and likely subject to change in the
     future.
 
 .. code:: python
 
     benchmark = Benchmark(root)
@@ -40,21 +40,43 @@
 ODIN Benchmark
 -----------------
 
 .. autoclass:: pytorch_ood.benchmark.CIFAR10_ODIN
     :members:
 
 
+OpenOOD Benchmark
+-----------------
+
+.. autoclass:: pytorch_ood.benchmark.CIFAR10_OpenOOD
+    :members:
+
 
 CIFAR 100
 ^^^^^^^^^^^
 
 ODIN Benchmark
 -----------------
 
 .. autoclass:: pytorch_ood.benchmark.CIFAR100_ODIN
     :members:
 
+OpenOOD Benchmark
+-----------------
+
+.. autoclass:: pytorch_ood.benchmark.CIFAR100_OpenOOD
+    :members:
+
+
+ImageNet
+^^^^^^^^^^^
+
+OpenOOD Benchmark
+-----------------
+
+.. autoclass:: pytorch_ood.benchmark.ImageNet_OpenOOD
+    :members:
+
 
 """
 from .base import Benchmark
-from .img import CIFAR10_ODIN, CIFAR100_ODIN
+from .img import CIFAR10_ODIN, CIFAR100_ODIN, CIFAR10_OpenOOD, CIFAR100_OpenOOD, ImageNet_OpenOOD
```

## pytorch_ood/benchmark/img/__init__.py

```diff
@@ -1,2 +1,3 @@
-from .cifar10 import CIFAR10_ODIN
-from .cifar100 import CIFAR100_ODIN
+from .cifar10 import CIFAR10_ODIN, CIFAR10_OpenOOD
+from .cifar100 import CIFAR100_ODIN, CIFAR100_OpenOOD
+from .imagenet import ImageNet_OpenOOD
```

## pytorch_ood/benchmark/img/cifar10.py

```diff
@@ -1,18 +1,20 @@
 """
 
 """
 from typing import Dict, List
 
 from torch.utils.data import DataLoader, Dataset
-from torchvision.datasets import CIFAR10
+from torchvision.datasets import CIFAR10, CIFAR100, MNIST, FashionMNIST
+from torchvision.transforms import Compose
 
 from pytorch_ood.api import Detector
-from pytorch_ood.dataset.img import LSUNCrop, LSUNResize, TinyImageNetCrop, TinyImageNetResize, GaussianNoise, UniformNoise
-from pytorch_ood.utils import OODMetrics, ToUnknown
+from pytorch_ood.dataset.img import LSUNCrop, LSUNResize, TinyImageNetCrop, TinyImageNetResize, GaussianNoise, \
+    UniformNoise, TinyImageNet, Textures, Places365
+from pytorch_ood.utils import OODMetrics, ToUnknown, ToRGB
 from pytorch_ood.benchmark import Benchmark
 
 
 class CIFAR10_ODIN(Benchmark):
     """
     Replicates the OOD detection benchmark from the ODIN paper for CIFAR 10.
 
@@ -46,20 +48,20 @@
             ),
             LSUNResize(
                 root, download=True, transform=transform, target_transform=ToUnknown()
             ),
             LSUNCrop(
                 root, download=True, transform=transform, target_transform=ToUnknown()
             ),
-            UniformNoise(1000, size=(32, 32, 3), transform=transform, target_transform=ToUnknown()),
-            GaussianNoise(1000, size=(32, 32, 3), transform=transform, target_transform=ToUnknown())
+            UniformNoise(1000, size=(32, 32, 3), transform=transform, target_transform=ToUnknown(), seed=123),
+            GaussianNoise(1000, size=(32, 32, 3), transform=transform, target_transform=ToUnknown(), seed=123)
         ]
 
         self.ood_names: List[str] = []  #: OOD Dataset names
-        self.ood_names = ["TinyImageNetCrop", "TinyImageNetResize", "LSUNResize", "LSUNCrop", "Uniform", "Gaussian"]
+        self.ood_names = [type(d).__name__ for d in self.test_oods]
 
     def train_set(self) -> Dataset:
         """
         Training dataset
         """
         return self.train_in
 
@@ -75,20 +77,130 @@
         if known and unknown:
             return [self.test_in + other for other in self.test_oods]
 
         if known and not unknown:
             return [self.train_in]
 
         if not known and unknown:
-            return self.ood_datasets
+            return self.test_oods
 
         raise ValueError()
 
     def evaluate(
-        self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
+            self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
+    ) -> List[Dict]:
+        """
+        Evaluates the given detector on all datasets and returns a list with the results
+
+        :param detector: the detector to evaluate
+        :param loader_kwargs: keyword arguments to give to the data loader
+        :param device: the device to move batches to
+        """
+        if loader_kwargs is None:
+            loader_kwargs = {}
+
+        metrics = []
+
+        for name, dataset in zip(self.ood_names, self.test_sets()):
+            loader = DataLoader(dataset=dataset, **loader_kwargs)
+
+            m = OODMetrics()
+
+            for x, y in loader:
+                m.update(detector(x.to(device)), y)
+
+            r = m.compute()
+            r.update({"Dataset": name})
+
+            metrics.append(r)
+
+        return metrics
+
+
+class CIFAR10_OpenOOD(Benchmark):
+    """
+    Aims to replicate the benchmark proposed in *OpenOOD: Benchmarking Generalized Out-of-Distribution Detection*.
+
+    :see Paper: `OpenOOD <https://openreview.net/pdf?id=gT6j4_tskUt>`__
+
+    Outlier datasets are
+
+     * CIFAR100
+     * TinyImageNet
+     * MNIST
+     * FashionMNIST
+     * Textures
+     * Places365
+
+    .. warning :: This currently does not reproduce the benchmark accurately, as it does not exclude images with
+        overlap with CIFAR10.
+
+    """
+
+    def __init__(self, root, transform):
+        """
+        :param root: where to store datasets
+        :param transform: transform to apply to images
+        """
+        self.transform = Compose([ToRGB(), transform])
+        self.train_in = CIFAR10(root, download=True, transform=transform, train=True)
+        self.test_in = CIFAR10(root, download=True, transform=transform, train=False)
+
+        self.test_oods = [
+            CIFAR100(
+                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+            ),
+            TinyImageNet(
+                root, download=True, transform=self.transform, target_transform=ToUnknown(), subset="val"
+            ),
+            MNIST(
+                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+            ),
+            FashionMNIST(
+                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+            ),
+            Textures(
+                root, download=True, transform=self.transform, target_transform=ToUnknown()
+            ),
+            Places365(
+                root, download=True, transform=self.transform, target_transform=ToUnknown()
+            )
+        ]
+
+        self.ood_names: List[str] = []  #: OOD Dataset names
+        self.ood_names = [type(d).__name__ for d in self.test_oods]
+
+    def train_set(self) -> Dataset:
+        """
+        Training dataset
+        """
+        return self.train_in
+
+    def test_sets(self, known=True, unknown=True) -> List[Dataset]:
+        """
+        List of the different test datasets.
+        If known and unknown are true, each dataset contains IN and OOD data.
+
+        :param known: include IN
+        :param unknown: include OOD
+        """
+
+        if known and unknown:
+            return [self.test_in + other for other in self.test_oods]
+
+        if known and not unknown:
+            return [self.train_in]
+
+        if not known and unknown:
+            return self.test_oods
+
+        raise ValueError()
+
+    def evaluate(
+            self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
     ) -> List[Dict]:
         """
         Evaluates the given detector on all datasets and returns a list with the results
 
         :param detector: the detector to evaluate
         :param loader_kwargs: keyword arguments to give to the data loader
         :param device: the device to move batches to
```

## pytorch_ood/benchmark/img/cifar100.py

```diff
@@ -1,18 +1,20 @@
 """
 
 """
 from typing import Dict, List
 
 from torch.utils.data import DataLoader, Dataset
-from torchvision.datasets import CIFAR100
+from torchvision.datasets import CIFAR100, MNIST, CIFAR10, FashionMNIST
+from torchvision.transforms import Compose
 
 from pytorch_ood.api import Detector
-from pytorch_ood.dataset.img import LSUNCrop, LSUNResize, TinyImageNetCrop, TinyImageNetResize, GaussianNoise, UniformNoise
-from pytorch_ood.utils import OODMetrics, ToUnknown
+from pytorch_ood.dataset.img import LSUNCrop, LSUNResize, TinyImageNetCrop, TinyImageNetResize, GaussianNoise, \
+    UniformNoise, TinyImageNet, Places365, Textures
+from pytorch_ood.utils import OODMetrics, ToUnknown, ToRGB
 from pytorch_ood.benchmark import Benchmark
 
 
 class CIFAR100_ODIN(Benchmark):
     """
     Replicates the OOD detection benchmark from the ODIN paper for CIFAR 100.
 
@@ -88,14 +90,124 @@
     ) -> List[Dict]:
         """
         Evaluates the given detector on all datasets and returns a list with the results
 
         :param detector: the detector to evaluate
         :param loader_kwargs: keyword arguments to give to the data loader
         :param device: the device to move batches to
+        """
+        if loader_kwargs is None:
+            loader_kwargs = {}
+
+        metrics = []
+
+        for name, dataset in zip(self.ood_names, self.test_sets()):
+            loader = DataLoader(dataset=dataset, **loader_kwargs)
+
+            m = OODMetrics()
+
+            for x, y in loader:
+                m.update(detector(x.to(device)), y)
+
+            r = m.compute()
+            r.update({"Dataset": name})
+
+            metrics.append(r)
+
+        return metrics
+
+
+class CIFAR100_OpenOOD(Benchmark):
+    """
+    Aims to replicate the benchmark proposed in *OpenOOD: Benchmarking Generalized Out-of-Distribution Detection*.
+
+    :see Paper: `OpenOOD <https://openreview.net/pdf?id=gT6j4_tskUt>`__
+
+    Outlier datasets are
+
+     * CIFAR10
+     * TinyImageNet
+     * MNIST
+     * FashionMNIST
+     * Textures
+     * Places365
+
+    .. warning :: This currently does not reproduce the benchmark accurately, as it does not exclude images with
+        overlap with CIFAR100.
+
+    """
+
+    def __init__(self, root, transform):
+        """
+        :param root: where to store datasets
+        :param transform: transform to apply to images
+        """
+        self.transform = Compose([ToRGB(), transform])
+        self.train_in = CIFAR100(root, download=True, transform=transform, train=True)
+        self.test_in = CIFAR100(root, download=True, transform=transform, train=False)
+
+        self.test_oods = [
+            CIFAR10(
+                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+            ),
+            TinyImageNet(
+                root, download=True, transform=self.transform, target_transform=ToUnknown(), subset="test"
+            ),
+            MNIST(
+                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+            ),
+            FashionMNIST(
+                root, download=True, transform=self.transform, target_transform=ToUnknown(), train=False
+            ),
+            Textures(
+                root, download=True, transform=self.transform, target_transform=ToUnknown()
+            ),
+            Places365(
+                root, download=True, transform=self.transform, target_transform=ToUnknown()
+            )
+        ]
+
+        self.ood_names: List[str] = []  #: OOD Dataset names
+        self.ood_names = [type(d).__name__ for d in self.test_oods]
+
+    def train_set(self) -> Dataset:
+        """
+        Training dataset
+        """
+        return self.train_in
+
+    def test_sets(self, known=True, unknown=True) -> List[Dataset]:
+        """
+        List of the different test datasets.
+        If known and unknown are true, each dataset contains IN and OOD data.
+
+        :param known: include IN
+        :param unknown: include OOD
+        """
+
+        if known and unknown:
+            return [self.test_in + other for other in self.test_oods]
+
+        if known and not unknown:
+            return [self.train_in]
+
+        if not known and unknown:
+            return self.test_oods
+
+        raise ValueError()
+
+    def evaluate(
+            self, detector: Detector, loader_kwargs: Dict = None, device: str = "cpu"
+    ) -> List[Dict]:
+        """
+        Evaluates the given detector on all datasets and returns a list with the results
+
+        :param detector: the detector to evaluate
+        :param loader_kwargs: keyword arguments to give to the data loader
+        :param device: the device to move batches to
         """
         if loader_kwargs is None:
             loader_kwargs = {}
 
         metrics = []
 
         for name, dataset in zip(self.ood_names, self.test_sets()):
```

## pytorch_ood/dataset/img/__init__.py

```diff
@@ -31,14 +31,19 @@
     :members:
 
 TinyImageNet
 ``````````````````````````
 ..  autoclass:: pytorch_ood.dataset.img.TinyImageNet
     :members:
 
+Places365
+``````````````````````````
+..  autoclass:: pytorch_ood.dataset.img.Places365
+    :members:
+
 80M TinyImages
 ``````````````````````````
 ..  autoclass:: pytorch_ood.dataset.img.TinyImages
     :members:
 
 300K Random Images
 ``````````````````````````
@@ -77,14 +82,24 @@
     :members:
 
 ImageNet-C
 `````````````
 ..  autoclass:: pytorch_ood.dataset.img.ImageNetC
     :members:
 
+OpenImages-O
+`````````````
+..  autoclass:: pytorch_ood.dataset.img.OpenImagesO
+    :members:
+
+iNaturalist
+`````````````
+..  autoclass:: pytorch_ood.dataset.img.iNaturalist
+    :members:
+
 Chars74k
 `````````````
 ..  autoclass:: pytorch_ood.dataset.img.Chars74k
     :members:
 
 
 PixMix
@@ -143,7 +158,8 @@
 from .noise import GaussianNoise, UniformNoise
 from .odin import LSUNCrop, LSUNResize, TinyImageNetCrop, TinyImageNetResize
 from .pixmix import FeatureVisDataset, FractalDataset, PixMixDataset
 from .streethazards import StreetHazards
 from .textures import Textures
 from .tinyimagenet import TinyImageNet
 from .tinyimages import TinyImages, TinyImages300k
+from .openood import OpenImagesO, iNaturalist, Places365
```

## pytorch_ood/dataset/img/base.py

```diff
@@ -1,18 +1,23 @@
 import logging
 import os
 from typing import Any, Callable, Optional, Tuple
 
 from PIL import Image
 from torchvision.datasets import VisionDataset
 from torchvision.datasets.utils import check_integrity, download_and_extract_archive
+from os.path import join, dirname
 
 log = logging.getLogger(__name__)
 
 
+def _get_resource_file(name):
+    return join(dirname(__file__), "resources", name)
+
+
 class ImageDatasetBase(VisionDataset):
     """
     Base Class for Downloading Image related Datasets
 
     Code inspired by : https://pytorch.org/vision/0.8/_modules/torchvision/datasets/cifar.html#CIFAR10
     """
 
@@ -37,29 +42,28 @@
 
         if not self._check_integrity():
             raise RuntimeError(
                 "Dataset not found or corrupted." + " You can use download=True to download it"
             )
 
         self.basedir = os.path.join(self.root, self.base_folder)
-        self.files = os.listdir(self.basedir)
+        self.files = [join(self.basedir, img) for img in os.listdir(self.basedir)]
 
     def __getitem__(self, index: int) -> Tuple[Any, Any]:
         """
         Args:
             index (int): Index
 
         Returns:
             tuple: (image, target) where target is index of the target class.
         """
-        file, target = self.files[index], -1
+        path, target = self.files[index], -1
 
         # doing this so that it is consistent with all other datasets
         # to return a PIL Image
-        path = os.path.join(self.root, self.base_folder, file)
         img = Image.open(path)
 
         if self.transform is not None:
             img = self.transform(img)
 
         if self.target_transform is not None:
             target = self.target_transform(target)
```

## pytorch_ood/dataset/img/tinyimagenet.py

```diff
@@ -1,13 +1,16 @@
 import logging
+import os
 from os.path import exists, join
+from PIL import Image
 
-from torchvision.datasets import ImageFolder, VisionDataset
+from torchvision.datasets import VisionDataset
 from torchvision.datasets.utils import download_and_extract_archive
 
+
 log = logging.getLogger(__name__)
 
 
 class TinyImageNet(VisionDataset):
     """
     Small Version of the ImageNet with images of size :math:`64 \\times 64` from 200 classes used by
     Stanford. Each class has 500 images for training.
@@ -16,43 +19,72 @@
         :width: 400px
         :alt: Textured Dataset
         :align: center
 
 
     This dataset is often used for training, but not included in Torchvision.
 
-    :see Website: `<Stanford http://cs231n.stanford.edu/>`__
+    :see Website: `Stanford <http://cs231n.stanford.edu/>`__
 
     """
 
     url = "http://cs231n.stanford.edu/tiny-imagenet-200.zip"
     dir_name = "tiny-imagenet-200"
     tgz_md5 = "90528d7ca1a48142e341f4ef8d21d0de"
     filename = "tiny-imagenet-200.zip"
     subsets = ["train", "val", "test"]
 
-    def __init__(self, root, subset, download=False, transform=None, target_transform=None):
+    def __init__(self, root, subset="train", download=False, transform=None, target_transform=None):
+        """
+        :para subset: can be one of ``train``, ``val`` and ``test``
+        """
+        if subset not in self.subsets:
+            raise ValueError(f"Invalid subset: {subset}. Possible values are {self.subsets}")
+
         super(TinyImageNet, self).__init__(
             root, target_transform=target_transform, transform=transform
         )
 
         self.subset = subset
 
         if download:
             self.download()
 
         if not self._check_integrity():
             raise RuntimeError(
                 "Dataset not found or corrupted." + " You can use download=True to download it"
             )
 
-        if subset not in self.subsets:
-            raise ValueError(f"Invalid subset: {subset}. Possible values are {self.subsets}")
-
-        self.data = ImageFolder(root=join(self.root, self.dir_name, self.subset))
+        classes = os.listdir(join(self.root, self.dir_name, "train"))
+        classes.sort()
+        self.class_map = {c: n for n, c in enumerate(classes)}  # : map class_names to integers
+        self.basename = join(self.root, self.dir_name, self.subset)
+        self.paths = []
+        self.labels = []
+
+        if subset == "train":
+            for d in classes:
+                p = join(self.basename, d, "images")
+                files = [join(p, img) for img in os.listdir(p)]
+
+                self.paths += files
+                self.labels += [self.class_map[d]] * len(files)
+
+        elif subset == "val":
+            anno_file = join(self.basename, "val_annotations.txt")
+            with open(anno_file, "r") as f:
+                for line in f.readlines():
+                    path, label, x, y, z, t = " ".join(line.split())
+                    self.paths.append(join(self.basename, "images", path))
+                    self.labels = self.class_map[label]
+
+        elif subset == "test":
+            d = join(self.basename, "images")
+            self.paths = [join(d, img) for img in os.listdir(d)]
+            self.labels = [-1] * len(self.paths)
 
     def download(self):
         if self._check_integrity():
             log.debug("Files already downloaded and verified")
             return
         download_and_extract_archive(self.url, self.root, filename=self.filename, md5=self.tgz_md5)
 
@@ -63,24 +95,22 @@
         """
         Args:
             index (int): Index
 
         Returns:
             tuple: (image, target) where target is index of the target class.
         """
-        img, target = self.data[index]
+        img, target = self.paths[index], self.labels[index]
+
+        img = Image.open(img)
 
         if self.transform is not None:
             img = self.transform(img)
 
         if self.target_transform is not None:
             target = self.target_transform(target)
 
         return img, target
 
     def __len__(self):
-        return len(self.dataset)
-
+        return len(self.paths)
 
-if __name__ == "__main__":
-    ds = TinyImageNet(root="/home/ki/datasets/", subset="train", download=True)
-    print(ds[0])
```

## pytorch_ood/loss/center.py

```diff
@@ -62,17 +62,17 @@
 
     def _init_centers(self):
         # In the published code, Wen et al. initialize centers randomly.
         # However, this might bot be optimal if the loss is used without an additional
         # inter-class-discriminability term.
         # The Class Anchor Clustering initializes the centers as scaled unit vectors.
         if self.num_classes == self.feat_dim:
-            torch.nn.init.eye_(self.centers.centers)
-            if not self.centers.centers.requires_grad:
-                self.centers.centers.mul_(self.magnitude)
+            torch.nn.init.eye_(self._centers._params)
+            if not self._centers._params.requires_grad:
+                self._centers._params.mul_(self.magnitude)
         # Orthogonal could also be a good option. this can also be used if the embedding dimensionality is
         # different then the number of classes
         # torch.nn.init.orthogonal_(self.centers, gain=10)
         else:
             torch.nn.init.normal_(self.centers.params)
             if self.magnitude != 1:
                 log.warning("Not applying magnitude parameter.")
```

## pytorch_ood/model/wrn.py

```diff
@@ -211,14 +211,26 @@
                     tvt.Resize(size=(32, 32)),
                     ToRGB(),
                     tvt.ToTensor(),
                     tvt.Normalize(std=std, mean=mean),
                 ]
             )
             return trans
+        elif pretrained in ["imagenet32-nocifar"]:
+            mean = [0.5] * 3
+            std = [0.5] * 3
+            trans = tvt.Compose(
+                [
+                    tvt.Resize(size=(32, 32)),
+                    ToRGB(),
+                    tvt.ToTensor(),
+                    tvt.Normalize(std=std, mean=mean),
+                ]
+            )
+            return trans
 
         raise ValueError("Unknown Model")
 
     def forward(self, x: Tensor) -> Tensor:
         """
         Forward propagate
```

## Comparing `pytorch_ood-0.1.3.dist-info/LICENSE` & `pytorch_ood-0.1.4.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pytorch_ood-0.1.3.dist-info/METADATA` & `pytorch_ood-0.1.4.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 Metadata-Version: 2.1
 Name: pytorch-ood
-Version: 0.1.3
+Version: 0.1.4
 Summary: A Library for Out-of-Distribution Detection with PyTorch
 Home-page: https://github.com/kkirchheim/pytorch-ood
 Author: Konstantin Kirchheim
 License: Apache 2.0
 Project-URL: Bug Tracker, https://github.com/kkirchheim/pytorch-ood/issues
 Project-URL: repository, https://github.com/kkirchheim/pytorch-ood
 Keywords: OOD,PyTorch,Out-of-Distribution Detection
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: OS Independent
 Requires-Python: >=3.8
 License-File: LICENSE
 Requires-Dist: torch (>=1.7.0)
 Requires-Dist: torchvision (>=0.12.0)
-Requires-Dist: torchmetrics (==0.10.2)
+Requires-Dist: torchmetrics (>=1.0.0)
 Requires-Dist: scipy (>=1.7.0)
 
 PyTorch Out-of-Distribution Detection
 ****************************************
 
 .. image:: https://img.shields.io/badge/docs-online-blue
    :target: https://pytorch-ood.readthedocs.io/en/latest/
@@ -103,15 +103,15 @@
 
     for x, y in data_loader:
         metrics.update(detector(x.cuda()), y)
 
     print(metrics.compute())
 
 
-You can find more examples in the `documentation <https://pytorch-ood.readthedocs.io/en/latest/auto_examples/index.html>`_.
+You can find more examples in the `documentation <https://pytorch-ood.readthedocs.io/en/latest/auto_examples/benchmarks/>`_.
 
 üõ† Ô∏èÔ∏èInstallation
 ^^^^^^^^^^^^^^^^^
 The package can be installed via PyPI:
 
 .. code-block:: shell
 
@@ -129,15 +129,16 @@
 
 
 **Optional Dependencies**
 
 
 * ``libmr``  for the OpenMax Detector [#OpenMax]_ . The library is currently broken and unlikely to be repaired.
   You will have to install ``cython`` and ``libmr`` afterwards manually.
-* ``scikit`` for ViM
+* ``scikit-learn`` for ViM
+* ``gdown`` to download some datasets and model weights
 * ``pandas`` for the `examples <https://pytorch-ood.readthedocs.io/en/latest/auto_examples/index.html>`_.
 * ``segmentation-models-pytorch`` to run the examples for anomaly segmentation
 
 
 üì¶ Implemented
 ^^^^^^^^^^^^^^^
```

## Comparing `pytorch_ood-0.1.3.dist-info/RECORD` & `pytorch_ood-0.1.4.dist-info/RECORD`

 * *Files 9% similar despite different names*

```diff
@@ -1,31 +1,33 @@
-pytorch_ood/__init__.py,sha256=U3wpHL9-T69oKbQwTYbubWgzE2WsS5BlODo6yYeLTIU,209
+pytorch_ood/__init__.py,sha256=8C0gUZGqnsI_3e4qRaRP3gfXMvEDm_8iHaeWxBruaeQ,209
 pytorch_ood/api.py,sha256=eOp1WgYknpzDq8v1hY8V7oWXsI6SQHyCAC1C1dSL6MI,2341
-pytorch_ood/benchmark/__init__.py,sha256=YtHbZGG9JcB4vgz4DqLkIHMwL0BC5uguHEPKAQI7Wnk,1005
+pytorch_ood/benchmark/__init__.py,sha256=sRY6Sh4aPdcIuuGbvApXq_uAnVBA5xkslpPod3Za-Vo,1404
 pytorch_ood/benchmark/base.py,sha256=2uX-drzDBoz1sG4S2W3Bgsa2riZV_-OsQbP8lf2dD4w,876
-pytorch_ood/benchmark/img/__init__.py,sha256=fuSekNp8fC-zrzMC57o99btzKsgPZjmMN6kVwGJBi7c,70
-pytorch_ood/benchmark/img/cifar10.py,sha256=GNeTdmNW2jswTdqX8oNuW3CzSy_55zsv7Qba7_-lsec,3573
-pytorch_ood/benchmark/img/cifar100.py,sha256=jHkM9q6FrTQDEapZg9jfb90aKF5s063DoBXGmZ_QFHE,3578
+pytorch_ood/benchmark/img/__init__.py,sha256=Iq_4O1y3fl3cGZyzq5-6bVEf_abHmKwHeuEL-75FhqM,144
+pytorch_ood/benchmark/img/cifar10.py,sha256=rKkIBukethyNUZU3GI7lDOGF8H8hpvcZ9S9SXffu-TI,7122
+pytorch_ood/benchmark/img/cifar100.py,sha256=Pt-C-KNiHTx5LZWiO1O6gY9OTmocbF-ORnLqzfQ18YM,7157
+pytorch_ood/benchmark/img/imagenet.py,sha256=esPDHIg_hNJcssqWDE1FynPlDFQ4px2H-OU24RpBDME,3922
 pytorch_ood/dataset/__init__.py,sha256=4mwJ-YED4MXBlZXeqi04Qg5zxqDMxaszHOupjVy9LAQ,11
 pytorch_ood/dataset/audio/__init__.py,sha256=zYLehvyFdN4DjcGRPpoLVMgSp9HNiAiXAfjxT8CH10k,165
 pytorch_ood/dataset/audio/fsdd.py,sha256=cYKcHtbV7BhGKav_d5qFvNiLRwhvwzWTC93txgu2ezg,3059
-pytorch_ood/dataset/img/__init__.py,sha256=jTf8p129jR9Sq8BOUb7XtFpNSz0m7kL1iJ9-t7h2rs4,3141
-pytorch_ood/dataset/img/base.py,sha256=LLFSqBB2tkVjCRHcqAUwZNW3KCPON0kJClOFvrq3xy0,2363
+pytorch_ood/dataset/img/__init__.py,sha256=RN_xTWNyZCERYt7y8_xMrt1EIfBN5Cvf-PiOAGX3iBg,3487
+pytorch_ood/dataset/img/base.py,sha256=4MGJXE2eQ35sSobrn5rVictgdRcYaNlAB1hV5zcBl6E,2457
 pytorch_ood/dataset/img/chars74k.py,sha256=YA4Hx92fS29jxrpG0zEc09RaRSpmGRdMQMBJ4lyAm2E,4329
 pytorch_ood/dataset/img/cifar.py,sha256=seM3RpADphYfIt9ON5Wpslv5_QNTs5fhi3JfEkVH-vE,3076
 pytorch_ood/dataset/img/fooling.py,sha256=IBi0nWCIYRZJSgFrJejhpgmTB5CWswlc466wXTo41EA,1331
 pytorch_ood/dataset/img/imagenet.py,sha256=MpA-6DD6R1JlIWmS1Ytuik-kfImCnPc-H_Urzj13eqU,5793
 pytorch_ood/dataset/img/mnistc.py,sha256=Wp06GFhRi9_iM6SSe_rUG3GuolMUtuZ_qlR7ctpY4l0,4270
 pytorch_ood/dataset/img/mvtech.py,sha256=Ki5BjbohoWQ-W3tGNAh_1yxidutiujQfBI8tvgEL5R4,5382
 pytorch_ood/dataset/img/noise.py,sha256=Nt0AF6iFAns0amJ1y3TZHu_WY1qyyb3splVa9Lt-49c,3936
 pytorch_ood/dataset/img/odin.py,sha256=ryBc43DqdRzQ5HMc1X6JNMy_-InR74mbO_-9AAxKOf4,4891
+pytorch_ood/dataset/img/openood.py,sha256=fxq9rhSAhHN1S-4hxa_uAZoFsEoGKpLLBi-iYL2iIlI,4836
 pytorch_ood/dataset/img/pixmix.py,sha256=um3PfP9Gb03TwjoEyOCxatI_ci824FHY35ygX-E7kBU,8477
 pytorch_ood/dataset/img/streethazards.py,sha256=R4-EMG9voEOshJEBmFtf17KijNcuveuufOVwQVMhqGE,4242
 pytorch_ood/dataset/img/textures.py,sha256=VLEv5Md6pNqivOhAp4vUwxuEWfAwfvkPMvnd7zSmDrM,3001
-pytorch_ood/dataset/img/tinyimagenet.py,sha256=xECgJA35wJ4RjZIn8VmenRLvo_-mlmVct6u9386MrQI,2641
+pytorch_ood/dataset/img/tinyimagenet.py,sha256=EkXQ3Lb6gFS480mf40HXKJl00SUNXoi732Kv09bcH6g,3766
 pytorch_ood/dataset/img/tinyimages.py,sha256=sOw_TIPUxqFaqR5rxAISizkKPNf4BgBcn_kp_bVsiTI,4692
 pytorch_ood/dataset/ossim/__init__.py,sha256=v3qdGAIvb1X6NUHDOK4-jAk1R8nq6Oisjds3ZG2IEp4,507
 pytorch_ood/dataset/ossim/ossim.py,sha256=W6MlSCxrTs_EGlYhB8sSpboh1EDztJJ9YhLVyXv1zSQ,8194
 pytorch_ood/dataset/txt/__init__.py,sha256=wpmf8nqnJkDI-CAgAI-pH07PoV0UKu_miSreNaEX9dw,1049
 pytorch_ood/dataset/txt/multi30k.py,sha256=35ssfLwz8zrj50UwxA0s9rw5VKXMQADuwRXt3Bmcspg,2665
 pytorch_ood/dataset/txt/newsgroups.py,sha256=4tTVikOfXEEUw0CX0vm2pzPKhund5VYNaUYlyGWCNIQ,4002
 pytorch_ood/dataset/txt/reuters.py,sha256=ZWyQ_8VfmPu-9CdDf1PByMqy6HMoxjXGqscmICcHQTk,5954
@@ -44,31 +46,31 @@
 pytorch_ood/detector/vim.py,sha256=v5Og7bL3oGEXSPfv9WQv1iF0wRofNfKrVttO4EDSE6I,5267
 pytorch_ood/detector/openmax/__init__.py,sha256=aLSjSeuO9EgxWo5SBbLyOqMLcIMIbgpju9WUL8wtgyE,323
 pytorch_ood/detector/openmax/numpy.py,sha256=zh5SHjrHhfeWX9Tol5aaDbaKtxT70j8JjSC7EtHi1s0,7041
 pytorch_ood/detector/openmax/torch.py,sha256=Ztzt7Y8IRgRocVZ-TzBbBBmGFIFlH5RZyITiMtAC-EQ,3318
 pytorch_ood/loss/__init__.py,sha256=p3b1JB96f4YjaDkk1PvaaMFs7tDxUyPW6_lZqR5gFGI,5408
 pytorch_ood/loss/background.py,sha256=d9TNUzeWqsZ7c-EjAGFrWPdOagnQIcJ4hTmTtJD50sc,1589
 pytorch_ood/loss/cac.py,sha256=p1qGxb3i447r6fY1vk3Hh7MWrPUC1KHD4aeh2wUAqmU,3877
-pytorch_ood/loss/center.py,sha256=xcG1GQWcq312GTg1LrlADvc_8g1yVSUkV5nMc12V_94,4014
+pytorch_ood/loss/center.py,sha256=oKXg_WD3JoQH3Gh38Zy6sTlL0vN44-ksSDCLkbvmnm0,4017
 pytorch_ood/loss/conf.py,sha256=KZdIlKVGqr_rRMR8M9CvDsWtBxTRlG4zgWUVZnBjBkM,2378
 pytorch_ood/loss/crossentropy.py,sha256=JAsScyzOA3GTn2IlHIKU6ya7KyLuu-8r_zhNuLqrNjY,1188
 pytorch_ood/loss/energy.py,sha256=Hns8hACIwuMewFzAu58_uozSRDdkFG-uT8qi-iukPtw,2565
 pytorch_ood/loss/entropy.py,sha256=K02U4FMyIKZAmOLRnifD_NUPjDZx3zo5LzNdLMT0jpY,2741
 pytorch_ood/loss/ii.py,sha256=ISc-GCRgYnACNMS9GpkfpjJEqPXwChbULp81ykq2M6U,4599
 pytorch_ood/loss/mchad.py,sha256=u-a4tOnDqHeKgwtUb8XPtJqdCsGPtOIUcOnm1p9LgaE,4875
 pytorch_ood/loss/objectosphere.py,sha256=eubnoMhEkmdIhuFAB485CBX-eT0NObxt7FTCkp8HAJE,2667
 pytorch_ood/loss/oe.py,sha256=k9VRWIGyodwk5crz6snu8KGb2VSzGRmP8lCYBxm10WE,2199
 pytorch_ood/loss/svdd.py,sha256=raL0nldFjwk77UM-IDM3B0mGHv50neBZULEPXd5zcaM,5289
 pytorch_ood/model/__init__.py,sha256=U2iKbCxQMfzkk1q9eeo2Wg2Bm81-UUpN9nA2o5dDwZ0,723
 pytorch_ood/model/centers.py,sha256=4pJ5_fWQ_BOO-QRkwAqc5lGZbUsO6F-ZHOlAmQQWYbs,4684
 pytorch_ood/model/gru.py,sha256=bWgH_9kHofqaqK19K0fm3VA-AimQ6EWuoq3pS6FUo48,1611
-pytorch_ood/model/wrn.py,sha256=ySSLklR5xzotqHB8ViPEMwyQBSmWeGr740QxqUV7z3M,11843
+pytorch_ood/model/wrn.py,sha256=hAOGgBiOGPdWubvlYZLEfL0-VC6eTXY145GvyT71CgQ,12226
 pytorch_ood/utils/__init__.py,sha256=7HadmLfYRDYnSzWduQZdcIIOzfH-8-_BO8lbq1g-wNk,70
 pytorch_ood/utils/gdown.py,sha256=EC-bvd3Qp82zO0wrpLpwh2QVzDQSCKM5SwwO759B5_0,10960
 pytorch_ood/utils/metrics.py,sha256=0a9-c9fUPMK3MjLGo6amWSn18wVGJZv3VotNv9oaILg,7093
 pytorch_ood/utils/transforms.py,sha256=uf2MAtsq0BezMv2Vv4e3_EgJNWPeTM7yeEC7eGtVPO0,2111
 pytorch_ood/utils/utils.py,sha256=mod65a7ibuaNcwc7fSe0VdWJFNlqDyCU7whrGMMtY_k,8878
-pytorch_ood-0.1.3.dist-info/LICENSE,sha256=sCskNJvx8BCnWvaR8F3J3W0pm3bJhObe3XZKfuLTvnA,11351
-pytorch_ood-0.1.3.dist-info/METADATA,sha256=lxZUOCZGoifI-BeeG4v_p8JCQfNj-nnO9977ev0tZx0,23666
-pytorch_ood-0.1.3.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-pytorch_ood-0.1.3.dist-info/top_level.txt,sha256=DQdb7oLs5bEbGwRkFDMkBe3xeGccj8U-P4eHsi8VOS0,12
-pytorch_ood-0.1.3.dist-info/RECORD,,
+pytorch_ood-0.1.4.dist-info/LICENSE,sha256=sCskNJvx8BCnWvaR8F3J3W0pm3bJhObe3XZKfuLTvnA,11351
+pytorch_ood-0.1.4.dist-info/METADATA,sha256=qqC8tyXxLl3jSdsD8PaOK0J6wno991qfLUnVGHnts-o,23728
+pytorch_ood-0.1.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+pytorch_ood-0.1.4.dist-info/top_level.txt,sha256=DQdb7oLs5bEbGwRkFDMkBe3xeGccj8U-P4eHsi8VOS0,12
+pytorch_ood-0.1.4.dist-info/RECORD,,
```

