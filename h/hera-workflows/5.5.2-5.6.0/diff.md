# Comparing `tmp/hera_workflows-5.5.2.tar.gz` & `tmp/hera_workflows-5.6.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "hera_workflows-5.5.2.tar", max compression
+gzip compressed data, was "hera_workflows-5.6.0.tar", max compression
```

## Comparing `hera_workflows-5.5.2.tar` & `hera_workflows-5.6.0.tar`

### file list

```diff
@@ -1,137 +1,135 @@
--rw-r--r--   0        0        0     1066 2023-06-26 15:59:36.612145 hera_workflows-5.5.2/LICENSE
--rw-r--r--   0        0        0    12133 2023-06-26 15:59:36.612145 hera_workflows-5.5.2/README.md
--rw-r--r--   0        0        0     3540 2023-06-26 16:00:00.128887 hera_workflows-5.5.2/pyproject.toml
--rw-r--r--   0        0        0      522 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/__init__.py
--rw-r--r--   0        0        0      310 2023-06-26 16:00:00.128887 hera_workflows-5.5.2/src/hera/_version.py
--rw-r--r--   0        0        0     1289 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/auth/__init__.py
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/__init__.py
--rw-r--r--   0        0        0     3267 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/__init__.py
--rw-r--r--   0        0        0     1418 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/eventsource.py
--rw-r--r--   0        0        0      861 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/eventsource.pyi
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/google/__init__.py
--rw-r--r--   0        0        0      347 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/google/protobuf.py
--rw-r--r--   0        0        0      164 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/google/protobuf.pyi
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/grpc/__init__.py
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/grpc/gateway/__init__.py
--rw-r--r--   0        0        0      691 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/grpc/gateway/runtime.py
--rw-r--r--   0        0        0      470 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/grpc/gateway/runtime.pyi
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/io/__init__.py
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/io/argoproj/__init__.py
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/io/argoproj/events/__init__.py
--rw-r--r--   0        0        0   102993 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/io/argoproj/events/v1alpha1.py
--rw-r--r--   0        0        0    25271 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/io/argoproj/events/v1alpha1.pyi
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.632146 hera_workflows-5.5.2/src/hera/events/models/io/argoproj/workflow/__init__.py
--rw-r--r--   0        0        0   144549 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/argoproj/workflow/v1alpha1.py
--rw-r--r--   0        0        0    31958 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/argoproj/workflow/v1alpha1.pyi
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/__init__.py
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/api/__init__.py
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/api/core/__init__.py
--rw-r--r--   0        0        0   127883 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/api/core/v1.py
--rw-r--r--   0        0        0    18004 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/api/core/v1.pyi
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/api/policy/__init__.py
--rw-r--r--   0        0        0     1751 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/api/policy/v1beta1.py
--rw-r--r--   0        0        0      376 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/api/policy/v1beta1.pyi
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/__init__.py
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/__init__.py
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/api/__init__.py
--rw-r--r--   0        0        0     3161 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/api/resource.py
--rw-r--r--   0        0        0      105 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/api/resource.pyi
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/apis/__init__.py
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/apis/meta/__init__.py
--rw-r--r--   0        0        0    22218 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/apis/meta/v1.py
--rw-r--r--   0        0        0     2064 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/apis/meta/v1.pyi
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/util/__init__.py
--rw-r--r--   0        0        0      277 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/util/intstr.py
--rw-r--r--   0        0        0      108 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/util/intstr.pyi
--rw-r--r--   0        0        0     1453 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/sensor.py
--rw-r--r--   0        0        0      887 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/models/sensor.pyi
--rw-r--r--   0        0        0    27607 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/events/service.py
--rw-r--r--   0        0        0     2186 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/exceptions/__init__.py
--rw-r--r--   0        0        0      282 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/expr/__init__.py
--rw-r--r--   0        0        0    12021 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/expr/_node.py
--rw-r--r--   0        0        0      647 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/expr/_sprig.py
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/py.typed
--rw-r--r--   0        0        0      188 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/shared/__init__.py
--rw-r--r--   0        0        0      269 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/shared/_base_model.py
--rw-r--r--   0        0        0     4997 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/shared/_global_config.py
--rw-r--r--   0        0        0      513 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/shared/serialization.py
--rw-r--r--   0        0        0     4585 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/workflows/__init__.py
--rw-r--r--   0        0        0     2912 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/workflows/_context.py
--rw-r--r--   0        0        0     4741 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/workflows/_inspect.py
--rw-r--r--   0        0        0    41532 2023-06-26 15:59:36.636146 hera_workflows-5.5.2/src/hera/workflows/_mixins.py
--rw-r--r--   0        0        0    32815 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/_unparse.py
--rw-r--r--   0        0        0       25 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/action.py
--rw-r--r--   0        0        0     1115 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/archive.py
--rw-r--r--   0        0        0     7697 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/artifact.py
--rw-r--r--   0        0        0     3640 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/cluster_workflow_template.py
--rw-r--r--   0        0        0     3464 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/container.py
--rw-r--r--   0        0        0     7280 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/container_set.py
--rw-r--r--   0        0        0     9088 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/cron_workflow.py
--rw-r--r--   0        0        0     2877 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/dag.py
--rw-r--r--   0        0        0     2241 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/data.py
--rw-r--r--   0        0        0     4855 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/env.py
--rw-r--r--   0        0        0     1329 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/env_from.py
--rw-r--r--   0        0        0      208 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/exceptions.py
--rw-r--r--   0        0        0     2346 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/http_template.py
--rw-r--r--   0        0        0       30 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/memoize.py
--rw-r--r--   0        0        0     4418 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/metrics.py
--rw-r--r--   0        0        0     6913 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/__init__.py
--rw-r--r--   0        0        0     1418 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/eventsource.py
--rw-r--r--   0        0        0      861 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/eventsource.pyi
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/google/__init__.py
--rw-r--r--   0        0        0      347 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/google/protobuf.py
--rw-r--r--   0        0        0      164 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/google/protobuf.pyi
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/grpc/__init__.py
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/grpc/gateway/__init__.py
--rw-r--r--   0        0        0      691 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/grpc/gateway/runtime.py
--rw-r--r--   0        0        0      470 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/grpc/gateway/runtime.pyi
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/__init__.py
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/__init__.py
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/events/__init__.py
--rw-r--r--   0        0        0   102993 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/events/v1alpha1.py
--rw-r--r--   0        0        0    25271 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/events/v1alpha1.pyi
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/workflow/__init__.py
--rw-r--r--   0        0        0   144549 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/workflow/v1alpha1.py
--rw-r--r--   0        0        0    31958 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/workflow/v1alpha1.pyi
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/__init__.py
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/api/__init__.py
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/api/core/__init__.py
--rw-r--r--   0        0        0   127883 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/api/core/v1.py
--rw-r--r--   0        0        0    18004 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/api/core/v1.pyi
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/api/policy/__init__.py
--rw-r--r--   0        0        0     1751 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/api/policy/v1beta1.py
--rw-r--r--   0        0        0      376 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/api/policy/v1beta1.pyi
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/__init__.py
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/__init__.py
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/api/__init__.py
--rw-r--r--   0        0        0     3161 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/api/resource.py
--rw-r--r--   0        0        0      105 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/api/resource.pyi
--rw-r--r--   0        0        0        0 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/__init__.py
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/meta/__init__.py
--rw-r--r--   0        0        0    22218 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/meta/v1.py
--rw-r--r--   0        0        0     2064 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/meta/v1.pyi
--rw-r--r--   0        0        0      144 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/util/__init__.py
--rw-r--r--   0        0        0      277 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/util/intstr.py
--rw-r--r--   0        0        0      108 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/util/intstr.pyi
--rw-r--r--   0        0        0     1453 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/sensor.py
--rw-r--r--   0        0        0      887 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/models/sensor.pyi
--rw-r--r--   0        0        0      722 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/operator.py
--rw-r--r--   0        0        0     4084 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/parameter.py
--rw-r--r--   0        0        0     1076 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/protocol.py
--rw-r--r--   0        0        0     3146 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/resource.py
--rw-r--r--   0        0        0     5262 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/resources.py
--rw-r--r--   0        0        0     1856 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/retry_strategy.py
--rw-r--r--   0        0        0     3703 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/runner.py
--rw-r--r--   0        0        0    16736 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/script.py
--rw-r--r--   0        0        0    55301 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/service.py
--rw-r--r--   0        0        0     6747 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/steps.py
--rw-r--r--   0        0        0     3332 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/suspend.py
--rw-r--r--   0        0        0     7959 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/task.py
--rw-r--r--   0        0        0       27 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/toleration.py
--rw-r--r--   0        0        0     2007 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/user_container.py
--rw-r--r--   0        0        0     2610 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/validators.py
--rw-r--r--   0        0        0    18709 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/volume.py
--rw-r--r--   0        0        0    22337 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/workflow.py
--rw-r--r--   0        0        0      911 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/workflow_status.py
--rw-r--r--   0        0        0     7450 2023-06-26 15:59:36.640146 hera_workflows-5.5.2/src/hera/workflows/workflow_template.py
--rw-r--r--   0        0        0    13566 1970-01-01 00:00:00.000000 hera_workflows-5.5.2/PKG-INFO
+-rw-r--r--   0        0        0     1066 2023-07-19 11:15:37.161443 hera_workflows-5.6.0/LICENSE
+-rw-r--r--   0        0        0    12772 2023-07-19 11:15:37.161443 hera_workflows-5.6.0/README.md
+-rw-r--r--   0        0        0     3777 2023-07-19 11:16:02.342994 hera_workflows-5.6.0/pyproject.toml
+-rw-r--r--   0        0        0      523 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/__init__.py
+-rw-r--r--   0        0        0      310 2023-07-19 11:16:02.346994 hera_workflows-5.6.0/src/hera/_version.py
+-rw-r--r--   0        0        0     1669 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/auth/__init__.py
+-rw-r--r--   0        0        0       19 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/__init__.py
+-rw-r--r--   0        0        0     9964 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/__init__.py
+-rw-r--r--   0        0        0     1471 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/eventsource.py
+-rw-r--r--   0        0        0      859 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/eventsource.pyi
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/google/__init__.py
+-rw-r--r--   0        0        0      347 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/google/protobuf.py
+-rw-r--r--   0        0        0      163 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/google/protobuf.pyi
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/grpc/__init__.py
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/grpc/gateway/__init__.py
+-rw-r--r--   0        0        0      691 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/grpc/gateway/runtime.py
+-rw-r--r--   0        0        0      468 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/grpc/gateway/runtime.pyi
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/io/__init__.py
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/io/argoproj/__init__.py
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.181444 hera_workflows-5.6.0/src/hera/events/models/io/argoproj/events/__init__.py
+-rw-r--r--   0        0        0   108306 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/argoproj/events/v1alpha1.py
+-rw-r--r--   0        0        0    25269 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/argoproj/events/v1alpha1.pyi
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/argoproj/workflow/__init__.py
+-rw-r--r--   0        0        0   149956 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/argoproj/workflow/v1alpha1.py
+-rw-r--r--   0        0        0    31956 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/argoproj/workflow/v1alpha1.pyi
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/__init__.py
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/api/__init__.py
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/api/core/__init__.py
+-rw-r--r--   0        0        0   130826 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/api/core/v1.py
+-rw-r--r--   0        0        0    18002 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/api/core/v1.pyi
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/api/policy/__init__.py
+-rw-r--r--   0        0        0     1774 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/api/policy/v1beta1.py
+-rw-r--r--   0        0        0      374 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/api/policy/v1beta1.pyi
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/__init__.py
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/__init__.py
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/api/__init__.py
+-rw-r--r--   0        0        0     3160 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/api/resource.py
+-rw-r--r--   0        0        0      105 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/api/resource.pyi
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/apis/__init__.py
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/apis/meta/__init__.py
+-rw-r--r--   0        0        0    22549 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/apis/meta/v1.py
+-rw-r--r--   0        0        0     2063 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/apis/meta/v1.pyi
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/util/__init__.py
+-rw-r--r--   0        0        0      277 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/util/intstr.py
+-rw-r--r--   0        0        0      108 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/util/intstr.pyi
+-rw-r--r--   0        0        0     1506 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/sensor.py
+-rw-r--r--   0        0        0      885 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/models/sensor.pyi
+-rw-r--r--   0        0        0    28683 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/events/service.py
+-rw-r--r--   0        0        0     2870 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/exceptions/__init__.py
+-rw-r--r--   0        0        0      282 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/expr/__init__.py
+-rw-r--r--   0        0        0    12024 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/expr/_node.py
+-rw-r--r--   0        0        0      647 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/expr/_sprig.py
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/py.typed
+-rw-r--r--   0        0        0      294 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/shared/__init__.py
+-rw-r--r--   0        0        0      962 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/shared/_base_model.py
+-rw-r--r--   0        0        0     6574 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/shared/_global_config.py
+-rw-r--r--   0        0        0     1287 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/shared/serialization.py
+-rw-r--r--   0        0        0     4585 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/workflows/__init__.py
+-rw-r--r--   0        0        0     3990 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/workflows/_context.py
+-rw-r--r--   0        0        0     4741 2023-07-19 11:15:37.185444 hera_workflows-5.6.0/src/hera/workflows/_inspect.py
+-rw-r--r--   0        0        0    53573 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/_mixins.py
+-rw-r--r--   0        0        0    32887 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/_unparse.py
+-rw-r--r--   0        0        0     1587 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/archive.py
+-rw-r--r--   0        0        0     9011 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/artifact.py
+-rw-r--r--   0        0        0     3729 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/cluster_workflow_template.py
+-rw-r--r--   0        0        0     3924 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/container.py
+-rw-r--r--   0        0        0     7830 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/container_set.py
+-rw-r--r--   0        0        0     9060 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/cron_workflow.py
+-rw-r--r--   0        0        0     3127 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/dag.py
+-rw-r--r--   0        0        0     2919 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/data.py
+-rw-r--r--   0        0        0     7886 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/env.py
+-rw-r--r--   0        0        0     1572 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/env_from.py
+-rw-r--r--   0        0        0      686 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/exceptions.py
+-rw-r--r--   0        0        0     2679 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/http_template.py
+-rw-r--r--   0        0        0     4536 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/metrics.py
+-rw-r--r--   0        0        0    22100 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/__init__.py
+-rw-r--r--   0        0        0     1471 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/eventsource.py
+-rw-r--r--   0        0        0      859 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/eventsource.pyi
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/google/__init__.py
+-rw-r--r--   0        0        0      347 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/google/protobuf.py
+-rw-r--r--   0        0        0      163 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/google/protobuf.pyi
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/grpc/__init__.py
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/grpc/gateway/__init__.py
+-rw-r--r--   0        0        0      691 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/grpc/gateway/runtime.py
+-rw-r--r--   0        0        0      468 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/grpc/gateway/runtime.pyi
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/__init__.py
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/__init__.py
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/events/__init__.py
+-rw-r--r--   0        0        0   108306 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/events/v1alpha1.py
+-rw-r--r--   0        0        0    25269 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/events/v1alpha1.pyi
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/workflow/__init__.py
+-rw-r--r--   0        0        0   149956 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/workflow/v1alpha1.py
+-rw-r--r--   0        0        0    31956 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/workflow/v1alpha1.pyi
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/__init__.py
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/api/__init__.py
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/api/core/__init__.py
+-rw-r--r--   0        0        0   130826 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/api/core/v1.py
+-rw-r--r--   0        0        0    18002 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/api/core/v1.pyi
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/api/policy/__init__.py
+-rw-r--r--   0        0        0     1774 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/api/policy/v1beta1.py
+-rw-r--r--   0        0        0      374 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/api/policy/v1beta1.pyi
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/__init__.py
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/__init__.py
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/api/__init__.py
+-rw-r--r--   0        0        0     3160 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/api/resource.py
+-rw-r--r--   0        0        0      105 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/api/resource.pyi
+-rw-r--r--   0        0        0        0 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/__init__.py
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/meta/__init__.py
+-rw-r--r--   0        0        0    22549 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/meta/v1.py
+-rw-r--r--   0        0        0     2063 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/meta/v1.pyi
+-rw-r--r--   0        0        0      144 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/util/__init__.py
+-rw-r--r--   0        0        0      277 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/util/intstr.py
+-rw-r--r--   0        0        0      108 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/util/intstr.pyi
+-rw-r--r--   0        0        0     1506 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/sensor.py
+-rw-r--r--   0        0        0      885 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/models/sensor.pyi
+-rw-r--r--   0        0        0      902 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/operator.py
+-rw-r--r--   0        0        0     4088 2023-07-19 11:15:37.189445 hera_workflows-5.6.0/src/hera/workflows/parameter.py
+-rw-r--r--   0        0        0     1816 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/protocol.py
+-rw-r--r--   0        0        0     3898 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/resource.py
+-rw-r--r--   0        0        0     5827 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/resources.py
+-rw-r--r--   0        0        0     2851 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/retry_strategy.py
+-rw-r--r--   0        0        0     4474 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/runner.py
+-rw-r--r--   0        0        0    18288 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/script.py
+-rw-r--r--   0        0        0    57856 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/service.py
+-rw-r--r--   0        0        0     6793 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/steps.py
+-rw-r--r--   0        0        0     3370 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/suspend.py
+-rw-r--r--   0        0        0     8760 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/task.py
+-rw-r--r--   0        0        0      129 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/toleration.py
+-rw-r--r--   0        0        0     2258 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/user_container.py
+-rw-r--r--   0        0        0     2615 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/validators.py
+-rw-r--r--   0        0        0    21439 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/volume.py
+-rw-r--r--   0        0        0    22389 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/workflow.py
+-rw-r--r--   0        0        0     1113 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/workflow_status.py
+-rw-r--r--   0        0        0     7404 2023-07-19 11:15:37.193445 hera_workflows-5.6.0/src/hera/workflows/workflow_template.py
+-rw-r--r--   0        0        0    14205 1970-01-01 00:00:00.000000 hera_workflows-5.6.0/PKG-INFO
```

### Comparing `hera_workflows-5.5.2/LICENSE` & `hera_workflows-5.6.0/LICENSE`

 * *Files identical despite different names*

### Comparing `hera_workflows-5.5.2/README.md` & `hera_workflows-5.6.0/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -30,26 +30,24 @@
 [![Downloads](https://pepy.tech/badge/hera-workflows)](https://pepy.tech/project/hera-workflows)
 [![Downloads/month](https://pepy.tech/badge/hera-workflows/month)](https://pepy.tech/project/hera-workflows)
 [![Downloads/week](https://pepy.tech/badge/hera-workflows/week)](https://pepy.tech/project/hera-workflows)
 
 Hera is a Python framework for constructing and submitting Argo Workflows. The main goal of Hera is to make the Argo
 ecosystem accessible by simplifying workflow construction and submission.
 
-You can watch the introductory Hera presentation at the "Argo Workflows and Events Community Meeting 20 Oct
-2021" [here](https://www.youtube.com/watch?v=QETfzfVV-GY&t=181s)!
-
 # Table of content
 
 - [Hera](#hera)
 - [Table of content](#table-of-content)
 - [Requirements](#requirements)
 - [Installation](#installation)
 - [Examples](#examples)
     - [Single step script](#single-step-script)
     - [DAG diamond](#dag-diamond)
+- [Presentations](#presentations)
 - [Contributing](#contributing)
 - [Comparison](#comparison)
 
 # Requirements
 
 Hera requires an Argo server to be deployed to a Kubernetes cluster. Currently, Hera assumes that the Argo server sits
 behind an authentication layer that can authenticate workflow submission requests by using the Bearer token on the
@@ -136,14 +134,26 @@
         A >> [B, C] >> D
 
 w.create()
 ```
 
 See the [examples](./examples/) directory for a collection of Argo workflow construction and submission via Hera!
 
+# Presentations
+
+- [Argo Workflows and Events Community Meeting 20 Oct 2021 - Hera introductory presentation](https://youtu.be/QETfzfVV-GY?t=181)
+- [Argo Workflows and Events Community Meeting 15 June 2022 - Hera project update](https://youtu.be/sdkBDPOdQ-g?t=231)
+- [KubeCon/ArgoCon EU 2023 - Scaling gene therapy with Argo Workflows and Hera](https://www.youtube.com/watch?v=h2TEw8kd1Ds)
+- [Unsticking ourselves from Glue - Migrating PayIt's Data Pipelines to Argo Workflows and Hera](https://youtu.be/sSLFVIIEKcE?t=2088)
+
+# Blogs
+
+- [Hera introduction and motivation](https://www.dynotx.com/hera-the-missing-argo-workflows-python-sdk/)
+- [Dyno is scaling gene therapy research with cloud-native tools like Argo Workflows and Hera](https://www.dynotx.com/argo-workflows-hera/)
+
 # Contributing
 
 If you plan to submit contributions to Hera you can install Hera in a virtual environment managed by `poetry`:
 
 ```shell
 poetry install
 ```
```

### Comparing `hera_workflows-5.5.2/pyproject.toml` & `hera_workflows-5.6.0/pyproject.toml`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 [tool.poetry]
 name = "hera-workflows"  # project-name
 # The version is automatically substituted by the CI
-version = "5.5.2"
+version = "5.6.0"
 description = "Hera is a Python framework for constructing and submitting Argo Workflows. The main goal of Hera is to make Argo Workflows more accessible by abstracting away some setup that is typically necessary for constructing Argo workflows."
 authors = ["Flaviu Vadan <flaviu.vadan@dynotx.com>", "Sambhav Kothari <sambhavs.email@gmail.com>", "Elliot Gunton <elliotgunton@gmail.com>"]
 maintainers = ["Flaviu Vadan <flaviu.vadan@dynotx.com>", "Sambhav Kothari <sambhavs.email@gmail.com>", "Elliot Gunton <elliotgunton@gmail.com>"]
 license = "MIT"
 readme = "README.md"
 homepage = "https://github.com/argoproj-labs/hera"
 repository = "https://github.com/argoproj-labs/hera"
@@ -118,19 +118,30 @@
 source = [
     "src/hera",
 ]
 
 [tool.ruff]
 line-length = 119
 show-fixes = true
+select = ["E", "F", "D"]
 ignore = ["E501"]
 target-version = "py38"
 extend-select = ["I"]
-
+src = ["src"]
+exclude = [
+    "examples", 
+    "tests",
+    "src/hera/workflows/models", 
+    "src/hera/events/models",
+    "src/hera/workflows/_unparse.py",
+]
 
 [tool.ruff.per-file-ignores]
-"**/__init__.py" = ["F401"]
+"**/__init__.py" = ["F401", "D107"]
 
 [tool.ruff.isort]
 force-wrap-aliases = true
 combine-as-imports = true
 known-first-party = ["hera"]
+
+[tool.ruff.pydocstyle]
+convention = "google"
```

### Comparing `hera_workflows-5.5.2/src/hera/__init__.py` & `hera_workflows-5.6.0/src/hera/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,9 @@
 """Hera is a Python framework for constructing and submitting Argo Workflows.
+
 The main goal of Hera is to make the Argo ecosystem accessible by simplifying
 workflow construction and submission. Hera presents an intuitive Python interface
 to the underlying API of Argo, with custom classes making use of context managers
 and callables, empowering users to focus on their own executable payloads rather
 than workflow setup.
 """
```

### Comparing `hera_workflows-5.5.2/src/hera/auth/__init__.py` & `hera_workflows-5.6.0/src/hera/auth/__init__.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,36 +1,46 @@
+"""The auth module of Hera consists of authentication related implementation.
+
+The module provides basic functionality such as token generation via the Argo CLI, the basis of token generation for
+implementing a client token generator, etc.
+"""
+import os
+import random
 import shutil
 import subprocess
 
 
 class TokenGenerator:
     """A token generator can be used to generate tokens for authentication with Argo Workflows/Events APIs.
 
     A token generator can be set for invocation on the Hera global config via
     `hera.shared.global_config.token`.
     """
 
     def __call__(self) -> str:
-        """Generates an authentication token for use with Argo Workflows/Events APIs"""
+        """Generates an authentication token for use with Argo Workflows/Events APIs."""
         raise NotImplementedError("Implement me")
 
 
 class ArgoCLITokenGenerator(TokenGenerator):
     """A token generator that uses the Argo CLI to generate a token.
 
-    Note that this involves invoking the Argo CLI, which means that the Argo CLI must be installed on the machine.
+    Notes:
+    -----
+    This involves invoking the Argo CLI, which means that the Argo CLI must be installed on the machine.
     An exception is raised if this is not the case.
 
-    Raises
+    Raises:
     ------
     RuntimeError
         If the Argo CLI is not installed.
     """
 
     def __call__(self) -> str:
+        """Executes the call necessary to generate the token and returns the token as a string."""
         if shutil.which("argo") is None:
             raise RuntimeError(
                 "The Argo CLI is not installed. "
                 "See `https://argoproj.github.io/argo-workflows/walk-through/argo-cli/` for more information"
             )
 
         token = subprocess.check_output("argo auth token".split()).strip().decode()
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/eventsource.py` & `hera_workflows-5.6.0/src/hera/events/models/eventsource.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,45 +1,46 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from .io.argoproj.events import v1alpha1
 from .io.k8s.apimachinery.pkg.apis.meta import v1
 
 
 class EventSourceDeletedResponse(BaseModel):
     pass
 
 
 class LogEntry(BaseModel):
-    event_name: Optional[str] = Field(None, alias="eventName", title="optional - the event name (e.g. `example`)")
-    event_source_name: Optional[str] = Field(None, alias="eventSourceName")
+    event_name: Optional[str] = Field(
+        default=None, alias="eventName", title="optional - the event name (e.g. `example`)"
+    )
+    event_source_name: Optional[str] = Field(default=None, alias="eventSourceName")
     event_source_type: Optional[str] = Field(
-        None, alias="eventSourceType", title="optional - the event source type (e.g. `webhook`)"
+        default=None, alias="eventSourceType", title="optional - the event source type (e.g. `webhook`)"
     )
     level: Optional[str] = None
     msg: Optional[str] = None
     namespace: Optional[str] = None
     time: Optional[v1.Time] = None
 
 
 class CreateEventSourceRequest(BaseModel):
-    event_source: Optional[v1alpha1.EventSource] = Field(None, alias="eventSource")
+    event_source: Optional[v1alpha1.EventSource] = Field(default=None, alias="eventSource")
     namespace: Optional[str] = None
 
 
 class EventSourceWatchEvent(BaseModel):
     object: Optional[v1alpha1.EventSource] = None
     type: Optional[str] = None
 
 
 class UpdateEventSourceRequest(BaseModel):
-    event_source: Optional[v1alpha1.EventSource] = Field(None, alias="eventSource")
+    event_source: Optional[v1alpha1.EventSource] = Field(default=None, alias="eventSource")
     name: Optional[str] = None
     namespace: Optional[str] = None
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/eventsource.pyi` & `hera_workflows-5.6.0/src/hera/events/models/eventsource.pyi`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,11 @@
-from typing import Optional
-
-from hera.shared._base_model import BaseModel as BaseModel
-
 from .io.argoproj.events import v1alpha1 as v1alpha1
 from .io.k8s.apimachinery.pkg.apis.meta import v1 as v1
+from hera.shared._base_model import BaseModel as BaseModel
+from typing import Optional
 
 class EventSourceDeletedResponse(BaseModel): ...
 
 class LogEntry(BaseModel):
     event_name: Optional[str]
     event_source_name: Optional[str]
     event_source_type: Optional[str]
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/grpc/gateway/runtime.py` & `hera_workflows-5.6.0/src/hera/events/models/grpc/gateway/runtime.py`

 * *Files identical despite different names*

### Comparing `hera_workflows-5.5.2/src/hera/events/models/io/argoproj/events/v1alpha1.py` & `hera_workflows-5.6.0/src/hera/events/models/io/argoproj/events/v1alpha1.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,279 +1,295 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from typing import Dict, List, Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from ...k8s.api.core import v1
 from ...k8s.apimachinery.pkg.apis.meta import v1 as v1_1
 
 
 class AMQPConsumeConfig(BaseModel):
     auto_ack: Optional[bool] = Field(
-        None,
+        default=None,
         alias="autoAck",
         title=(
             "AutoAck when true, the server will acknowledge deliveries to this consumer prior to writing\nthe delivery"
             " to the network\n+optional"
         ),
     )
     consumer_tag: Optional[str] = Field(
-        None,
+        default=None,
         alias="consumerTag",
         title="ConsumerTag is the identity of the consumer included in every delivery\n+optional",
     )
     exclusive: Optional[bool] = Field(
-        None,
+        default=None,
         title="Exclusive when true, the server will ensure that this is the sole consumer from this queue\n+optional",
     )
     no_local: Optional[bool] = Field(
-        None, alias="noLocal", title="NoLocal flag is not supported by RabbitMQ\n+optional"
+        default=None, alias="noLocal", title="NoLocal flag is not supported by RabbitMQ\n+optional"
     )
     no_wait: Optional[bool] = Field(
-        None,
+        default=None,
         alias="noWait",
         title=(
             "NowWait when true, do not wait for the server to confirm the request and immediately begin"
             " deliveries\n+optional"
         ),
     )
 
 
 class AMQPExchangeDeclareConfig(BaseModel):
     auto_delete: Optional[bool] = Field(
-        None, alias="autoDelete", title="AutoDelete removes the exchange when no bindings are active\n+optional"
+        default=None,
+        alias="autoDelete",
+        title="AutoDelete removes the exchange when no bindings are active\n+optional",
     )
-    durable: Optional[bool] = Field(None, title="Durable keeps the exchange also after the server restarts\n+optional")
-    internal: Optional[bool] = Field(None, title="Internal when true does not accept publishings\n+optional")
+    durable: Optional[bool] = Field(
+        default=None, title="Durable keeps the exchange also after the server restarts\n+optional"
+    )
+    internal: Optional[bool] = Field(default=None, title="Internal when true does not accept publishings\n+optional")
     no_wait: Optional[bool] = Field(
-        None, alias="noWait", title="NowWait when true does not wait for a confirmation from the server\n+optional"
+        default=None,
+        alias="noWait",
+        title="NowWait when true does not wait for a confirmation from the server\n+optional",
     )
 
 
 class AMQPQueueBindConfig(BaseModel):
     no_wait: Optional[bool] = Field(
-        None,
+        default=None,
         alias="noWait",
         title="NowWait false and the queue could not be bound, the channel will be closed with an error\n+optional",
     )
 
 
 class AMQPQueueDeclareConfig(BaseModel):
     arguments: Optional[str] = Field(
-        None,
+        default=None,
         title='Arguments of a queue (also known as "x-arguments") used for optional features and plugins\n+optional',
     )
     auto_delete: Optional[bool] = Field(
-        None, alias="autoDelete", title="AutoDelete removes the queue when no consumers are active\n+optional"
+        default=None, alias="autoDelete", title="AutoDelete removes the queue when no consumers are active\n+optional"
+    )
+    durable: Optional[bool] = Field(
+        default=None, title="Durable keeps the queue also after the server restarts\n+optional"
     )
-    durable: Optional[bool] = Field(None, title="Durable keeps the queue also after the server restarts\n+optional")
     exclusive: Optional[bool] = Field(
-        None,
+        default=None,
         title=(
             "Exclusive sets the queues to be accessible only by the connection that declares them and will be\ndeleted"
             " wgen the connection closes\n+optional"
         ),
     )
     name: Optional[str] = Field(
-        None, title="Name of the queue. If empty the server auto-generates a unique name for this queue\n+optional"
+        default=None,
+        title="Name of the queue. If empty the server auto-generates a unique name for this queue\n+optional",
     )
     no_wait: Optional[bool] = Field(
-        None, alias="noWait", title="NowWait when true, the queue assumes to be declared on the server\n+optional"
+        default=None,
+        alias="noWait",
+        title="NowWait when true, the queue assumes to be declared on the server\n+optional",
     )
 
 
 class Amount(BaseModel):
     value: Optional[str] = None
 
 
 class BitbucketRepository(BaseModel):
-    owner: Optional[str] = Field(None, title="Owner is the owner of the repository")
+    owner: Optional[str] = Field(default=None, title="Owner is the owner of the repository")
     repository_slug: Optional[str] = Field(
-        None,
+        default=None,
         alias="repositorySlug",
         title=(
             "RepositorySlug is a URL-friendly version of a repository name, automatically generated by Bitbucket for"
             " use in the URL"
         ),
     )
 
 
 class BitbucketServerRepository(BaseModel):
     project_key: Optional[str] = Field(
-        None, alias="projectKey", title="ProjectKey is the key of project for which integration needs to set up"
+        default=None,
+        alias="projectKey",
+        title="ProjectKey is the key of project for which integration needs to set up",
     )
     repository_slug: Optional[str] = Field(
-        None,
+        default=None,
         alias="repositorySlug",
         title="RepositorySlug is the slug of the repository for which integration needs to set up",
     )
 
 
 class CatchupConfiguration(BaseModel):
     enabled: Optional[bool] = Field(
-        None, title="Enabled enables to triggered the missed schedule when eventsource restarts"
+        default=None, title="Enabled enables to triggered the missed schedule when eventsource restarts"
+    )
+    max_duration: Optional[str] = Field(
+        default=None, alias="maxDuration", title="MaxDuration holds max catchup duration"
     )
-    max_duration: Optional[str] = Field(None, alias="maxDuration", title="MaxDuration holds max catchup duration")
 
 
 class ConditionsResetByTime(BaseModel):
     cron: Optional[str] = Field(
-        None, title="Cron is a cron-like expression. For reference, see: https://en.wikipedia.org/wiki/Cron"
+        default=None, title="Cron is a cron-like expression. For reference, see: https://en.wikipedia.org/wiki/Cron"
     )
-    timezone: Optional[str] = Field(None, title="+optional")
+    timezone: Optional[str] = Field(default=None, title="+optional")
 
 
 class ConditionsResetCriteria(BaseModel):
     by_time: Optional[ConditionsResetByTime] = Field(
-        None,
+        default=None,
         alias="byTime",
         title="Schedule is a cron-like expression. For reference, see: https://en.wikipedia.org/wiki/Cron",
     )
 
 
 class ConfigMapPersistence(BaseModel):
     create_if_not_exist: Optional[bool] = Field(
-        None, alias="createIfNotExist", title="CreateIfNotExist will create configmap if it doesn't exists"
+        default=None, alias="createIfNotExist", title="CreateIfNotExist will create configmap if it doesn't exists"
     )
-    name: Optional[str] = Field(None, title="Name of the configmap")
+    name: Optional[str] = Field(default=None, title="Name of the configmap")
 
 
 class DataFilter(BaseModel):
     comparator: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Comparator compares the event data with a user given value.\nCan be ">=", ">", "=", "!=", "<", or'
             ' "<=".\nIs optional, and if left blank treated as equality "=".'
         ),
     )
     path: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Path is the JSONPath of the event's (JSON decoded) data key\nPath is a series of keys separated by a dot."
             " A key may contain wildcard characters '*' and '?'.\nTo access an array value use the index as the key."
             " The dot and wildcard characters can be escaped with '\\\\'.\nSee"
             " https://github.com/tidwall/gjson#path-syntax for more information on how to use this."
         ),
     )
     template: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Template is a go-template for extracting a string from the event's data.\nA Template is evaluated with"
             " provided path, type and value.\nThe templating follows the standard go-template syntax as well as"
             " sprig's extra functions.\nSee https://pkg.go.dev/text/template and https://masterminds.github.io/sprig/"
         ),
     )
-    type: Optional[str] = Field(None, title="Type contains the JSON type of the data")
+    type: Optional[str] = Field(default=None, title="Type contains the JSON type of the data")
     value: Optional[List[str]] = Field(
-        None,
+        default=None,
         title=(
             "Value is the allowed string values for this key\nBooleans are passed using strconv.ParseBool()\nNumbers"
             " are parsed using as float64 using strconv.ParseFloat()\nStrings are taken as is\nNils this value is"
             " ignored"
         ),
     )
 
 
 class EventDependencyTransformer(BaseModel):
-    jq: Optional[str] = Field(None, title="JQ holds the jq command applied for transformation\n+optional")
-    script: Optional[str] = Field(None, title="Script refers to a Lua script used to transform the event\n+optional")
+    jq: Optional[str] = Field(default=None, title="JQ holds the jq command applied for transformation\n+optional")
+    script: Optional[str] = Field(
+        default=None, title="Script refers to a Lua script used to transform the event\n+optional"
+    )
 
 
 class EventPersistence(BaseModel):
     catchup: Optional[CatchupConfiguration] = Field(
-        None, title="Catchup enables to triggered the missed schedule when eventsource restarts"
+        default=None, title="Catchup enables to triggered the missed schedule when eventsource restarts"
     )
     config_map: Optional[ConfigMapPersistence] = Field(
-        None, alias="configMap", title="ConfigMap holds configmap details for persistence"
+        default=None, alias="configMap", title="ConfigMap holds configmap details for persistence"
     )
 
 
 class EventSourceFilter(BaseModel):
     expression: Optional[str] = None
 
 
 class FileArtifact(BaseModel):
     path: Optional[str] = None
 
 
 class GitRemoteConfig(BaseModel):
-    name: Optional[str] = Field(None, description="Name of the remote to fetch from.")
+    name: Optional[str] = Field(default=None, description="Name of the remote to fetch from.")
     urls: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "URLs the URLs of a remote repository. It must be non-empty. Fetch will\nalways use the first URL, while"
             " push will use all of them."
         ),
     )
 
 
 class Int64OrString(BaseModel):
-    int64_val: Optional[str] = Field(None, alias="int64Val")
-    str_val: Optional[str] = Field(None, alias="strVal")
+    int64_val: Optional[str] = Field(default=None, alias="int64Val")
+    str_val: Optional[str] = Field(default=None, alias="strVal")
     type: Optional[str] = None
 
 
 class KafkaConsumerGroup(BaseModel):
-    group_name: Optional[str] = Field(None, alias="groupName", title="The name for the consumer group to use")
+    group_name: Optional[str] = Field(default=None, alias="groupName", title="The name for the consumer group to use")
     oldest: Optional[bool] = Field(
-        None,
+        default=None,
         title=(
             "When starting up a new group do we want to start from the oldest event (true) or the newest event"
             " (false), defaults to false\n+optional"
         ),
     )
     rebalance_strategy: Optional[str] = Field(
-        None,
+        default=None,
         alias="rebalanceStrategy",
         title="Rebalance strategy can be one of: sticky, roundrobin, range. Range is the default.\n+optional",
     )
 
 
 class LogTrigger(BaseModel):
     interval_seconds: Optional[str] = Field(
-        None,
+        default=None,
         alias="intervalSeconds",
         title=(
             "Only print messages every interval. Useful to prevent logging too much data for busy events.\n+optional"
         ),
     )
 
 
 class Metadata(BaseModel):
     annotations: Optional[Dict[str, str]] = None
     labels: Optional[Dict[str, str]] = None
 
 
 class OwnedRepositories(BaseModel):
-    names: Optional[List[str]] = Field(None, title="Repository names")
-    owner: Optional[str] = Field(None, title="Organization or user name")
+    names: Optional[List[str]] = Field(default=None, title="Repository names")
+    owner: Optional[str] = Field(default=None, title="Organization or user name")
 
 
 class PayloadField(BaseModel):
-    name: Optional[str] = Field(None, description="Name acts as key that holds the value at the path.")
+    name: Optional[str] = Field(default=None, description="Name acts as key that holds the value at the path.")
     path: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Path is the JSONPath of the event's (JSON decoded) data key\nPath is a series of keys separated by a dot."
             " A key may contain wildcard characters '*' and '?'.\nTo access an array value use the index as the key."
             " The dot and wildcard characters can be escaped with '\\\\'.\nSee"
             " https://github.com/tidwall/gjson#path-syntax for more information on how to use this."
         ),
     )
 
 
 class RateLimit(BaseModel):
-    requests_per_unit: Optional[int] = Field(None, alias="requestsPerUnit")
-    unit: Optional[str] = Field(None, title="Defaults to Second")
+    requests_per_unit: Optional[int] = Field(default=None, alias="requestsPerUnit")
+    unit: Optional[str] = Field(default=None, title="Defaults to Second")
 
 
 class Resource(BaseModel):
     value: Optional[str] = None
 
 
 class S3Bucket(BaseModel):
@@ -283,2050 +299,2151 @@
 
 class S3Filter(BaseModel):
     prefix: Optional[str] = None
     suffix: Optional[str] = None
 
 
 class Selector(BaseModel):
-    key: Optional[str] = Field(None, title="Key name")
+    key: Optional[str] = Field(default=None, title="Key name")
     operation: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Supported operations like ==, !=, <=, >= etc.\nDefaults to ==.\nRefer"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors for more"
             " io.argoproj.workflow.v1alpha1.\n+optional"
         ),
     )
-    value: Optional[str] = Field(None, title="Value")
+    value: Optional[str] = Field(default=None, title="Value")
 
 
 class StatusPolicy(BaseModel):
     allow: Optional[List[int]] = None
 
 
 class StorageGridFilter(BaseModel):
     prefix: Optional[str] = None
     suffix: Optional[str] = None
 
 
 class TimeFilter(BaseModel):
     start: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Start is the beginning of a time window in UTC.\nBefore this time, events for this dependency are"
             " ignored.\nFormat is hh:mm:ss."
         ),
     )
     stop: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Stop is the end of a time window in UTC.\nAfter or equal to this time, events for this dependency are"
             " ignored and\nFormat is hh:mm:ss.\nIf it is smaller than Start, it is treated as next day of"
             " Start\n(e.g.: 22:00:00-01:00:00 means 22:00:00-25:00:00)."
         ),
     )
 
 
 class TriggerParameterSource(BaseModel):
     context_key: Optional[str] = Field(
-        None,
+        default=None,
         alias="contextKey",
         description=(
             "ContextKey is the JSONPath of the event's (JSON decoded) context key\nContextKey is a series of keys"
             " separated by a dot. A key may contain wildcard characters '*' and '?'.\nTo access an array value use the"
             " index as the key. The dot and wildcard characters can be escaped with '\\\\'.\nSee"
             " https://github.com/tidwall/gjson#path-syntax for more information on how to use this."
         ),
     )
     context_template: Optional[str] = Field(
-        None,
+        default=None,
         alias="contextTemplate",
         title=(
             "ContextTemplate is a go-template for extracting a string from the event's context.\nIf a ContextTemplate"
             " is provided with a ContextKey, the template will be evaluated first and fallback to the ContextKey.\nThe"
             " templating follows the standard go-template syntax as well as sprig's extra functions.\nSee"
             " https://pkg.go.dev/text/template and https://masterminds.github.io/sprig/"
         ),
     )
     data_key: Optional[str] = Field(
-        None,
+        default=None,
         alias="dataKey",
         description=(
             "DataKey is the JSONPath of the event's (JSON decoded) data key\nDataKey is a series of keys separated by"
             " a dot. A key may contain wildcard characters '*' and '?'.\nTo access an array value use the index as the"
             " key. The dot and wildcard characters can be escaped with '\\\\'.\nSee"
             " https://github.com/tidwall/gjson#path-syntax for more information on how to use this."
         ),
     )
     data_template: Optional[str] = Field(
-        None,
+        default=None,
         alias="dataTemplate",
         title=(
             "DataTemplate is a go-template for extracting a string from the event's data.\nIf a DataTemplate is"
             " provided with a DataKey, the template will be evaluated first and fallback to the DataKey.\nThe"
             " templating follows the standard go-template syntax as well as sprig's extra functions.\nSee"
             " https://pkg.go.dev/text/template and https://masterminds.github.io/sprig/"
         ),
     )
     dependency_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="dependencyName",
         description=(
             "DependencyName refers to the name of the dependency. The event which is stored for this dependency is"
             " used as payload\nfor the parameterization. Make sure to refer to one of the dependencies you have"
             " defined under Dependencies list."
         ),
     )
     value: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Value is the default literal value to use for this parameter source\nThis is only used if the DataKey is"
             " invalid.\nIf the DataKey is invalid and this is not defined, this param source will produce an error."
         ),
     )
 
 
 class URLArtifact(BaseModel):
-    path: Optional[str] = Field(None, title="Path is the complete URL")
+    path: Optional[str] = Field(default=None, title="Path is the complete URL")
     verify_cert: Optional[bool] = Field(
-        None, alias="verifyCert", title="VerifyCert decides whether the connection is secure or not"
+        default=None, alias="verifyCert", title="VerifyCert decides whether the connection is secure or not"
     )
 
 
 class WatchPathConfig(BaseModel):
-    directory: Optional[str] = Field(None, title="Directory to watch for events")
-    path: Optional[str] = Field(None, title="Path is relative path of object to watch with respect to the directory")
+    directory: Optional[str] = Field(default=None, title="Directory to watch for events")
+    path: Optional[str] = Field(
+        default=None, title="Path is relative path of object to watch with respect to the directory"
+    )
     path_regexp: Optional[str] = Field(
-        None,
+        default=None,
         alias="pathRegexp",
         title="PathRegexp is regexp of relative path of object to watch with respect to the directory",
     )
 
 
 class AzureEventsHubEventSource(BaseModel):
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     fqdn: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "FQDN of the EventHubs namespace you created\nMore info at"
             " https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-get-connection-string"
         ),
     )
-    hub_name: Optional[str] = Field(None, alias="hubName", title="Event Hub path/name")
+    hub_name: Optional[str] = Field(default=None, alias="hubName", title="Event Hub path/name")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     shared_access_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="sharedAccessKey", title="SharedAccessKey is the generated value of the key"
+        default=None, alias="sharedAccessKey", title="SharedAccessKey is the generated value of the key"
     )
     shared_access_key_name: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="sharedAccessKeyName",
         title="SharedAccessKeyName is the name you chose for your application's SAS keys",
     )
 
 
 class Backoff(BaseModel):
     duration: Optional[Int64OrString] = Field(
-        None, title='The initial duration in nanoseconds or strings like "1s", "3m"\n+optional'
+        default=None, title='The initial duration in nanoseconds or strings like "1s", "3m"\n+optional'
     )
-    factor: Optional[Amount] = Field(None, title="Duration is multiplied by factor each iteration\n+optional")
-    jitter: Optional[Amount] = Field(None, title="The amount of jitter applied each iteration\n+optional")
-    steps: Optional[int] = Field(None, title="Exit with error after this many steps\n+optional")
+    factor: Optional[Amount] = Field(default=None, title="Duration is multiplied by factor each iteration\n+optional")
+    jitter: Optional[Amount] = Field(default=None, title="The amount of jitter applied each iteration\n+optional")
+    steps: Optional[int] = Field(default=None, title="Exit with error after this many steps\n+optional")
 
 
 class BasicAuth(BaseModel):
     password: Optional[v1.SecretKeySelector] = Field(
-        None, description="Password refers to the Kubernetes secret that holds the password required for basic auth."
+        default=None,
+        description="Password refers to the Kubernetes secret that holds the password required for basic auth.",
     )
     username: Optional[v1.SecretKeySelector] = Field(
-        None, description="Username refers to the Kubernetes secret that holds the username required for basic auth."
+        default=None,
+        description="Username refers to the Kubernetes secret that holds the username required for basic auth.",
     )
 
 
 class BitbucketBasicAuth(BaseModel):
     password: Optional[v1.SecretKeySelector] = Field(
-        None, description="Password refers to the K8s secret that holds the password."
+        default=None, description="Password refers to the K8s secret that holds the password."
     )
     username: Optional[v1.SecretKeySelector] = Field(
-        None, description="Username refers to the K8s secret that holds the username."
+        default=None, description="Username refers to the K8s secret that holds the username."
     )
 
 
 class CalendarEventSource(BaseModel):
     exclusion_dates: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="exclusionDates",
         description="ExclusionDates defines the list of DATE-TIME exceptions for recurring events.",
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     interval: Optional[str] = Field(
-        None, title="Interval is a string that describes an interval duration, e.g. 1s, 30m, 2h...\n+optional"
+        default=None, title="Interval is a string that describes an interval duration, e.g. 1s, 30m, 2h...\n+optional"
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     persistence: Optional[EventPersistence] = Field(
-        None, title="Persistence hold the configuration for event persistence"
+        default=None, title="Persistence hold the configuration for event persistence"
     )
     schedule: Optional[str] = Field(
-        None,
+        default=None,
         title="Schedule is a cron-like expression. For reference, see: https://en.wikipedia.org/wiki/Cron\n+optional",
     )
-    timezone: Optional[str] = Field(None, title="Timezone in which to run the schedule\n+optional")
+    timezone: Optional[str] = Field(default=None, title="Timezone in which to run the schedule\n+optional")
 
 
 class Condition(BaseModel):
     last_transition_time: Optional[v1_1.Time] = Field(
-        None,
+        default=None,
         alias="lastTransitionTime",
         title="Last time the condition transitioned from one status to another.\n+optional",
     )
     message: Optional[str] = Field(
-        None, title="Human-readable message indicating details about last transition.\n+optional"
+        default=None, title="Human-readable message indicating details about last transition.\n+optional"
     )
     reason: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Unique, this should be a short, machine understandable string that gives the reason\nfor condition's"
             ' last transition. For example, "ImageNotFound"\n+optional'
         ),
     )
-    status: Optional[str] = Field(None, title="Condition status, True, False or Unknown.\n+required")
-    type: Optional[str] = Field(None, title="Condition type.\n+required")
+    status: Optional[str] = Field(default=None, title="Condition status, True, False or Unknown.\n+required")
+    type: Optional[str] = Field(default=None, title="Condition type.\n+required")
 
 
 class EventContext(BaseModel):
     datacontenttype: Optional[str] = Field(
-        None, description="DataContentType - A MIME (RFC2046) string describing the media type of `data`."
+        default=None, description="DataContentType - A MIME (RFC2046) string describing the media type of `data`."
     )
     id: Optional[str] = Field(
-        None, description="ID of the event; must be non-empty and unique within the scope of the producer."
+        default=None, description="ID of the event; must be non-empty and unique within the scope of the producer."
     )
-    source: Optional[str] = Field(None, description="Source - A URI describing the event producer.")
+    source: Optional[str] = Field(default=None, description="Source - A URI describing the event producer.")
     specversion: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "SpecVersion - The version of the CloudEvents specification used by the io.argoproj.workflow.v1alpha1."
         ),
     )
     subject: Optional[str] = Field(
-        None, title="Subject - The subject of the event in the context of the event producer"
+        default=None, title="Subject - The subject of the event in the context of the event producer"
     )
-    time: Optional[v1_1.Time] = Field(None, description="Time - A Timestamp when the event happened.")
-    type: Optional[str] = Field(None, description="Type - The type of the occurrence which has happened.")
+    time: Optional[v1_1.Time] = Field(default=None, description="Time - A Timestamp when the event happened.")
+    type: Optional[str] = Field(default=None, description="Type - The type of the occurrence which has happened.")
 
 
 class ExprFilter(BaseModel):
     expr: Optional[str] = Field(
-        None, description="Expr refers to the expression that determines the outcome of the filter."
+        default=None, description="Expr refers to the expression that determines the outcome of the filter."
     )
     fields: Optional[List[PayloadField]] = Field(
-        None, description="Fields refers to set of keys that refer to the paths within event payload."
+        default=None, description="Fields refers to set of keys that refer to the paths within event payload."
     )
 
 
 class FileEventSource(BaseModel):
     event_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="eventType",
         title=(
             "Type of file operations to watch\nRefer https://github.com/fsnotify/fsnotify/blob/master/fsnotify.go for"
             " more information"
         ),
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    polling: Optional[bool] = Field(None, title="Use polling instead of inotify")
+    polling: Optional[bool] = Field(default=None, title="Use polling instead of inotify")
     watch_path_config: Optional[WatchPathConfig] = Field(
-        None, alias="watchPathConfig", title="WatchPathConfig contains configuration about the file path to watch"
+        default=None,
+        alias="watchPathConfig",
+        title="WatchPathConfig contains configuration about the file path to watch",
     )
 
 
 class GenericEventSource(BaseModel):
     auth_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="authSecret",
         title="AuthSecret holds a secret selector that contains a bearer token for authentication\n+optional",
     )
-    config: Optional[str] = Field(None, title="Config is the event source configuration")
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
-    insecure: Optional[bool] = Field(None, description="Insecure determines the type of connection.")
+    config: Optional[str] = Field(default=None, title="Config is the event source configuration")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
+    insecure: Optional[bool] = Field(default=None, description="Insecure determines the type of connection.")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    url: Optional[str] = Field(None, description="URL of the gRPC server that implements the event source.")
+    url: Optional[str] = Field(default=None, description="URL of the gRPC server that implements the event source.")
 
 
 class GitCreds(BaseModel):
     password: Optional[v1.SecretKeySelector] = None
     username: Optional[v1.SecretKeySelector] = None
 
 
 class GithubAppCreds(BaseModel):
     app_id: Optional[str] = Field(
-        None, alias="appID", title="AppID refers to the GitHub App ID for the application you created"
+        default=None, alias="appID", title="AppID refers to the GitHub App ID for the application you created"
     )
     installation_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="installationID",
         title="InstallationID refers to the Installation ID of the GitHub app you created and installed",
     )
     private_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="privateKey", title="PrivateKey refers to a K8s secret containing the GitHub app private key"
+        default=None,
+        alias="privateKey",
+        title="PrivateKey refers to a K8s secret containing the GitHub app private key",
     )
 
 
 class HDFSEventSource(BaseModel):
     addresses: Optional[List[str]] = None
     check_interval: Optional[str] = Field(
-        None,
+        default=None,
         alias="checkInterval",
         title=(
             "CheckInterval is a string that describes an interval duration to check the directory state, e.g. 1s, 30m,"
             " 2h... (defaults to 1m)"
         ),
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     hdfs_user: Optional[str] = Field(
-        None,
+        default=None,
         alias="hdfsUser",
         description=(
             "HDFSUser is the user to access HDFS file system.\nIt is ignored if either ccache or keytab is used."
         ),
     )
     krb_c_cache_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbCCacheSecret",
         description=(
             "KrbCCacheSecret is the secret selector for Kerberos ccache\nEither ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_config_config_map: Optional[v1.ConfigMapKeySelector] = Field(
-        None,
+        default=None,
         alias="krbConfigConfigMap",
         description=(
             "KrbConfig is the configmap selector for Kerberos config as string\nIt must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_keytab_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbKeytabSecret",
         description=(
             "KrbKeytabSecret is the secret selector for Kerberos keytab\nEither ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_realm: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbRealm",
         description="KrbRealm is the Kerberos realm used with Kerberos keytab\nIt must be set if keytab is used.",
     )
     krb_service_principal_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbServicePrincipalName",
         description=(
             "KrbServicePrincipalName is the principal name of Kerberos service\nIt must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_username: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbUsername",
         description=(
             "KrbUsername is the Kerberos username used with Kerberos keytab\nIt must be set if keytab is used."
         ),
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    type: Optional[str] = Field(None, title="Type of file operations to watch")
-    watch_path_config: Optional[WatchPathConfig] = Field(None, alias="watchPathConfig")
+    type: Optional[str] = Field(default=None, title="Type of file operations to watch")
+    watch_path_config: Optional[WatchPathConfig] = Field(default=None, alias="watchPathConfig")
 
 
 class K8SResourcePolicy(BaseModel):
-    backoff: Optional[Backoff] = Field(None, title="Backoff before checking resource state")
+    backoff: Optional[Backoff] = Field(default=None, title="Backoff before checking resource state")
     error_on_backoff_timeout: Optional[bool] = Field(
-        None,
+        default=None,
         alias="errorOnBackoffTimeout",
         title=(
             "ErrorOnBackoffTimeout determines whether sensor should transition to error state if the trigger policy is"
             " unable to determine\nthe state of the resource"
         ),
     )
     labels: Optional[Dict[str, str]] = Field(
-        None, title="Labels required to identify whether a resource is in success state"
+        default=None, title="Labels required to identify whether a resource is in success state"
     )
 
 
 class NATSAuth(BaseModel):
-    basic: Optional[BasicAuth] = Field(None, title="Baisc auth with username and password\n+optional")
-    credential: Optional[v1.SecretKeySelector] = Field(None, title="credential used to connect\n+optional")
-    nkey: Optional[v1.SecretKeySelector] = Field(None, title="NKey used to connect\n+optional")
-    token: Optional[v1.SecretKeySelector] = Field(None, title="Token used to connect\n+optional")
+    basic: Optional[BasicAuth] = Field(default=None, title="Baisc auth with username and password\n+optional")
+    credential: Optional[v1.SecretKeySelector] = Field(default=None, title="credential used to connect\n+optional")
+    nkey: Optional[v1.SecretKeySelector] = Field(default=None, title="NKey used to connect\n+optional")
+    token: Optional[v1.SecretKeySelector] = Field(default=None, title="Token used to connect\n+optional")
 
 
 class PubSubEventSource(BaseModel):
     credential_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="credentialSecret",
         title=(
             "CredentialSecret references to the secret that contains JSON credentials to access GCP.\nIf it is"
             " missing, it implicitly uses Workload Identity to"
             " access.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity\n+optional"
         ),
     )
     delete_subscription_on_finish: Optional[bool] = Field(
-        None,
+        default=None,
         alias="deleteSubscriptionOnFinish",
         title=(
             "DeleteSubscriptionOnFinish determines whether to delete the GCP PubSub subscription once the event source"
             " is stopped.\n+optional"
         ),
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     project_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="projectID",
         title=(
             "ProjectID is GCP project ID for the subscription.\nRequired if you run Argo Events outside of"
             " GKE/GCE.\n(otherwise, the default value is its project)\n+optional"
         ),
     )
     subscription_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="subscriptionID",
         title=(
             "SubscriptionID is ID of subscription.\nRequired if you use existing subscription.\nThe default value will"
             " be auto generated hash based on this eventsource setting, so the subscription\nmight be recreated every"
             " time you update the setting, which has a possibility of event loss.\n+optional"
         ),
     )
     topic: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Topic to which the subscription should belongs.\nRequired if you want the eventsource to create a new"
             " subscription.\nIf you specify this field along with an existing subscription,\nit will be verified"
             " whether it actually belongs to the specified topic.\n+optional"
         ),
     )
     topic_project_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="topicProjectID",
         title="TopicProjectID is GCP project ID for the topic.\nBy default, it is same as ProjectID.\n+optional",
     )
 
 
 class ResourceFilter(BaseModel):
     after_start: Optional[bool] = Field(
-        None,
+        default=None,
         alias="afterStart",
         title="If the resource is created after the start time then the event is treated as valid.\n+optional",
     )
     created_by: Optional[v1_1.Time] = Field(
-        None,
+        default=None,
         alias="createdBy",
         title="If resource is created before the specified time then the event is treated as valid.\n+optional",
     )
     fields: Optional[List[Selector]] = Field(
-        None,
+        default=None,
         title=(
             "Fields provide field filters similar to K8s field selector\n(see"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/).\nUnlike K8s field"
             ' selector, it supports arbitrary fileds like "spec.serviceAccountName",\nand the value could be a string'
             ' or a regex.\nSame as K8s field selector, operator "=", "==" and "!=" are supported.\n+optional'
         ),
     )
     labels: Optional[List[Selector]] = Field(
-        None,
+        default=None,
         title=(
             "Labels provide listing options to K8s API to watch resource/s.\nRefer"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/label-selectors/ for more"
             " io.argoproj.workflow.v1alpha1.\n+optional"
         ),
     )
-    prefix: Optional[str] = Field(None, title="Prefix filter is applied on the resource name.\n+optional")
+    prefix: Optional[str] = Field(default=None, title="Prefix filter is applied on the resource name.\n+optional")
 
 
 class S3Artifact(BaseModel):
-    access_key: Optional[v1.SecretKeySelector] = Field(None, alias="accessKey")
+    access_key: Optional[v1.SecretKeySelector] = Field(default=None, alias="accessKey")
     bucket: Optional[S3Bucket] = None
     endpoint: Optional[str] = None
     events: Optional[List[str]] = None
     filter: Optional[S3Filter] = None
     insecure: Optional[bool] = None
     metadata: Optional[Dict[str, str]] = None
     region: Optional[str] = None
-    secret_key: Optional[v1.SecretKeySelector] = Field(None, alias="secretKey")
+    secret_key: Optional[v1.SecretKeySelector] = Field(default=None, alias="secretKey")
 
 
 class SASLConfig(BaseModel):
     mechanism: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "SASLMechanism is the name of the enabled SASL mechanism.\nPossible values: OAUTHBEARER, PLAIN (defaults"
             " to PLAIN).\n+optional"
         ),
     )
-    password: Optional[v1.SecretKeySelector] = Field(None, title="Password for SASL/PLAIN authentication")
+    password: Optional[v1.SecretKeySelector] = Field(default=None, title="Password for SASL/PLAIN authentication")
     user: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         title="User is the authentication identity (authcid) to present for\nSASL/PLAIN or SASL/SCRAM authentication",
     )
 
 
 class SQSEventSource(BaseModel):
     access_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key"
+        default=None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key"
     )
     dlq: Optional[bool] = Field(
-        None,
+        default=None,
         title=(
             "DLQ specifies if a dead-letter queue is configured for messages that can't be processed successfully.\nIf"
             " set to true, messages with invalid payload won't be acknowledged to allow to forward them farther to the"
             " dead-letter queue.\nThe default value is false.\n+optional"
         ),
     )
     endpoint: Optional[str] = Field(
-        None, title="Endpoint configures connection to a specific SQS endpoint instead of Amazons servers\n+optional"
+        default=None,
+        title="Endpoint configures connection to a specific SQS endpoint instead of Amazons servers\n+optional",
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    queue: Optional[str] = Field(None, title="Queue is AWS SQS queue to listen to for messages")
+    queue: Optional[str] = Field(default=None, title="Queue is AWS SQS queue to listen to for messages")
     queue_account_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="queueAccountId",
         title="QueueAccountID is the ID of the account that created the queue to monitor\n+optional",
     )
-    region: Optional[str] = Field(None, title="Region is AWS region")
+    region: Optional[str] = Field(default=None, title="Region is AWS region")
     role_arn: Optional[str] = Field(
-        None, alias="roleARN", title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional"
+        default=None,
+        alias="roleARN",
+        title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional",
     )
     secret_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key"
+        default=None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key"
     )
     session_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="sessionToken",
         title="SessionToken refers to K8s secret containing AWS temporary credentials(STS) session token\n+optional",
     )
     wait_time_seconds: Optional[str] = Field(
-        None,
+        default=None,
         alias="waitTimeSeconds",
         description=(
             "WaitTimeSeconds is The duration (in seconds) for which the call waits for a message to arrive\nin the"
             " queue before returning."
         ),
     )
 
 
 class Status(BaseModel):
     conditions: Optional[List[Condition]] = Field(
-        None,
+        default=None,
         title=(
             "Conditions are the latest available observations of a resource's current"
             " state.\n+optional\n+patchMergeKey=type\n+patchStrategy=merge"
         ),
     )
 
 
 class TLSConfig(BaseModel):
     ca_cert_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="caCertSecret", title="CACertSecret refers to the secret that contains the CA cert"
+        default=None, alias="caCertSecret", title="CACertSecret refers to the secret that contains the CA cert"
     )
     client_cert_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="clientCertSecret", title="ClientCertSecret refers to the secret that contains the client cert"
+        default=None,
+        alias="clientCertSecret",
+        title="ClientCertSecret refers to the secret that contains the client cert",
     )
     client_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="clientKeySecret", title="ClientKeySecret refers to the secret that contains the client key"
+        default=None,
+        alias="clientKeySecret",
+        title="ClientKeySecret refers to the secret that contains the client key",
     )
     insecure_skip_verify: Optional[bool] = Field(
-        None,
+        default=None,
         alias="insecureSkipVerify",
         title=(
             "If true, skips creation of TLSConfig with certs and creates an empty TLSConfig. (Defaults to"
             " false)\n+optional"
         ),
     )
 
 
 class TriggerParameter(BaseModel):
     dest: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Dest is the JSONPath of a resource key.\nA path is a series of keys separated by a dot. The colon"
             " character can be escaped with '.'\nThe -1 key can be used to append a value to an existing array.\nSee"
             " https://github.com/tidwall/sjson#path-syntax for more information about how this is used."
         ),
     )
     operation: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Operation is what to do with the existing value at Dest, whether to\n'prepend', 'overwrite', or"
             " 'append' it."
         ),
     )
     src: Optional[TriggerParameterSource] = Field(
-        None, title="Src contains a source reference to the value of the parameter from a dependency"
+        default=None, title="Src contains a source reference to the value of the parameter from a dependency"
     )
 
 
 class TriggerPolicy(BaseModel):
     k8s: Optional[K8SResourcePolicy] = Field(
-        None,
+        default=None,
         title=(
             "K8SResourcePolicy refers to the policy used to check the state of K8s based triggers using using labels"
         ),
     )
     status: Optional[StatusPolicy] = Field(
-        None, title="Status refers to the policy used to check the state of the trigger using response status"
+        default=None, title="Status refers to the policy used to check the state of the trigger using response status"
     )
 
 
 class ValueFromSource(BaseModel):
-    config_map_key_ref: Optional[v1.ConfigMapKeySelector] = Field(None, alias="configMapKeyRef")
-    secret_key_ref: Optional[v1.SecretKeySelector] = Field(None, alias="secretKeyRef")
+    config_map_key_ref: Optional[v1.ConfigMapKeySelector] = Field(default=None, alias="configMapKeyRef")
+    secret_key_ref: Optional[v1.SecretKeySelector] = Field(default=None, alias="secretKeyRef")
 
 
 class WebhookContext(BaseModel):
     auth_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="authSecret",
         title="AuthSecret holds a secret selector that contains a bearer token for authentication\n+optional",
     )
-    endpoint: Optional[str] = Field(None, title="REST API endpoint")
+    endpoint: Optional[str] = Field(default=None, title="REST API endpoint")
     max_payload_size: Optional[str] = Field(
-        None,
+        default=None,
         alias="maxPayloadSize",
         title=(
             "MaxPayloadSize is the maximum webhook payload size that the server will accept.\nRequests exceeding that"
             ' limit will be rejected with "request too large" response.\nDefault value: 1048576 (1MB).\n+optional'
         ),
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     method: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Method is HTTP request method that indicates the desired action to be performed for a given"
             " resource.\nSee RFC7231 Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content"
         ),
     )
-    port: Optional[str] = Field(None, description="Port on which HTTP server is listening for incoming events.")
+    port: Optional[str] = Field(
+        default=None, description="Port on which HTTP server is listening for incoming events."
+    )
     server_cert_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="serverCertSecret", description="ServerCertPath refers the file that contains the cert."
+        default=None, alias="serverCertSecret", description="ServerCertPath refers the file that contains the cert."
     )
     server_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="serverKeySecret", title="ServerKeyPath refers the file that contains private key"
+        default=None, alias="serverKeySecret", title="ServerKeyPath refers the file that contains private key"
     )
-    url: Optional[str] = Field(None, description="URL is the url of the server.")
+    url: Optional[str] = Field(default=None, description="URL is the url of the server.")
 
 
 class WebhookEventSource(BaseModel):
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
-    webhook_context: Optional[WebhookContext] = Field(None, alias="webhookContext")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
+    webhook_context: Optional[WebhookContext] = Field(default=None, alias="webhookContext")
 
 
 class AMQPEventSource(BaseModel):
-    auth: Optional[BasicAuth] = Field(None, title="Auth hosts secret selectors for username and password\n+optional")
+    auth: Optional[BasicAuth] = Field(
+        default=None, title="Auth hosts secret selectors for username and password\n+optional"
+    )
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
+        default=None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
     )
     consume: Optional[AMQPConsumeConfig] = Field(
-        None,
+        default=None,
         title=(
             "Consume holds the configuration to immediately starts delivering queued messages\nFor more information,"
             " visit https://pkg.go.dev/github.com/rabbitmq/amqp091-go#Channel.Consume\n+optional"
         ),
     )
     exchange_declare: Optional[AMQPExchangeDeclareConfig] = Field(
-        None,
+        default=None,
         alias="exchangeDeclare",
         title=(
             "ExchangeDeclare holds the configuration for the exchange on the server\nFor more information, visit"
             " https://pkg.go.dev/github.com/rabbitmq/amqp091-go#Channel.ExchangeDeclare\n+optional"
         ),
     )
     exchange_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="exchangeName",
         title=(
             "ExchangeName is the exchange name\nFor more information, visit"
             " https://www.rabbitmq.com/tutorials/amqp-concepts.html"
         ),
     )
-    exchange_type: Optional[str] = Field(None, alias="exchangeType", title="ExchangeType is rabbitmq exchange type")
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    exchange_type: Optional[str] = Field(
+        default=None, alias="exchangeType", title="ExchangeType is rabbitmq exchange type"
+    )
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     queue_bind: Optional[AMQPQueueBindConfig] = Field(
-        None,
+        default=None,
         alias="queueBind",
         title=(
             "QueueBind holds the configuration that binds an exchange to a queue so that publishings to the\nexchange"
             " will be routed to the queue when the publishing routing key matches the binding routing key\nFor more"
             " information, visit https://pkg.go.dev/github.com/rabbitmq/amqp091-go#Channel.QueueBind\n+optional"
         ),
     )
     queue_declare: Optional[AMQPQueueDeclareConfig] = Field(
-        None,
+        default=None,
         alias="queueDeclare",
         title=(
             "QueueDeclare holds the configuration of a queue to hold messages and deliver to consumers.\nDeclaring"
             " creates a queue if it doesn't already exist, or ensures that an existing queue matches\nthe same"
             " parameters\nFor more information, visit"
             " https://pkg.go.dev/github.com/rabbitmq/amqp091-go#Channel.QueueDeclare\n+optional"
         ),
     )
-    routing_key: Optional[str] = Field(None, alias="routingKey", title="Routing key for bindings")
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the amqp client.\n+optional")
-    url: Optional[str] = Field(None, title="URL for rabbitmq service")
+    routing_key: Optional[str] = Field(default=None, alias="routingKey", title="Routing key for bindings")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the amqp client.\n+optional")
+    url: Optional[str] = Field(default=None, title="URL for rabbitmq service")
     url_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="urlSecret", title="URLSecret is secret reference for rabbitmq service URL"
+        default=None, alias="urlSecret", title="URLSecret is secret reference for rabbitmq service URL"
     )
 
 
 class AWSLambdaTrigger(BaseModel):
     access_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key\n+optional"
+        default=None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key\n+optional"
     )
     function_name: Optional[str] = Field(
-        None, alias="functionName", description="FunctionName refers to the name of the function to invoke."
+        default=None, alias="functionName", description="FunctionName refers to the name of the function to invoke."
     )
     invocation_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="invocationType",
         description=(
             "Choose from the following options.\n\n   * RequestResponse (default) - Invoke the function synchronously."
             " Keep\n   the connection open until the function returns a response or times out.\n   The API response"
             " includes the function response and additional data.\n\n   * Event - Invoke the function asynchronously."
             " Send events that fail multiple\n   times to the function's dead-letter queue (if it's configured). The"
             " API\n   response only includes a status code.\n\n   * DryRun - Validate parameter values and verify that"
             " the user or role\n   has permission to invoke the function.\n+optional"
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         title=(
             "Parameters is the list of key-value extracted from event's payload that are applied to\nthe trigger"
             " resource.\n+optional"
         ),
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
-    region: Optional[str] = Field(None, title="Region is AWS region")
+    region: Optional[str] = Field(default=None, title="Region is AWS region")
     role_arn: Optional[str] = Field(
-        None, alias="roleARN", title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional"
+        default=None,
+        alias="roleARN",
+        title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional",
     )
     secret_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key\n+optional"
+        default=None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key\n+optional"
     )
 
 
 class AzureEventHubsTrigger(BaseModel):
     fqdn: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "FQDN refers to the namespace dns of Azure Event Hubs to be used i.e. <namespace>.servicebus.windows.net"
         ),
     )
     hub_name: Optional[str] = Field(
-        None, alias="hubName", title="HubName refers to the Azure Event Hub to send events to"
+        default=None, alias="hubName", title="HubName refers to the Azure Event Hub to send events to"
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         title=(
             "Parameters is the list of key-value extracted from event's payload that are applied to\nthe trigger"
             " resource.\n+optional"
         ),
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
     shared_access_key: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="sharedAccessKey",
         title="SharedAccessKey refers to a K8s secret containing the primary key for the",
     )
     shared_access_key_name: Optional[v1.SecretKeySelector] = Field(
-        None, alias="sharedAccessKeyName", title="SharedAccessKeyName refers to the name of the Shared Access Key"
+        default=None,
+        alias="sharedAccessKeyName",
+        title="SharedAccessKeyName refers to the name of the Shared Access Key",
     )
 
 
 class BitbucketAuth(BaseModel):
-    basic: Optional[BitbucketBasicAuth] = Field(None, title="Basic is BasicAuth auth strategy.\n+optional")
+    basic: Optional[BitbucketBasicAuth] = Field(default=None, title="Basic is BasicAuth auth strategy.\n+optional")
     oauth_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="oauthToken",
         title="OAuthToken refers to the K8s secret that holds the OAuth Bearer token.\n+optional",
     )
 
 
 class BitbucketEventSource(BaseModel):
-    auth: Optional[BitbucketAuth] = Field(None, description="Auth information required to connect to Bitbucket.")
+    auth: Optional[BitbucketAuth] = Field(
+        default=None, description="Auth information required to connect to Bitbucket."
+    )
     delete_hook_on_finish: Optional[bool] = Field(
-        None,
+        default=None,
         alias="deleteHookOnFinish",
         title=(
             "DeleteHookOnFinish determines whether to delete the defined Bitbucket hook once the event source is"
             " stopped.\n+optional"
         ),
     )
-    events: Optional[List[str]] = Field(None, description="Events this webhook is subscribed to.")
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    events: Optional[List[str]] = Field(default=None, description="Events this webhook is subscribed to.")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will be passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will be passed along the event payload.\n+optional",
     )
     owner: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "DeprecatedOwner is the owner of the repository.\nDeprecated: use Repositories instead. Will be"
             " unsupported in v1.9\n+optional"
         ),
     )
     project_key: Optional[str] = Field(
-        None,
+        default=None,
         alias="projectKey",
         title=(
             "DeprecatedProjectKey is the key of the project to which the repository relates\nDeprecated: use"
             " Repositories instead. Will be unsupported in v1.9\n+optional"
         ),
     )
     repositories: Optional[List[BitbucketRepository]] = Field(
-        None, title="Repositories holds a list of repositories for which integration needs to set up\n+optional"
+        default=None,
+        title="Repositories holds a list of repositories for which integration needs to set up\n+optional",
     )
     repository_slug: Optional[str] = Field(
-        None,
+        default=None,
         alias="repositorySlug",
         title=(
             "DeprecatedRepositorySlug is a URL-friendly version of a repository name, automatically generated by"
             " Bitbucket for use in the URL\nDeprecated: use Repositories instead. Will be unsupported in"
             " v1.9\n+optional"
         ),
     )
     webhook: Optional[WebhookContext] = Field(
-        None, title="Webhook refers to the configuration required to run an http server"
+        default=None, title="Webhook refers to the configuration required to run an http server"
     )
 
 
 class BitbucketServerEventSource(BaseModel):
     access_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="accessToken",
         title="AccessToken is reference to K8s secret which holds the bitbucket api access information",
     )
     bitbucketserver_base_url: Optional[str] = Field(
-        None,
+        default=None,
         alias="bitbucketserverBaseURL",
         title="BitbucketServerBaseURL is the base URL for API requests to a custom endpoint",
     )
     delete_hook_on_finish: Optional[bool] = Field(
-        None,
+        default=None,
         alias="deleteHookOnFinish",
         title=(
             "DeleteHookOnFinish determines whether to delete the Bitbucket Server hook for the project once the event"
             " source is stopped.\n+optional"
         ),
     )
     events: Optional[List[str]] = Field(
-        None,
+        default=None,
         title=(
             "Events are bitbucket event to listen to.\nRefer"
             " https://confluence.atlassian.com/bitbucketserver/event-payload-938025882.html"
         ),
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     project_key: Optional[str] = Field(
-        None,
+        default=None,
         alias="projectKey",
         title=(
             "DeprecatedProjectKey is the key of project for which integration needs to set up\nDeprecated: use"
             " Repositories instead. Will be unsupported in v1.8\n+optional"
         ),
     )
     repositories: Optional[List[BitbucketServerRepository]] = Field(
-        None, title="Repositories holds a list of repositories for which integration needs to set up\n+optional"
+        default=None,
+        title="Repositories holds a list of repositories for which integration needs to set up\n+optional",
     )
     repository_slug: Optional[str] = Field(
-        None,
+        default=None,
         alias="repositorySlug",
         title=(
             "DeprecatedRepositorySlug is the slug of the repository for which integration needs to set up\nDeprecated:"
             " use Repositories instead. Will be unsupported in v1.8\n+optional"
         ),
     )
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook holds configuration to run a http server")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook holds configuration to run a http server")
     webhook_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="webhookSecret",
         title=(
             "WebhookSecret is reference to K8s secret which holds the bitbucket webhook secret (for HMAC validation)"
         ),
     )
 
 
 class CustomTrigger(BaseModel):
     cert_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="certSecret",
         description=(
             "CertSecret refers to the secret that contains cert for secure connection between sensor and custom"
             " trigger gRPC server."
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description="Parameters is the list of parameters that is applied to resolved custom trigger trigger object.",
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
     secure: Optional[bool] = Field(
-        None, title="Secure refers to type of the connection between sensor to custom trigger gRPC"
+        default=None, title="Secure refers to type of the connection between sensor to custom trigger gRPC"
     )
     server_name_override: Optional[str] = Field(
-        None,
+        default=None,
         alias="serverNameOverride",
         description="ServerNameOverride for the secure connection between sensor and custom trigger gRPC server.",
     )
     server_url: Optional[str] = Field(
-        None, alias="serverURL", title="ServerURL is the url of the gRPC server that executes custom trigger"
+        default=None, alias="serverURL", title="ServerURL is the url of the gRPC server that executes custom trigger"
     )
     spec: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         description=(
             "Spec is the custom trigger resource specification that custom trigger gRPC server knows how to interpret."
         ),
     )
 
 
 class EmitterEventSource(BaseModel):
-    broker: Optional[str] = Field(None, description="Broker URI to connect to.")
-    channel_key: Optional[str] = Field(None, alias="channelKey", title="ChannelKey refers to the channel key")
-    channel_name: Optional[str] = Field(None, alias="channelName", title="ChannelName refers to the channel name")
+    broker: Optional[str] = Field(default=None, description="Broker URI to connect to.")
+    channel_key: Optional[str] = Field(default=None, alias="channelKey", title="ChannelKey refers to the channel key")
+    channel_name: Optional[str] = Field(
+        default=None, alias="channelName", title="ChannelName refers to the channel name"
+    )
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
+        default=None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
+    )
+    password: Optional[v1.SecretKeySelector] = Field(
+        default=None, title="Password to use to connect to broker\n+optional"
+    )
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the emitter client.\n+optional")
+    username: Optional[v1.SecretKeySelector] = Field(
+        default=None, title="Username to use to connect to broker\n+optional"
     )
-    password: Optional[v1.SecretKeySelector] = Field(None, title="Password to use to connect to broker\n+optional")
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the emitter client.\n+optional")
-    username: Optional[v1.SecretKeySelector] = Field(None, title="Username to use to connect to broker\n+optional")
 
 
 class EventDependencyFilter(BaseModel):
-    context: Optional[EventContext] = Field(None, title="Context filter constraints")
-    data: Optional[List[DataFilter]] = Field(None, title="Data filter constraints with escalation")
+    context: Optional[EventContext] = Field(default=None, title="Context filter constraints")
+    data: Optional[List[DataFilter]] = Field(default=None, title="Data filter constraints with escalation")
     data_logical_operator: Optional[str] = Field(
-        None,
+        default=None,
         alias="dataLogicalOperator",
         description=(
             "DataLogicalOperator defines how multiple Data filters (if defined) are evaluated together.\nAvailable"
             " values: and (&&), or (||)\nIs optional and if left blank treated as and (&&)."
         ),
     )
     expr_logical_operator: Optional[str] = Field(
-        None,
+        default=None,
         alias="exprLogicalOperator",
         description=(
             "ExprLogicalOperator defines how multiple Exprs filters (if defined) are evaluated together.\nAvailable"
             " values: and (&&), or (||)\nIs optional and if left blank treated as and (&&)."
         ),
     )
     exprs: Optional[List[ExprFilter]] = Field(
-        None, description="Exprs contains the list of expressions evaluated against the event payload."
+        default=None, description="Exprs contains the list of expressions evaluated against the event payload."
     )
     script: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Script refers to a Lua script evaluated to determine the validity of an io.argoproj.workflow.v1alpha1."
         ),
     )
-    time: Optional[TimeFilter] = Field(None, title="Time filter on the event with escalation")
+    time: Optional[TimeFilter] = Field(default=None, title="Time filter on the event with escalation")
 
 
 class EventSourceStatus(BaseModel):
     status: Optional[Status] = None
 
 
 class GitArtifact(BaseModel):
-    branch: Optional[str] = Field(None, title="Branch to use to pull trigger resource\n+optional")
+    branch: Optional[str] = Field(default=None, title="Branch to use to pull trigger resource\n+optional")
     clone_directory: Optional[str] = Field(
-        None,
+        default=None,
         alias="cloneDirectory",
         description=(
             "Directory to clone the repository. We clone complete directory because GitArtifact is not limited to any"
             " specific Git service providers.\nHence we don't use any specific git provider client."
         ),
     )
-    creds: Optional[GitCreds] = Field(None, title="Creds contain reference to git username and password\n+optional")
+    creds: Optional[GitCreds] = Field(
+        default=None, title="Creds contain reference to git username and password\n+optional"
+    )
     file_path: Optional[str] = Field(
-        None, alias="filePath", title="Path to file that contains trigger resource definition"
+        default=None, alias="filePath", title="Path to file that contains trigger resource definition"
     )
     insecure_ignore_host_key: Optional[bool] = Field(
-        None, alias="insecureIgnoreHostKey", title="Whether to ignore host key\n+optional"
+        default=None, alias="insecureIgnoreHostKey", title="Whether to ignore host key\n+optional"
     )
     ref: Optional[str] = Field(
-        None, title="Ref to use to pull trigger resource. Will result in a shallow clone and\nfetch.\n+optional"
+        default=None,
+        title="Ref to use to pull trigger resource. Will result in a shallow clone and\nfetch.\n+optional",
     )
     remote: Optional[GitRemoteConfig] = Field(
-        None,
+        default=None,
         title=(
             'Remote to manage set of tracked repositories. Defaults to "origin".\nRefer'
             " https://git-scm.com/docs/git-remote\n+optional"
         ),
     )
     ssh_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="sshKeySecret", title="SSHKeySecret refers to the secret that contains SSH key"
+        default=None, alias="sshKeySecret", title="SSHKeySecret refers to the secret that contains SSH key"
     )
-    tag: Optional[str] = Field(None, title="Tag to use to pull trigger resource\n+optional")
-    url: Optional[str] = Field(None, title="Git URL")
+    tag: Optional[str] = Field(default=None, title="Tag to use to pull trigger resource\n+optional")
+    url: Optional[str] = Field(default=None, title="Git URL")
 
 
 class GithubEventSource(BaseModel):
     active: Optional[bool] = Field(
-        None,
+        default=None,
         title=(
             "Active refers to status of the webhook for event"
             " deliveries.\nhttps://developer.github.com/webhooks/creating/#active\n+optional"
         ),
     )
     api_token: Optional[v1.SecretKeySelector] = Field(
-        None, alias="apiToken", title="APIToken refers to a K8s secret containing github api token\n+optional"
+        default=None, alias="apiToken", title="APIToken refers to a K8s secret containing github api token\n+optional"
     )
-    content_type: Optional[str] = Field(None, alias="contentType", title="ContentType of the event delivery")
+    content_type: Optional[str] = Field(default=None, alias="contentType", title="ContentType of the event delivery")
     delete_hook_on_finish: Optional[bool] = Field(
-        None,
+        default=None,
         alias="deleteHookOnFinish",
         title=(
             "DeleteHookOnFinish determines whether to delete the GitHub hook for the repository once the event source"
             " is stopped.\n+optional"
         ),
     )
     events: Optional[List[str]] = Field(
-        None, title="Events refer to Github events to which the event source will subscribe"
+        default=None, title="Events refer to Github events to which the event source will subscribe"
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     github_app: Optional[GithubAppCreds] = Field(
-        None, alias="githubApp", title="GitHubApp holds the GitHub app credentials\n+optional"
+        default=None, alias="githubApp", title="GitHubApp holds the GitHub app credentials\n+optional"
     )
     github_base_url: Optional[str] = Field(
-        None, alias="githubBaseURL", title="GitHub base URL (for GitHub Enterprise)\n+optional"
+        default=None, alias="githubBaseURL", title="GitHub base URL (for GitHub Enterprise)\n+optional"
     )
     github_upload_url: Optional[str] = Field(
-        None, alias="githubUploadURL", title="GitHub upload URL (for GitHub Enterprise)\n+optional"
+        default=None, alias="githubUploadURL", title="GitHub upload URL (for GitHub Enterprise)\n+optional"
     )
     id: Optional[str] = Field(
-        None, title="Id is the webhook's id\nDeprecated: This is not used at all, will be removed in v1.6\n+optional"
+        default=None,
+        title="Id is the webhook's id\nDeprecated: This is not used at all, will be removed in v1.6\n+optional",
     )
-    insecure: Optional[bool] = Field(None, title="Insecure tls verification")
+    insecure: Optional[bool] = Field(default=None, title="Insecure tls verification")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     organizations: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Organizations holds the names of organizations (used for organization level webhooks). Not required if"
             " Repositories is set."
         ),
     )
     owner: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "DeprecatedOwner refers to GitHub owner name i.e. argoproj\nDeprecated: use Repositories instead. Will be"
             " unsupported in v 1.6\n+optional"
         ),
     )
     repositories: Optional[List[OwnedRepositories]] = Field(
-        None,
+        default=None,
         description=(
             "Repositories holds the information of repositories, which uses repo owner as the key,\nand list of repo"
             " names as the value. Not required if Organizations is set."
         ),
     )
     repository: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "DeprecatedRepository refers to GitHub repo name i.e. argo-events\nDeprecated: use Repositories instead."
             " Will be unsupported in v 1.6\n+optional"
         ),
     )
     webhook: Optional[WebhookContext] = Field(
-        None, title="Webhook refers to the configuration required to run a http server"
+        default=None, title="Webhook refers to the configuration required to run a http server"
     )
     webhook_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="webhookSecret",
         title=(
             "WebhookSecret refers to K8s secret containing GitHub webhook"
             " secret\nhttps://developer.github.com/webhooks/securing/\n+optional"
         ),
     )
 
 
 class GitlabEventSource(BaseModel):
     access_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="accessToken",
         title="AccessToken references to k8 secret which holds the gitlab api access information",
     )
     delete_hook_on_finish: Optional[bool] = Field(
-        None,
+        default=None,
         alias="deleteHookOnFinish",
         title=(
             "DeleteHookOnFinish determines whether to delete the GitLab hook for the project once the event source is"
             " stopped.\n+optional"
         ),
     )
     enable_ssl_verification: Optional[bool] = Field(
-        None, alias="enableSSLVerification", title="EnableSSLVerification to enable ssl verification\n+optional"
+        default=None,
+        alias="enableSSLVerification",
+        title="EnableSSLVerification to enable ssl verification\n+optional",
     )
     events: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Events are gitlab event to listen to.\nRefer"
             " https://github.com/xanzy/go-gitlab/blob/bf34eca5d13a9f4c3f501d8a97b8ac226d55e4d9/projects.go#L794."
         ),
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     gitlab_base_url: Optional[str] = Field(
-        None, alias="gitlabBaseURL", title="GitlabBaseURL is the base URL for API requests to a custom endpoint"
+        default=None,
+        alias="gitlabBaseURL",
+        title="GitlabBaseURL is the base URL for API requests to a custom endpoint",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     project_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="projectID",
         title=(
             "DeprecatedProjectID is the id of project for which integration needs to setup\nDeprecated: use Projects"
             " instead. Will be unsupported in v 1.7\n+optional"
         ),
     )
     projects: Optional[List[str]] = Field(
-        None, title='List of project IDs or project namespace paths like "whynowy/test"'
+        default=None, title='List of project IDs or project namespace paths like "whynowy/test"'
     )
     secret_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="secretToken",
         title="SecretToken references to k8 secret which holds the Secret Token used by webhook config",
     )
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook holds configuration to run a http server")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook holds configuration to run a http server")
 
 
 class KafkaEventSource(BaseModel):
     config: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Yaml format Sarama config for Kafka connection.\nIt follows the struct of sarama.Config. See"
             " https://github.com/Shopify/sarama/blob/main/config.go\ne.g.\n\nconsumer:\n  fetch:\n    min: 1\nnet:\n "
             " MaxOpenRequests: 5\n\n+optional"
         ),
     )
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", description="Backoff holds parameters applied to connection."
+        default=None, alias="connectionBackoff", description="Backoff holds parameters applied to connection."
     )
     consumer_group: Optional[KafkaConsumerGroup] = Field(
-        None, alias="consumerGroup", title="Consumer group for kafka client\n+optional"
+        default=None, alias="consumerGroup", title="Consumer group for kafka client\n+optional"
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     limit_events_per_second: Optional[str] = Field(
-        None,
+        default=None,
         alias="limitEventsPerSecond",
         title="Sets a limit on how many events get read from kafka per second.\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    partition: Optional[str] = Field(None, title="Partition name")
-    sasl: Optional[SASLConfig] = Field(None, title="SASL configuration for the kafka client\n+optional")
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the kafka client.\n+optional")
-    topic: Optional[str] = Field(None, title="Topic name")
-    url: Optional[str] = Field(None, title="URL to kafka cluster, multiple URLs separated by comma")
+    partition: Optional[str] = Field(default=None, title="Partition name")
+    sasl: Optional[SASLConfig] = Field(default=None, title="SASL configuration for the kafka client\n+optional")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the kafka client.\n+optional")
+    topic: Optional[str] = Field(default=None, title="Topic name")
+    url: Optional[str] = Field(default=None, title="URL to kafka cluster, multiple URLs separated by comma")
     version: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Specify what kafka version is being connected to enables certain features in sarama, defaults to"
             " 1.0.0\n+optional"
         ),
     )
 
 
 class KafkaTrigger(BaseModel):
     compress: Optional[bool] = Field(
-        None,
+        default=None,
         title=(
             "Compress determines whether to compress message or not.\nDefaults to false.\nIf set to true, compresses"
             " message using snappy compression.\n+optional"
         ),
     )
     flush_frequency: Optional[int] = Field(
-        None,
+        default=None,
         alias="flushFrequency",
         title=(
             "FlushFrequency refers to the frequency in milliseconds to flush batches.\nDefaults to 500"
             " milliseconds.\n+optional"
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None, description="Parameters is the list of parameters that is applied to resolved Kafka trigger object."
+        default=None,
+        description="Parameters is the list of parameters that is applied to resolved Kafka trigger object.",
     )
-    partition: Optional[int] = Field(None, description="Partition to write data to.")
+    partition: Optional[int] = Field(default=None, description="Partition to write data to.")
     partitioning_key: Optional[str] = Field(
-        None,
+        default=None,
         alias="partitioningKey",
         description=(
             "The partitioning key for the messages put on the Kafka topic.\nDefaults to broker url.\n+optional."
         ),
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
     required_acks: Optional[int] = Field(
-        None,
+        default=None,
         alias="requiredAcks",
         description=(
             "RequiredAcks used in producer to tell the broker how many replica acknowledgements\nDefaults to 1 (Only"
             " wait for the leader to ack).\n+optional."
         ),
     )
-    sasl: Optional[SASLConfig] = Field(None, title="SASL configuration for the kafka client\n+optional")
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the Kafka producer.\n+optional")
+    sasl: Optional[SASLConfig] = Field(default=None, title="SASL configuration for the kafka client\n+optional")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the Kafka producer.\n+optional")
     topic: Optional[str] = Field(
-        None, title="Name of the topic.\nMore info at https://kafka.apache.org/documentation/#intro_topics"
+        default=None, title="Name of the topic.\nMore info at https://kafka.apache.org/documentation/#intro_topics"
     )
-    url: Optional[str] = Field(None, description="URL of the Kafka broker, multiple URLs separated by comma.")
+    url: Optional[str] = Field(default=None, description="URL of the Kafka broker, multiple URLs separated by comma.")
     version: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Specify what kafka version is being connected to enables certain features in sarama, defaults to"
             " 1.0.0\n+optional"
         ),
     )
 
 
 class MQTTEventSource(BaseModel):
-    client_id: Optional[str] = Field(None, alias="clientId", title="ClientID is the id of the client")
+    client_id: Optional[str] = Field(default=None, alias="clientId", title="ClientID is the id of the client")
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", description="ConnectionBackoff holds backoff applied to connection."
+        default=None, alias="connectionBackoff", description="ConnectionBackoff holds backoff applied to connection."
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the mqtt client.\n+optional")
-    topic: Optional[str] = Field(None, title="Topic name")
-    url: Optional[str] = Field(None, title="URL to connect to broker")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the mqtt client.\n+optional")
+    topic: Optional[str] = Field(default=None, title="Topic name")
+    url: Optional[str] = Field(default=None, title="URL to connect to broker")
 
 
 class NATSEventsSource(BaseModel):
-    auth: Optional[NATSAuth] = Field(None, title="Auth information\n+optional")
+    auth: Optional[NATSAuth] = Field(default=None, title="Auth information\n+optional")
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", description="ConnectionBackoff holds backoff applied to connection."
+        default=None, alias="connectionBackoff", description="ConnectionBackoff holds backoff applied to connection."
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     subject: Optional[str] = Field(
-        None, title="Subject holds the name of the subject onto which messages are published"
+        default=None, title="Subject holds the name of the subject onto which messages are published"
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the nats client.\n+optional")
-    url: Optional[str] = Field(None, title="URL to connect to NATS cluster")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the nats client.\n+optional")
+    url: Optional[str] = Field(default=None, title="URL to connect to NATS cluster")
 
 
 class NATSTrigger(BaseModel):
     parameters: Optional[List[TriggerParameter]] = None
     payload: Optional[List[TriggerParameter]] = None
-    subject: Optional[str] = Field(None, description="Name of the subject to put message on.")
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the NATS producer.\n+optional")
-    url: Optional[str] = Field(None, description="URL of the NATS cluster.")
+    subject: Optional[str] = Field(default=None, description="Name of the subject to put message on.")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the NATS producer.\n+optional")
+    url: Optional[str] = Field(default=None, description="URL of the NATS cluster.")
 
 
 class NSQEventSource(BaseModel):
-    channel: Optional[str] = Field(None, title="Channel used for subscription")
+    channel: Optional[str] = Field(default=None, title="Channel used for subscription")
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
+        default=None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     host_address: Optional[str] = Field(
-        None, alias="hostAddress", title="HostAddress is the address of the host for NSQ lookup"
+        default=None, alias="hostAddress", title="HostAddress is the address of the host for NSQ lookup"
     )
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the nsq client.\n+optional")
-    topic: Optional[str] = Field(None, description="Topic to subscribe to.")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the nsq client.\n+optional")
+    topic: Optional[str] = Field(default=None, description="Topic to subscribe to.")
 
 
 class OpenWhiskTrigger(BaseModel):
-    action_name: Optional[str] = Field(None, alias="actionName", description="Name of the action/function.")
+    action_name: Optional[str] = Field(default=None, alias="actionName", description="Name of the action/function.")
     auth_token: Optional[v1.SecretKeySelector] = Field(
-        None, alias="authToken", title="AuthToken for authentication.\n+optional"
+        default=None, alias="authToken", title="AuthToken for authentication.\n+optional"
+    )
+    host: Optional[str] = Field(default=None, description="Host URL of the OpenWhisk.")
+    namespace: Optional[str] = Field(
+        default=None, description='Namespace for the action.\nDefaults to "_".\n+optional.'
     )
-    host: Optional[str] = Field(None, description="Host URL of the OpenWhisk.")
-    namespace: Optional[str] = Field(None, description='Namespace for the action.\nDefaults to "_".\n+optional.')
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         title=(
             "Parameters is the list of key-value extracted from event's payload that are applied to\nthe trigger"
             " resource.\n+optional"
         ),
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
-    version: Optional[str] = Field(None, title="Version for the API.\nDefaults to v1.\n+optional")
+    version: Optional[str] = Field(default=None, title="Version for the API.\nDefaults to v1.\n+optional")
 
 
 class PulsarEventSource(BaseModel):
     auth_token_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="authTokenSecret", title="Authentication token for the pulsar client.\n+optional"
+        default=None, alias="authTokenSecret", title="Authentication token for the pulsar client.\n+optional"
     )
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
+        default=None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the pulsar client.\n+optional")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the pulsar client.\n+optional")
     tls_allow_insecure_connection: Optional[bool] = Field(
-        None,
+        default=None,
         alias="tlsAllowInsecureConnection",
         title="Whether the Pulsar client accept untrusted TLS certificate from broker.\n+optional",
     )
     tls_trust_certs_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="tlsTrustCertsSecret", title="Trusted TLS certificate secret.\n+optional"
+        default=None, alias="tlsTrustCertsSecret", title="Trusted TLS certificate secret.\n+optional"
     )
     tls_validate_hostname: Optional[bool] = Field(
-        None,
+        default=None,
         alias="tlsValidateHostname",
         title="Whether the Pulsar client verify the validity of the host name from broker.\n+optional",
     )
-    topics: Optional[List[str]] = Field(None, title="Name of the topics to subscribe to.\n+required")
+    topics: Optional[List[str]] = Field(default=None, title="Name of the topics to subscribe to.\n+required")
     type: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             'Type of the subscription.\nOnly "exclusive" and "shared" is supported.\nDefaults to exclusive.\n+optional'
         ),
     )
-    url: Optional[str] = Field(None, title="Configure the service URL for the Pulsar service.\n+required")
+    url: Optional[str] = Field(default=None, title="Configure the service URL for the Pulsar service.\n+required")
 
 
 class PulsarTrigger(BaseModel):
     auth_token_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="authTokenSecret", title="Authentication token for the pulsar client.\n+optional"
+        default=None, alias="authTokenSecret", title="Authentication token for the pulsar client.\n+optional"
     )
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
+        default=None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None, description="Parameters is the list of parameters that is applied to resolved Kafka trigger object."
+        default=None,
+        description="Parameters is the list of parameters that is applied to resolved Kafka trigger object.",
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the pulsar client.\n+optional")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the pulsar client.\n+optional")
     tls_allow_insecure_connection: Optional[bool] = Field(
-        None,
+        default=None,
         alias="tlsAllowInsecureConnection",
         title="Whether the Pulsar client accept untrusted TLS certificate from broker.\n+optional",
     )
     tls_trust_certs_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="tlsTrustCertsSecret", title="Trusted TLS certificate secret.\n+optional"
+        default=None, alias="tlsTrustCertsSecret", title="Trusted TLS certificate secret.\n+optional"
     )
     tls_validate_hostname: Optional[bool] = Field(
-        None,
+        default=None,
         alias="tlsValidateHostname",
         title="Whether the Pulsar client verify the validity of the host name from broker.\n+optional",
     )
     topic: Optional[str] = Field(
-        None, title="Name of the topic.\nSee https://pulsar.apache.org/docs/en/concepts-messaging/"
+        default=None, title="Name of the topic.\nSee https://pulsar.apache.org/docs/en/concepts-messaging/"
     )
-    url: Optional[str] = Field(None, title="Configure the service URL for the Pulsar service.\n+required")
+    url: Optional[str] = Field(default=None, title="Configure the service URL for the Pulsar service.\n+required")
 
 
 class RedisEventSource(BaseModel):
     channels: Optional[List[str]] = None
-    db: Optional[int] = Field(None, title="DB to use. If not specified, default DB 0 will be used.\n+optional")
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    db: Optional[int] = Field(default=None, title="DB to use. If not specified, default DB 0 will be used.\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     host_address: Optional[str] = Field(
-        None, alias="hostAddress", title="HostAddress refers to the address of the Redis host/server"
+        default=None, alias="hostAddress", title="HostAddress refers to the address of the Redis host/server"
     )
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     namespace: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Namespace to use to retrieve the password from. It should only be specified if password is"
             " declared\n+optional"
         ),
     )
     password: Optional[v1.SecretKeySelector] = Field(
-        None, title="Password required for authentication if any.\n+optional"
+        default=None, title="Password required for authentication if any.\n+optional"
+    )
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the redis client.\n+optional")
+    username: Optional[str] = Field(
+        default=None, title="Username required for ACL style authentication if any.\n+optional"
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the redis client.\n+optional")
-    username: Optional[str] = Field(None, title="Username required for ACL style authentication if any.\n+optional")
 
 
 class RedisStreamEventSource(BaseModel):
     consumer_group: Optional[str] = Field(
-        None,
+        default=None,
         alias="consumerGroup",
         title=(
             "ConsumerGroup refers to the Redis stream consumer group that will be\ncreated on all redis streams."
             " Messages are read through this group. Defaults to 'argo-events-cg'\n+optional"
         ),
     )
-    db: Optional[int] = Field(None, title="DB to use. If not specified, default DB 0 will be used.\n+optional")
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    db: Optional[int] = Field(default=None, title="DB to use. If not specified, default DB 0 will be used.\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     host_address: Optional[str] = Field(
-        None, alias="hostAddress", title="HostAddress refers to the address of the Redis host/server (master instance)"
+        default=None,
+        alias="hostAddress",
+        title="HostAddress refers to the address of the Redis host/server (master instance)",
     )
     max_msg_count_per_read: Optional[int] = Field(
-        None,
+        default=None,
         alias="maxMsgCountPerRead",
         title=(
             "MaxMsgCountPerRead holds the maximum number of messages per stream that will be read in each XREADGROUP"
             " of all streams\nExample: if there are 2 streams and MaxMsgCountPerRead=10, then each XREADGROUP may read"
             " upto a total of 20 messages.\nSame as COUNT option in XREADGROUP(https://redis.io/topics/streams-intro)."
             " Defaults to 10\n+optional"
         ),
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     password: Optional[v1.SecretKeySelector] = Field(
-        None, title="Password required for authentication if any.\n+optional"
+        default=None, title="Password required for authentication if any.\n+optional"
     )
     streams: Optional[List[str]] = Field(
-        None,
+        default=None,
         description="Streams to look for entries. XREADGROUP is used on all streams using a single consumer group.",
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the redis client.\n+optional")
-    username: Optional[str] = Field(None, title="Username required for ACL style authentication if any.\n+optional")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the redis client.\n+optional")
+    username: Optional[str] = Field(
+        default=None, title="Username required for ACL style authentication if any.\n+optional"
+    )
 
 
 class ResourceEventSource(BaseModel):
     event_types: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="eventTypes",
         description="EventTypes is the list of event type to watch.\nPossible values are - ADD, UPDATE and DELETE.",
     )
     filter: Optional[ResourceFilter] = Field(
-        None,
+        default=None,
         title=(
             "Filter is applied on the metadata of the resource\nIf you apply filter, then the internal event informer"
             " will only monitor objects that pass the filter.\n+optional"
         ),
     )
     group_version_resource: Optional[v1_1.GroupVersionResource] = Field(
-        None, alias="groupVersionResource", title="Group of the resource"
+        default=None, alias="groupVersionResource", title="Group of the resource"
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    namespace: Optional[str] = Field(None, title="Namespace where resource is deployed")
+    namespace: Optional[str] = Field(default=None, title="Namespace where resource is deployed")
 
 
 class SNSEventSource(BaseModel):
     access_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key"
+        default=None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key"
     )
     endpoint: Optional[str] = Field(
-        None, title="Endpoint configures connection to a specific SNS endpoint instead of Amazons servers\n+optional"
+        default=None,
+        title="Endpoint configures connection to a specific SNS endpoint instead of Amazons servers\n+optional",
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    region: Optional[str] = Field(None, title="Region is AWS region")
+    region: Optional[str] = Field(default=None, title="Region is AWS region")
     role_arn: Optional[str] = Field(
-        None, alias="roleARN", title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional"
+        default=None,
+        alias="roleARN",
+        title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional",
     )
     secret_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key"
+        default=None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key"
     )
-    topic_arn: Optional[str] = Field(None, alias="topicArn", title="TopicArn")
+    topic_arn: Optional[str] = Field(default=None, alias="topicArn", title="TopicArn")
     validate_signature: Optional[bool] = Field(
-        None,
+        default=None,
         alias="validateSignature",
         title="ValidateSignature is boolean that can be set to true for SNS signature verification\n+optional",
     )
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook configuration for http server")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook configuration for http server")
 
 
 class SecureHeader(BaseModel):
     name: Optional[str] = None
     value_from: Optional[ValueFromSource] = Field(
-        None, alias="valueFrom", title="Values can be read from either secrets or configmaps"
+        default=None, alias="valueFrom", title="Values can be read from either secrets or configmaps"
     )
 
 
 class SensorStatus(BaseModel):
     status: Optional[Status] = None
 
 
 class Service(BaseModel):
     cluster_ip: Optional[str] = Field(
-        None,
+        default=None,
         alias="clusterIP",
         title=(
             "clusterIP is the IP address of the service and is usually assigned\nrandomly by the master. If an address"
             " is specified manually and is not in\nuse by others, it will be allocated to the service; otherwise,"
             " creation\nof the service will fail. This field can not be changed through updates.\nValid values are"
             ' "None", empty string (""), or a valid IP address. "None"\ncan be specified for headless services when'
             " proxying is not required.\nMore info:"
             " https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies\n+optional"
         ),
     )
     ports: Optional[List[v1.ServicePort]] = Field(
-        None,
+        default=None,
         title=(
             "The list of ports that are exposed by this ClusterIP"
             " service.\n+patchMergeKey=port\n+patchStrategy=merge\n+listType=map\n+listMapKey=port\n+listMapKey=protocol"
         ),
     )
 
 
 class SlackEventSource(BaseModel):
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     signing_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="signingSecret", title="Slack App signing secret"
+        default=None, alias="signingSecret", title="Slack App signing secret"
     )
-    token: Optional[v1.SecretKeySelector] = Field(None, title="Token for URL verification handshake")
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook holds configuration for a REST endpoint")
+    token: Optional[v1.SecretKeySelector] = Field(default=None, title="Token for URL verification handshake")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook holds configuration for a REST endpoint")
 
 
 class SlackTrigger(BaseModel):
     channel: Optional[str] = Field(
-        None, title="Channel refers to which Slack channel to send slack message.\n+optional"
+        default=None, title="Channel refers to which Slack channel to send slack message.\n+optional"
     )
     message: Optional[str] = Field(
-        None, title="Message refers to the message to send to the Slack channel.\n+optional"
+        default=None, title="Message refers to the message to send to the Slack channel.\n+optional"
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         title=(
             "Parameters is the list of key-value extracted from event's payload that are applied to\nthe trigger"
             " resource.\n+optional"
         ),
     )
     slack_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="slackToken",
         description="SlackToken refers to the Kubernetes secret that holds the slack token required to send messages.",
     )
 
 
 class StorageGridEventSource(BaseModel):
-    api_url: Optional[str] = Field(None, alias="apiURL", description="APIURL is the url of the storagegrid api.")
-    auth_token: Optional[v1.SecretKeySelector] = Field(None, alias="authToken", title="Auth token for storagegrid api")
-    bucket: Optional[str] = Field(None, description="Name of the bucket to register notifications for.")
+    api_url: Optional[str] = Field(
+        default=None, alias="apiURL", description="APIURL is the url of the storagegrid api."
+    )
+    auth_token: Optional[v1.SecretKeySelector] = Field(
+        default=None, alias="authToken", title="Auth token for storagegrid api"
+    )
+    bucket: Optional[str] = Field(default=None, description="Name of the bucket to register notifications for.")
     events: Optional[List[str]] = None
     filter: Optional[StorageGridFilter] = Field(
-        None, description="Filter on object key which caused the notification."
+        default=None, description="Filter on object key which caused the notification."
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    region: Optional[str] = Field(None, title="S3 region.\nDefaults to us-east-1\n+optional")
-    topic_arn: Optional[str] = Field(None, alias="topicArn", title="TopicArn")
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook holds configuration for a REST endpoint")
+    region: Optional[str] = Field(default=None, title="S3 region.\nDefaults to us-east-1\n+optional")
+    topic_arn: Optional[str] = Field(default=None, alias="topicArn", title="TopicArn")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook holds configuration for a REST endpoint")
 
 
 class StripeEventSource(BaseModel):
     api_key: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="apiKey",
         title=(
             "APIKey refers to K8s secret that holds Stripe API key. Used only if CreateWebhook is enabled.\n+optional"
         ),
     )
     create_webhook: Optional[bool] = Field(
-        None,
+        default=None,
         alias="createWebhook",
         title="CreateWebhook if specified creates a new webhook programmatically.\n+optional",
     )
     event_filter: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="eventFilter",
         title=(
             "EventFilter describes the type of events to listen to. If not specified, all types of events will be"
             " processed.\nMore info at https://stripe.com/docs/api/events/list\n+optional"
         ),
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook holds configuration for a REST endpoint")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook holds configuration for a REST endpoint")
 
 
 class ArtifactLocation(BaseModel):
-    configmap: Optional[v1.ConfigMapKeySelector] = Field(None, title="Configmap that stores the artifact")
-    file: Optional[FileArtifact] = Field(None, title="File artifact is artifact stored in a file")
-    git: Optional[GitArtifact] = Field(None, title="Git repository hosting the artifact")
-    inline: Optional[str] = Field(None, title="Inline artifact is embedded in sensor spec as a string")
-    resource: Optional[Resource] = Field(None, title="Resource is generic template for K8s resource")
-    s3: Optional[S3Artifact] = Field(None, title="S3 compliant artifact")
-    url: Optional[URLArtifact] = Field(None, title="URL to fetch the artifact from")
+    configmap: Optional[v1.ConfigMapKeySelector] = Field(default=None, title="Configmap that stores the artifact")
+    file: Optional[FileArtifact] = Field(default=None, title="File artifact is artifact stored in a file")
+    git: Optional[GitArtifact] = Field(default=None, title="Git repository hosting the artifact")
+    inline: Optional[str] = Field(default=None, title="Inline artifact is embedded in sensor spec as a string")
+    resource: Optional[Resource] = Field(default=None, title="Resource is generic template for K8s resource")
+    s3: Optional[S3Artifact] = Field(default=None, title="S3 compliant artifact")
+    url: Optional[URLArtifact] = Field(default=None, title="URL to fetch the artifact from")
 
 
 class EventDependency(BaseModel):
-    event_name: Optional[str] = Field(None, alias="eventName", title="EventName is the name of the event")
+    event_name: Optional[str] = Field(default=None, alias="eventName", title="EventName is the name of the event")
     event_source_name: Optional[str] = Field(
-        None, alias="eventSourceName", title="EventSourceName is the name of EventSource that Sensor depends on"
+        default=None,
+        alias="eventSourceName",
+        title="EventSourceName is the name of EventSource that Sensor depends on",
     )
     filters: Optional[EventDependencyFilter] = Field(
-        None,
+        default=None,
         title="Filters and rules governing toleration of success and constraints on the context and data of an event",
     )
     filters_logical_operator: Optional[str] = Field(
-        None,
+        default=None,
         alias="filtersLogicalOperator",
         description=(
             "FiltersLogicalOperator defines how different filters are evaluated together.\nAvailable values: and (&&),"
             " or (||)\nIs optional and if left blank treated as and (&&)."
         ),
     )
-    name: Optional[str] = Field(None, title="Name is a unique name of this dependency")
-    transform: Optional[EventDependencyTransformer] = Field(None, title="Transform transforms the event data")
+    name: Optional[str] = Field(default=None, title="Name is a unique name of this dependency")
+    transform: Optional[EventDependencyTransformer] = Field(default=None, title="Transform transforms the event data")
 
 
 class HTTPTrigger(BaseModel):
     basic_auth: Optional[BasicAuth] = Field(
-        None, alias="basicAuth", title="BasicAuth configuration for the http request.\n+optional"
+        default=None, alias="basicAuth", title="BasicAuth configuration for the http request.\n+optional"
     )
-    headers: Optional[Dict[str, str]] = Field(None, title="Headers for the HTTP request.\n+optional")
+    headers: Optional[Dict[str, str]] = Field(default=None, title="Headers for the HTTP request.\n+optional")
     method: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Method refers to the type of the HTTP request.\nRefer https://golang.org/src/net/http/method.go for more"
             " io.argoproj.workflow.v1alpha1.\nDefault value is POST.\n+optional"
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Parameters is the list of key-value extracted from event's payload that are applied to\nthe HTTP trigger"
             " resource."
         ),
     )
     payload: Optional[List[TriggerParameter]] = None
     secure_headers: Optional[List[SecureHeader]] = Field(
-        None,
+        default=None,
         alias="secureHeaders",
         title="Secure Headers stored in Kubernetes Secrets for the HTTP requests.\n+optional",
     )
     timeout: Optional[str] = Field(
-        None, title="Timeout refers to the HTTP request timeout in seconds.\nDefault value is 60 seconds.\n+optional"
+        default=None,
+        title="Timeout refers to the HTTP request timeout in seconds.\nDefault value is 60 seconds.\n+optional",
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the HTTP client.\n+optional")
-    url: Optional[str] = Field(None, description="URL refers to the URL to send HTTP request to.")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the HTTP client.\n+optional")
+    url: Optional[str] = Field(default=None, description="URL refers to the URL to send HTTP request to.")
 
 
 class StandardK8STrigger(BaseModel):
     live_object: Optional[bool] = Field(
-        None,
+        default=None,
         alias="liveObject",
         title=(
             "LiveObject specifies whether the resource should be directly fetched from K8s instead\nof being marshaled"
             " from the resource artifact. If set to true, the resource artifact\nmust contain the information required"
             ' to uniquely identify the resource in the cluster,\nthat is, you must specify "apiVersion", "kind" as'
             ' well as "name" and "namespace" meta\ndata.\nOnly valid for operation type `update`\n+optional'
         ),
     )
     operation: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Operation refers to the type of operation performed on the k8s resource.\nDefault value is"
             " Create.\n+optional"
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None, description="Parameters is the list of parameters that is applied to resolved K8s trigger object."
+        default=None,
+        description="Parameters is the list of parameters that is applied to resolved K8s trigger object.",
     )
     patch_strategy: Optional[str] = Field(
-        None,
+        default=None,
         alias="patchStrategy",
         title=(
             "PatchStrategy controls the K8s object patching strategy when the trigger operation is specified as"
             " patch.\npossible"
             ' values:\n"application/json-patch+json"\n"application/merge-patch+json"\n"application/strategic-merge-patch+json"\n"application/apply-patch+yaml".\nDefaults'
             ' to "application/merge-patch+json"\n+optional'
         ),
     )
-    source: Optional[ArtifactLocation] = Field(None, title="Source of the K8s resource file(s)")
+    source: Optional[ArtifactLocation] = Field(default=None, title="Source of the K8s resource file(s)")
 
 
 class ArgoWorkflowTrigger(BaseModel):
-    args: Optional[List[str]] = Field(None, title="Args is the list of arguments to pass to the argo CLI")
+    args: Optional[List[str]] = Field(default=None, title="Args is the list of arguments to pass to the argo CLI")
     operation: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Operation refers to the type of operation performed on the argo workflow resource.\nDefault value is"
             " Submit.\n+optional"
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None, title="Parameters is the list of parameters to pass to resolved Argo Workflow object"
+        default=None, title="Parameters is the list of parameters to pass to resolved Argo Workflow object"
     )
-    source: Optional[ArtifactLocation] = Field(None, title="Source of the K8s resource file(s)")
+    source: Optional[ArtifactLocation] = Field(default=None, title="Source of the K8s resource file(s)")
 
 
 class TriggerTemplate(BaseModel):
     argo_workflow: Optional[ArgoWorkflowTrigger] = Field(
-        None,
+        default=None,
         alias="argoWorkflow",
         title=(
             "ArgoWorkflow refers to the trigger that can perform various operations on an Argo"
             " io.argoproj.workflow.v1alpha1.\n+optional"
         ),
     )
     aws_lambda: Optional[AWSLambdaTrigger] = Field(
-        None,
+        default=None,
         alias="awsLambda",
         title=(
             "AWSLambda refers to the trigger designed to invoke AWS Lambda function with with on-the-fly constructable"
             " payload.\n+optional"
         ),
     )
     azure_event_hubs: Optional[AzureEventHubsTrigger] = Field(
-        None,
+        default=None,
         alias="azureEventHubs",
         title="AzureEventHubs refers to the trigger send an event to an Azure Event Hub.\n+optional",
     )
     conditions: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             'Conditions is the conditions to execute the trigger.\nFor example: "(dep01 || dep02) && dep04"\n+optional'
         ),
     )
     conditions_reset: Optional[List[ConditionsResetCriteria]] = Field(
-        None, alias="conditionsReset", title="Criteria to reset the conditons\n+optional"
+        default=None, alias="conditionsReset", title="Criteria to reset the conditons\n+optional"
     )
     custom: Optional[CustomTrigger] = Field(
-        None,
+        default=None,
         title=(
             "CustomTrigger refers to the trigger designed to connect to a gRPC trigger server and execute a custom"
             " trigger.\n+optional"
         ),
     )
     http: Optional[HTTPTrigger] = Field(
-        None,
+        default=None,
         title=(
             "HTTP refers to the trigger designed to dispatch a HTTP request with on-the-fly constructable"
             " payload.\n+optional"
         ),
     )
     k8s: Optional[StandardK8STrigger] = Field(
-        None,
+        default=None,
         title=(
             "StandardK8STrigger refers to the trigger designed to create or update a generic Kubernetes"
             " resource.\n+optional"
         ),
     )
     kafka: Optional[KafkaTrigger] = Field(
-        None, description="Kafka refers to the trigger designed to place messages on Kafka topic.\n+optional."
+        default=None, description="Kafka refers to the trigger designed to place messages on Kafka topic.\n+optional."
     )
     log: Optional[LogTrigger] = Field(
-        None, title="Log refers to the trigger designed to invoke log the io.argoproj.workflow.v1alpha1.\n+optional"
+        default=None,
+        title="Log refers to the trigger designed to invoke log the io.argoproj.workflow.v1alpha1.\n+optional",
     )
-    name: Optional[str] = Field(None, description="Name is a unique name of the action to take.")
+    name: Optional[str] = Field(default=None, description="Name is a unique name of the action to take.")
     nats: Optional[NATSTrigger] = Field(
-        None, description="NATS refers to the trigger designed to place message on NATS subject.\n+optional."
+        default=None, description="NATS refers to the trigger designed to place message on NATS subject.\n+optional."
     )
     open_whisk: Optional[OpenWhiskTrigger] = Field(
-        None,
+        default=None,
         alias="openWhisk",
         title="OpenWhisk refers to the trigger designed to invoke OpenWhisk action.\n+optional",
     )
     pulsar: Optional[PulsarTrigger] = Field(
-        None, title="Pulsar refers to the trigger designed to place messages on Pulsar topic.\n+optional"
+        default=None, title="Pulsar refers to the trigger designed to place messages on Pulsar topic.\n+optional"
     )
     slack: Optional[SlackTrigger] = Field(
-        None, title="Slack refers to the trigger designed to send slack notification message.\n+optional"
+        default=None, title="Slack refers to the trigger designed to send slack notification message.\n+optional"
     )
 
 
 class Template(BaseModel):
-    affinity: Optional[v1.Affinity] = Field(None, title="If specified, the pod's scheduling constraints\n+optional")
+    affinity: Optional[v1.Affinity] = Field(
+        default=None, title="If specified, the pod's scheduling constraints\n+optional"
+    )
     container: Optional[v1.Container] = Field(
-        None, title="Container is the main container image to run in the sensor pod\n+optional"
+        default=None, title="Container is the main container image to run in the sensor pod\n+optional"
     )
     image_pull_secrets: Optional[List[v1.LocalObjectReference]] = Field(
-        None,
+        default=None,
         alias="imagePullSecrets",
         title=(
             "ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling"
             " any of the images used by this PodSpec.\nIf specified, these secrets will be passed to individual puller"
             " implementations for them to use. For example,\nin the case of docker, only DockerConfig type secrets are"
             " honored.\nMore info:"
             " https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod\n+optional\n+patchMergeKey=name\n+patchStrategy=merge"
         ),
     )
-    metadata: Optional[Metadata] = Field(None, title="Metadata sets the pods's metadata, i.e. annotations and labels")
+    metadata: Optional[Metadata] = Field(
+        default=None, title="Metadata sets the pods's metadata, i.e. annotations and labels"
+    )
     node_selector: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         alias="nodeSelector",
         title=(
             "NodeSelector is a selector which must be true for the pod to fit on a node.\nSelector which must match a"
             " node's labels for the pod to be scheduled on that node.\nMore info:"
             " https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n+optional"
         ),
     )
     priority: Optional[int] = Field(
-        None,
+        default=None,
         title=(
             "The priority value. Various system components use this field to find the\npriority of the EventSource"
             " pod. When Priority Admission Controller is enabled,\nit prevents users from setting this field. The"
             " admission controller populates\nthis field from PriorityClassName.\nThe higher the value, the higher the"
             " priority.\nMore info:"
             " https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n+optional"
         ),
     )
     priority_class_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="priorityClassName",
         title=(
             'If specified, indicates the EventSource pod\'s priority. "system-node-critical"\nand'
             ' "system-cluster-critical" are two special keywords which indicate the\nhighest priorities with the'
             " former being the highest priority. Any other\nname must be defined by creating a PriorityClass object"
             " with that name.\nIf not specified, the pod priority will be default or zero if there is"
             " no\ndefault.\nMore info:"
             " https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n+optional"
         ),
     )
     security_context: Optional[v1.PodSecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         title=(
             "SecurityContext holds pod-level security attributes and common container settings.\nOptional: Defaults to"
             " empty.  See type description for default values of each field.\n+optional"
         ),
     )
     service_account_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="serviceAccountName",
         title=(
             "ServiceAccountName is the name of the ServiceAccount to use to run sensor pod.\nMore info:"
             " https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n+optional"
         ),
     )
-    tolerations: Optional[List[v1.Toleration]] = Field(None, title="If specified, the pod's tolerations.\n+optional")
+    tolerations: Optional[List[v1.Toleration]] = Field(
+        default=None, title="If specified, the pod's tolerations.\n+optional"
+    )
     volumes: Optional[List[v1.Volume]] = Field(
-        None,
+        default=None,
         title=(
             "Volumes is a list of volumes that can be mounted by containers in a"
             " io.argoproj.workflow.v1alpha1.\n+patchStrategy=merge\n+patchMergeKey=name\n+optional"
         ),
     )
 
 
 class Trigger(BaseModel):
     parameters: Optional[List[TriggerParameter]] = Field(
-        None, title="Parameters is the list of parameters applied to the trigger template definition"
+        default=None, title="Parameters is the list of parameters applied to the trigger template definition"
     )
     policy: Optional[TriggerPolicy] = Field(
-        None, title="Policy to configure backoff and execution criteria for the trigger\n+optional"
+        default=None, title="Policy to configure backoff and execution criteria for the trigger\n+optional"
     )
     rate_limit: Optional[RateLimit] = Field(
-        None, alias="rateLimit", title="Rate limit, default unit is Second\n+optional"
+        default=None, alias="rateLimit", title="Rate limit, default unit is Second\n+optional"
     )
     retry_strategy: Optional[Backoff] = Field(
-        None, alias="retryStrategy", title="Retry strategy, defaults to no retry\n+optional"
+        default=None, alias="retryStrategy", title="Retry strategy, defaults to no retry\n+optional"
+    )
+    template: Optional[TriggerTemplate] = Field(
+        default=None, description="Template describes the trigger specification."
     )
-    template: Optional[TriggerTemplate] = Field(None, description="Template describes the trigger specification.")
 
 
 class EventSourceSpec(BaseModel):
-    amqp: Optional[Dict[str, AMQPEventSource]] = Field(None, title="AMQP event sources")
+    amqp: Optional[Dict[str, AMQPEventSource]] = Field(default=None, title="AMQP event sources")
     azure_events_hub: Optional[Dict[str, AzureEventsHubEventSource]] = Field(
-        None, alias="azureEventsHub", title="AzureEventsHub event sources"
+        default=None, alias="azureEventsHub", title="AzureEventsHub event sources"
     )
-    bitbucket: Optional[Dict[str, BitbucketEventSource]] = Field(None, title="Bitbucket event sources")
+    bitbucket: Optional[Dict[str, BitbucketEventSource]] = Field(default=None, title="Bitbucket event sources")
     bitbucketserver: Optional[Dict[str, BitbucketServerEventSource]] = Field(
-        None, title="Bitbucket Server event sources"
+        default=None, title="Bitbucket Server event sources"
     )
-    calendar: Optional[Dict[str, CalendarEventSource]] = Field(None, title="Calendar event sources")
-    emitter: Optional[Dict[str, EmitterEventSource]] = Field(None, title="Emitter event source")
+    calendar: Optional[Dict[str, CalendarEventSource]] = Field(default=None, title="Calendar event sources")
+    emitter: Optional[Dict[str, EmitterEventSource]] = Field(default=None, title="Emitter event source")
     event_bus_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="eventBusName",
         title='EventBusName references to a EventBus name. By default the value is "default"',
     )
-    file: Optional[Dict[str, FileEventSource]] = Field(None, title="File event sources")
-    generic: Optional[Dict[str, GenericEventSource]] = Field(None, title="Generic event source")
-    github: Optional[Dict[str, GithubEventSource]] = Field(None, title="Github event sources")
-    gitlab: Optional[Dict[str, GitlabEventSource]] = Field(None, title="Gitlab event sources")
-    hdfs: Optional[Dict[str, HDFSEventSource]] = Field(None, title="HDFS event sources")
-    kafka: Optional[Dict[str, KafkaEventSource]] = Field(None, title="Kafka event sources")
-    minio: Optional[Dict[str, S3Artifact]] = Field(None, title="Minio event sources")
-    mqtt: Optional[Dict[str, MQTTEventSource]] = Field(None, title="MQTT event sources")
-    nats: Optional[Dict[str, NATSEventsSource]] = Field(None, title="NATS event sources")
-    nsq: Optional[Dict[str, NSQEventSource]] = Field(None, title="NSQ event source")
-    pub_sub: Optional[Dict[str, PubSubEventSource]] = Field(None, alias="pubSub", title="PubSub event sources")
-    pulsar: Optional[Dict[str, PulsarEventSource]] = Field(None, title="Pulsar event source")
-    redis: Optional[Dict[str, RedisEventSource]] = Field(None, title="Redis event source")
+    file: Optional[Dict[str, FileEventSource]] = Field(default=None, title="File event sources")
+    generic: Optional[Dict[str, GenericEventSource]] = Field(default=None, title="Generic event source")
+    github: Optional[Dict[str, GithubEventSource]] = Field(default=None, title="Github event sources")
+    gitlab: Optional[Dict[str, GitlabEventSource]] = Field(default=None, title="Gitlab event sources")
+    hdfs: Optional[Dict[str, HDFSEventSource]] = Field(default=None, title="HDFS event sources")
+    kafka: Optional[Dict[str, KafkaEventSource]] = Field(default=None, title="Kafka event sources")
+    minio: Optional[Dict[str, S3Artifact]] = Field(default=None, title="Minio event sources")
+    mqtt: Optional[Dict[str, MQTTEventSource]] = Field(default=None, title="MQTT event sources")
+    nats: Optional[Dict[str, NATSEventsSource]] = Field(default=None, title="NATS event sources")
+    nsq: Optional[Dict[str, NSQEventSource]] = Field(default=None, title="NSQ event source")
+    pub_sub: Optional[Dict[str, PubSubEventSource]] = Field(default=None, alias="pubSub", title="PubSub event sources")
+    pulsar: Optional[Dict[str, PulsarEventSource]] = Field(default=None, title="Pulsar event source")
+    redis: Optional[Dict[str, RedisEventSource]] = Field(default=None, title="Redis event source")
     redis_stream: Optional[Dict[str, RedisStreamEventSource]] = Field(
-        None, alias="redisStream", title="Redis stream source"
+        default=None, alias="redisStream", title="Redis stream source"
     )
-    replicas: Optional[int] = Field(None, title="Replicas is the event source deployment replicas")
-    resource: Optional[Dict[str, ResourceEventSource]] = Field(None, title="Resource event sources")
+    replicas: Optional[int] = Field(default=None, title="Replicas is the event source deployment replicas")
+    resource: Optional[Dict[str, ResourceEventSource]] = Field(default=None, title="Resource event sources")
     service: Optional[Service] = Field(
-        None, title="Service is the specifications of the service to expose the event source\n+optional"
+        default=None, title="Service is the specifications of the service to expose the event source\n+optional"
     )
-    slack: Optional[Dict[str, SlackEventSource]] = Field(None, title="Slack event sources")
-    sns: Optional[Dict[str, SNSEventSource]] = Field(None, title="SNS event sources")
-    sqs: Optional[Dict[str, SQSEventSource]] = Field(None, title="SQS event sources")
+    slack: Optional[Dict[str, SlackEventSource]] = Field(default=None, title="Slack event sources")
+    sns: Optional[Dict[str, SNSEventSource]] = Field(default=None, title="SNS event sources")
+    sqs: Optional[Dict[str, SQSEventSource]] = Field(default=None, title="SQS event sources")
     storage_grid: Optional[Dict[str, StorageGridEventSource]] = Field(
-        None, alias="storageGrid", title="StorageGrid event sources"
+        default=None, alias="storageGrid", title="StorageGrid event sources"
     )
-    stripe: Optional[Dict[str, StripeEventSource]] = Field(None, title="Stripe event sources")
+    stripe: Optional[Dict[str, StripeEventSource]] = Field(default=None, title="Stripe event sources")
     template: Optional[Template] = Field(
-        None, title="Template is the pod specification for the event source\n+optional"
+        default=None, title="Template is the pod specification for the event source\n+optional"
     )
-    webhook: Optional[Dict[str, WebhookEventSource]] = Field(None, title="Webhook event sources")
+    webhook: Optional[Dict[str, WebhookEventSource]] = Field(default=None, title="Webhook event sources")
 
 
 class SensorSpec(BaseModel):
     dependencies: Optional[List[EventDependency]] = Field(
-        None, description="Dependencies is a list of the events that this sensor is dependent on."
+        default=None, description="Dependencies is a list of the events that this sensor is dependent on."
     )
     error_on_failed_round: Optional[bool] = Field(
-        None,
+        default=None,
         alias="errorOnFailedRound",
         description=(
             "ErrorOnFailedRound if set to true, marks sensor state as `error` if the previous trigger round"
             " fails.\nOnce sensor state is set to `error`, no further triggers will be processed."
         ),
     )
     event_bus_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="eventBusName",
         title='EventBusName references to a EventBus name. By default the value is "default"',
     )
-    replicas: Optional[int] = Field(None, title="Replicas is the sensor deployment replicas")
-    template: Optional[Template] = Field(None, title="Template is the pod specification for the sensor\n+optional")
+    replicas: Optional[int] = Field(default=None, title="Replicas is the sensor deployment replicas")
+    template: Optional[Template] = Field(
+        default=None, title="Template is the pod specification for the sensor\n+optional"
+    )
     triggers: Optional[List[Trigger]] = Field(
-        None,
+        default=None,
         description=(
             "Triggers is a list of the things that this sensor evokes. These are the outputs from this sensor."
         ),
     )
 
 
 class EventSource(BaseModel):
     metadata: Optional[v1_1.ObjectMeta] = None
     spec: Optional[EventSourceSpec] = None
-    status: Optional[EventSourceStatus] = Field(None, title="+optional")
+    status: Optional[EventSourceStatus] = Field(default=None, title="+optional")
 
 
 class EventSourceList(BaseModel):
     items: Optional[List[EventSource]] = None
     metadata: Optional[v1_1.ListMeta] = None
 
 
 class Sensor(BaseModel):
     metadata: Optional[v1_1.ObjectMeta] = None
     spec: Optional[SensorSpec] = None
-    status: Optional[SensorStatus] = Field(None, title="+optional")
+    status: Optional[SensorStatus] = Field(default=None, title="+optional")
 
 
 class SensorList(BaseModel):
     items: Optional[List[Sensor]] = None
     metadata: Optional[v1_1.ListMeta] = None
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/io/argoproj/events/v1alpha1.pyi` & `hera_workflows-5.6.0/src/hera/events/models/io/argoproj/events/v1alpha1.pyi`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,11 @@
-from typing import Dict, List, Optional
-
-from hera.shared._base_model import BaseModel as BaseModel
-
 from ...k8s.api.core import v1 as v1
 from ...k8s.apimachinery.pkg.apis.meta import v1 as v1_1
+from hera.shared._base_model import BaseModel as BaseModel
+from typing import Dict, List, Optional
 
 class AMQPConsumeConfig(BaseModel):
     auto_ack: Optional[bool]
     consumer_tag: Optional[str]
     exclusive: Optional[bool]
     no_local: Optional[bool]
     no_wait: Optional[bool]
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/io/argoproj/workflow/v1alpha1.py` & `hera_workflows-5.6.0/src/hera/events/models/io/argoproj/workflow/v1alpha1.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,16 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from ...k8s.api.core import v1
 from ...k8s.api.policy import v1beta1
 from ...k8s.apimachinery.pkg.apis.meta import v1 as v1_1
 from ...k8s.apimachinery.pkg.util import intstr
 
 
@@ -21,59 +20,61 @@
 
 class ArchivedWorkflowDeletedResponse(BaseModel):
     pass
 
 
 class ArtGCStatus(BaseModel):
     not_specified: Optional[bool] = Field(
-        None,
+        default=None,
         alias="notSpecified",
         description="if this is true, we already checked to see if we need to do it and we don't",
     )
     pods_recouped: Optional[Dict[str, bool]] = Field(
-        None,
+        default=None,
         alias="podsRecouped",
         description=(
             "have completed Pods been processed? (mapped by Pod name) used to prevent re-processing the Status of a"
             " Pod more than once"
         ),
     )
     strategies_processed: Optional[Dict[str, bool]] = Field(
-        None,
+        default=None,
         alias="strategiesProcessed",
         description=(
             "have Pods been started to perform this strategy? (enables us not to re-process what we've already done)"
         ),
     )
 
 
 class ArtifactRepositoryRef(BaseModel):
     config_map: Optional[str] = Field(
-        None, alias="configMap", description='The name of the config map. Defaults to "artifact-repositories".'
+        default=None, alias="configMap", description='The name of the config map. Defaults to "artifact-repositories".'
     )
     key: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'The config map key. Defaults to the value of the "workflows.argoproj.io/default-artifact-repository"'
             " annotation."
         ),
     )
 
 
 class ArtifactResult(BaseModel):
     error: Optional[str] = Field(
-        None, description="Error is an optional error message which should be set if Success==false"
+        default=None, description="Error is an optional error message which should be set if Success==false"
     )
     name: str = Field(..., description="Name is the name of the Artifact")
-    success: Optional[bool] = Field(None, description="Success describes whether the deletion succeeded")
+    success: Optional[bool] = Field(default=None, description="Success describes whether the deletion succeeded")
 
 
 class ArtifactResultNodeStatus(BaseModel):
     artifact_results: Optional[Dict[str, ArtifactResult]] = Field(
-        None, alias="artifactResults", description="ArtifactResults maps Artifact name to result of the deletion"
+        default=None,
+        alias="artifactResults",
+        description="ArtifactResults maps Artifact name to result of the deletion",
     )
 
 
 class ClusterWorkflowTemplateDeleteResponse(BaseModel):
     pass
 
 
@@ -82,31 +83,31 @@
 
 
 class CollectEventResponse(BaseModel):
     pass
 
 
 class Condition(BaseModel):
-    message: Optional[str] = Field(None, description="Message is the condition message")
-    status: Optional[str] = Field(None, description="Status is the status of the condition")
-    type: Optional[str] = Field(None, description="Type is the type of condition")
+    message: Optional[str] = Field(default=None, description="Message is the condition message")
+    status: Optional[str] = Field(default=None, description="Status is the status of the condition")
+    type: Optional[str] = Field(default=None, description="Type is the type of condition")
 
 
 class ContinueOn(BaseModel):
     error: Optional[bool] = None
     failed: Optional[bool] = None
 
 
 class Counter(BaseModel):
     value: str = Field(..., description="Value is the value of the metric")
 
 
 class CreateS3BucketOptions(BaseModel):
     object_locking: Optional[bool] = Field(
-        None, alias="objectLocking", description="ObjectLocking Enable object locking"
+        default=None, alias="objectLocking", description="ObjectLocking Enable object locking"
     )
 
 
 class CronWorkflowDeletedResponse(BaseModel):
     pass
 
 
@@ -132,32 +133,32 @@
 
 class EventResponse(BaseModel):
     pass
 
 
 class ExecutorConfig(BaseModel):
     service_account_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="serviceAccountName",
         description="ServiceAccountName specifies the service account name of the executor container.",
     )
 
 
 class Gauge(BaseModel):
     realtime: bool = Field(..., description="Realtime emits this metric in real time if applicable")
     value: str = Field(..., description="Value is the value of the metric")
 
 
 class GetUserInfoResponse(BaseModel):
     email: Optional[str] = None
-    email_verified: Optional[bool] = Field(None, alias="emailVerified")
+    email_verified: Optional[bool] = Field(default=None, alias="emailVerified")
     groups: Optional[List[str]] = None
     issuer: Optional[str] = None
-    service_account_name: Optional[str] = Field(None, alias="serviceAccountName")
-    service_account_namespace: Optional[str] = Field(None, alias="serviceAccountNamespace")
+    service_account_name: Optional[str] = Field(default=None, alias="serviceAccountName")
+    service_account_namespace: Optional[str] = Field(default=None, alias="serviceAccountNamespace")
     subject: Optional[str] = None
 
 
 class HTTPBodySource(BaseModel):
     bytes: Optional[str] = None
 
 
@@ -204,15 +205,15 @@
             ' "${io.argoproj.workflow.v1alpha1.metadata.annotations.userDefinedKey}"'
         ),
     )
 
 
 class LogEntry(BaseModel):
     content: Optional[str] = None
-    pod_name: Optional[str] = Field(None, alias="podName")
+    pod_name: Optional[str] = Field(default=None, alias="podName")
 
 
 class MemoizationStatus(BaseModel):
     cache_name: str = Field(..., alias="cacheName", description="Cache is the name of the cache that was used")
     hit: bool = Field(..., description="Hit indicates whether this node was created from a cache entry")
     key: str = Field(..., description="Key is the name of the key used for this node's cache")
 
@@ -224,84 +225,89 @@
 
 class MetricLabel(BaseModel):
     key: str
     value: str
 
 
 class Mutex(BaseModel):
-    name: Optional[str] = Field(None, description="name of the mutex")
+    name: Optional[str] = Field(default=None, description="name of the mutex")
 
 
 class MutexHolding(BaseModel):
     holder: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Holder is a reference to the object which holds the Mutex. Holding Scenario:\n  1. Current workflow's"
             " NodeID which is holding the lock.\n     e.g: ${NodeID}\nWaiting Scenario:\n  1. Current workflow or"
             " other workflow NodeID which is holding the lock.\n     e.g: ${WorkflowName}/${NodeID}"
         ),
     )
-    mutex: Optional[str] = Field(None, description="Reference for the mutex e.g: ${namespace}/mutex/${mutexName}")
+    mutex: Optional[str] = Field(
+        default=None, description="Reference for the mutex e.g: ${namespace}/mutex/${mutexName}"
+    )
 
 
 class MutexStatus(BaseModel):
     holding: Optional[List[MutexHolding]] = Field(
-        None,
+        default=None,
         description=(
             "Holding is a list of mutexes and their respective objects that are held by mutex lock for this"
             " io.argoproj.workflow.v1alpha1."
         ),
     )
     waiting: Optional[List[MutexHolding]] = Field(
-        None, description="Waiting is a list of mutexes and their respective objects this workflow is waiting for."
+        default=None,
+        description="Waiting is a list of mutexes and their respective objects this workflow is waiting for.",
     )
 
 
 class NodeSynchronizationStatus(BaseModel):
-    waiting: Optional[str] = Field(None, description="Waiting is the name of the lock that this node is waiting for")
+    waiting: Optional[str] = Field(
+        default=None, description="Waiting is the name of the lock that this node is waiting for"
+    )
 
 
 class NoneStrategy(BaseModel):
     pass
 
 
 class OAuth2EndpointParam(BaseModel):
     key: str = Field(..., description="Name is the header name")
-    value: Optional[str] = Field(None, description="Value is the literal value to use for the header")
+    value: Optional[str] = Field(default=None, description="Value is the literal value to use for the header")
 
 
 class OSSLifecycleRule(BaseModel):
     mark_deletion_after_days: Optional[int] = Field(
-        None,
+        default=None,
         alias="markDeletionAfterDays",
         description="MarkDeletionAfterDays is the number of days before we delete objects in the bucket",
     )
     mark_infrequent_access_after_days: Optional[int] = Field(
-        None,
+        default=None,
         alias="markInfrequentAccessAfterDays",
         description=(
             "MarkInfrequentAccessAfterDays is the number of days before we convert the objects in the bucket to"
             " Infrequent Access (IA) storage type"
         ),
     )
 
 
 class Plugin(BaseModel):
     pass
 
 
 class Prometheus(BaseModel):
-    counter: Optional[Counter] = Field(None, description="Counter is a counter metric")
-    gauge: Optional[Gauge] = Field(None, description="Gauge is a gauge metric")
+    counter: Optional[Counter] = Field(default=None, description="Counter is a counter metric")
+    gauge: Optional[Gauge] = Field(default=None, description="Gauge is a gauge metric")
     help: str = Field(..., description="Help is a string that describes the metric")
-    histogram: Optional[Histogram] = Field(None, description="Histogram is a histogram metric")
-    labels: Optional[List[MetricLabel]] = Field(None, description="Labels is a list of metric labels")
+    histogram: Optional[Histogram] = Field(default=None, description="Histogram is a histogram metric")
+    labels: Optional[List[MetricLabel]] = Field(default=None, description="Labels is a list of metric labels")
     name: str = Field(..., description="Name is the name of the metric")
     when: Optional[str] = Field(
-        None, description="When is a conditional statement that decides when to emit the metric"
+        default=None, description="When is a conditional statement that decides when to emit the metric"
     )
 
 
 class RawArtifact(BaseModel):
     data: str = Field(..., description="Data is the string contents of the artifact")
 
 
@@ -312,98 +318,101 @@
     parameters: Optional[List[str]] = None
     uid: Optional[str] = None
 
 
 class RetryArchivedWorkflowRequest(BaseModel):
     name: Optional[str] = None
     namespace: Optional[str] = None
-    node_field_selector: Optional[str] = Field(None, alias="nodeFieldSelector")
+    node_field_selector: Optional[str] = Field(default=None, alias="nodeFieldSelector")
     parameters: Optional[List[str]] = None
-    restart_successful: Optional[bool] = Field(None, alias="restartSuccessful")
+    restart_successful: Optional[bool] = Field(default=None, alias="restartSuccessful")
     uid: Optional[str] = None
 
 
 class RetryNodeAntiAffinity(BaseModel):
     pass
 
 
 class SemaphoreHolding(BaseModel):
     holders: Optional[List[str]] = Field(
-        None, description="Holders stores the list of current holder names in the io.argoproj.workflow.v1alpha1."
+        default=None,
+        description="Holders stores the list of current holder names in the io.argoproj.workflow.v1alpha1.",
     )
-    semaphore: Optional[str] = Field(None, description="Semaphore stores the semaphore name.")
+    semaphore: Optional[str] = Field(default=None, description="Semaphore stores the semaphore name.")
 
 
 class SemaphoreStatus(BaseModel):
     holding: Optional[List[SemaphoreHolding]] = Field(
-        None, description="Holding stores the list of resource acquired synchronization lock for workflows."
+        default=None, description="Holding stores the list of resource acquired synchronization lock for workflows."
     )
     waiting: Optional[List[SemaphoreHolding]] = Field(
-        None, description="Waiting indicates the list of current synchronization lock holders."
+        default=None, description="Waiting indicates the list of current synchronization lock holders."
     )
 
 
 class SuppliedValueFrom(BaseModel):
     pass
 
 
 class SuspendTemplate(BaseModel):
     duration: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Duration is the seconds to wait before automatically resuming a template. Must be a string. Default unit"
             ' is seconds. Could also be a Duration, e.g.: "2m", "6h", "1d"'
         ),
     )
 
 
 class SynchronizationStatus(BaseModel):
-    mutex: Optional[MutexStatus] = Field(None, description="Mutex stores this workflow's mutex holder details")
+    mutex: Optional[MutexStatus] = Field(default=None, description="Mutex stores this workflow's mutex holder details")
     semaphore: Optional[SemaphoreStatus] = Field(
-        None, description="Semaphore stores this workflow's Semaphore holder details"
+        default=None, description="Semaphore stores this workflow's Semaphore holder details"
     )
 
 
 class TTLStrategy(BaseModel):
     seconds_after_completion: Optional[int] = Field(
-        None,
+        default=None,
         alias="secondsAfterCompletion",
         description="SecondsAfterCompletion is the number of seconds to live after completion",
     )
     seconds_after_failure: Optional[int] = Field(
-        None,
+        default=None,
         alias="secondsAfterFailure",
         description="SecondsAfterFailure is the number of seconds to live after failure",
     )
     seconds_after_success: Optional[int] = Field(
-        None,
+        default=None,
         alias="secondsAfterSuccess",
         description="SecondsAfterSuccess is the number of seconds to live after success",
     )
 
 
 class TarStrategy(BaseModel):
     compression_level: Optional[int] = Field(
-        None,
+        default=None,
         alias="compressionLevel",
         description=(
             "CompressionLevel specifies the gzip compression level to use for the artifact. Defaults to"
             " gzip.DefaultCompression."
         ),
     )
 
 
 class TemplateRef(BaseModel):
     cluster_scope: Optional[bool] = Field(
-        None,
+        default=None,
         alias="clusterScope",
         description="ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate).",
     )
-    name: Optional[str] = Field(None, description="Name is the resource name of the template.")
-    template: Optional[str] = Field(None, description="Template is the name of referred template in the resource.")
+    name: Optional[str] = Field(default=None, description="Name is the resource name of the template.")
+    template: Optional[str] = Field(
+        default=None, description="Template is the name of referred template in the resource."
+    )
 
 
 class TransformationStep(BaseModel):
     expression: str = Field(..., description="Expression defines an expr expression to apply")
 
 
 class Version(BaseModel):
@@ -415,81 +424,81 @@
     go_version: str = Field(..., alias="goVersion")
     platform: str
     version: str
 
 
 class VolumeClaimGC(BaseModel):
     strategy: Optional[str] = Field(
-        None, description='Strategy is the strategy to use. One of "OnWorkflowCompletion", "OnWorkflowSuccess"'
+        default=None, description='Strategy is the strategy to use. One of "OnWorkflowCompletion", "OnWorkflowSuccess"'
     )
 
 
 class WorkflowDeleteResponse(BaseModel):
     pass
 
 
 class WorkflowMetadata(BaseModel):
     annotations: Optional[Dict[str, str]] = None
     labels: Optional[Dict[str, str]] = None
-    labels_from: Optional[Dict[str, LabelValueFrom]] = Field(None, alias="labelsFrom")
+    labels_from: Optional[Dict[str, LabelValueFrom]] = Field(default=None, alias="labelsFrom")
 
 
 class WorkflowResubmitRequest(BaseModel):
     memoized: Optional[bool] = None
     name: Optional[str] = None
     namespace: Optional[str] = None
     parameters: Optional[List[str]] = None
 
 
 class WorkflowResumeRequest(BaseModel):
     name: Optional[str] = None
     namespace: Optional[str] = None
-    node_field_selector: Optional[str] = Field(None, alias="nodeFieldSelector")
+    node_field_selector: Optional[str] = Field(default=None, alias="nodeFieldSelector")
 
 
 class WorkflowRetryRequest(BaseModel):
     name: Optional[str] = None
     namespace: Optional[str] = None
-    node_field_selector: Optional[str] = Field(None, alias="nodeFieldSelector")
+    node_field_selector: Optional[str] = Field(default=None, alias="nodeFieldSelector")
     parameters: Optional[List[str]] = None
-    restart_successful: Optional[bool] = Field(None, alias="restartSuccessful")
+    restart_successful: Optional[bool] = Field(default=None, alias="restartSuccessful")
 
 
 class WorkflowSetRequest(BaseModel):
     message: Optional[str] = None
     name: Optional[str] = None
     namespace: Optional[str] = None
-    node_field_selector: Optional[str] = Field(None, alias="nodeFieldSelector")
-    output_parameters: Optional[str] = Field(None, alias="outputParameters")
+    node_field_selector: Optional[str] = Field(default=None, alias="nodeFieldSelector")
+    output_parameters: Optional[str] = Field(default=None, alias="outputParameters")
     phase: Optional[str] = None
 
 
 class WorkflowStopRequest(BaseModel):
     message: Optional[str] = None
     name: Optional[str] = None
     namespace: Optional[str] = None
-    node_field_selector: Optional[str] = Field(None, alias="nodeFieldSelector")
+    node_field_selector: Optional[str] = Field(default=None, alias="nodeFieldSelector")
 
 
 class WorkflowSuspendRequest(BaseModel):
     name: Optional[str] = None
     namespace: Optional[str] = None
 
 
 class WorkflowTemplateDeleteResponse(BaseModel):
     pass
 
 
 class WorkflowTemplateRef(BaseModel):
     cluster_scope: Optional[bool] = Field(
-        None,
+        default=None,
         alias="clusterScope",
         description="ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate).",
     )
-    name: Optional[str] = Field(None, description="Name is the resource name of the workflow template.")
+    name: Optional[str] = Field(default=None, description="Name is the resource name of the workflow template.")
 
 
 class WorkflowTerminateRequest(BaseModel):
     name: Optional[str] = None
     namespace: Optional[str] = None
 
 
@@ -501,61 +510,71 @@
     none: Optional[NoneStrategy] = None
     tar: Optional[TarStrategy] = None
     zip: Optional[ZipStrategy] = None
 
 
 class ArtifactGC(BaseModel):
     pod_metadata: Optional[Metadata] = Field(
-        None,
+        default=None,
         alias="podMetadata",
         description=(
             "PodMetadata is an optional field for specifying the Labels and Annotations that should be assigned to the"
             " Pod doing the deletion"
         ),
     )
     service_account_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="serviceAccountName",
         description=(
             "ServiceAccountName is an optional field for specifying the Service Account that should be assigned to the"
             " Pod doing the deletion"
         ),
     )
-    strategy: Optional[str] = Field(None, description="Strategy is the strategy to use.")
+    strategy: Optional[str] = Field(default=None, description="Strategy is the strategy to use.")
 
 
 class ArtifactGCStatus(BaseModel):
     artifact_results_by_node: Optional[Dict[str, ArtifactResultNodeStatus]] = Field(
-        None, alias="artifactResultsByNode", description="ArtifactResultsByNode maps Node name to result"
+        default=None, alias="artifactResultsByNode", description="ArtifactResultsByNode maps Node name to result"
     )
 
 
 class ArtifactoryArtifact(BaseModel):
     password_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="passwordSecret", description="PasswordSecret is the secret selector to the repository password"
+        default=None,
+        alias="passwordSecret",
+        description="PasswordSecret is the secret selector to the repository password",
     )
     url: str = Field(..., description="URL of the artifact")
     username_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="usernameSecret", description="UsernameSecret is the secret selector to the repository username"
+        default=None,
+        alias="usernameSecret",
+        description="UsernameSecret is the secret selector to the repository username",
     )
 
 
 class ArtifactoryArtifactRepository(BaseModel):
     password_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="passwordSecret", description="PasswordSecret is the secret selector to the repository password"
+        default=None,
+        alias="passwordSecret",
+        description="PasswordSecret is the secret selector to the repository password",
+    )
+    repo_url: Optional[str] = Field(
+        default=None, alias="repoURL", description="RepoURL is the url for artifactory repo."
     )
-    repo_url: Optional[str] = Field(None, alias="repoURL", description="RepoURL is the url for artifactory repo.")
     username_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="usernameSecret", description="UsernameSecret is the secret selector to the repository username"
+        default=None,
+        alias="usernameSecret",
+        description="UsernameSecret is the secret selector to the repository username",
     )
 
 
 class AzureArtifact(BaseModel):
     account_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="accountKeySecret",
         description="AccountKeySecret is the secret selector to the Azure Blob Storage account access key",
     )
     blob: str = Field(
         ..., description="Blob is the blob name (i.e., path) in the container where the artifact resides"
     )
     container: str = Field(..., description="Container is the container where resources will be stored")
@@ -563,89 +582,93 @@
         ...,
         description=(
             "Endpoint is the service url associated with an account. It is most likely"
             ' "https://<ACCOUNT_NAME>.blob.core.windows.net"'
         ),
     )
     use_sdk_creds: Optional[bool] = Field(
-        None,
+        default=None,
         alias="useSDKCreds",
         description="UseSDKCreds tells the driver to figure out credentials based on sdk defaults.",
     )
 
 
 class AzureArtifactRepository(BaseModel):
     account_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="accountKeySecret",
         description="AccountKeySecret is the secret selector to the Azure Blob Storage account access key",
     )
     blob_name_format: Optional[str] = Field(
-        None,
+        default=None,
         alias="blobNameFormat",
         description=(
             "BlobNameFormat is defines the format of how to store blob names. Can reference workflow variables"
         ),
     )
     container: str = Field(..., description="Container is the container where resources will be stored")
     endpoint: str = Field(
         ...,
         description=(
             "Endpoint is the service url associated with an account. It is most likely"
             ' "https://<ACCOUNT_NAME>.blob.core.windows.net"'
         ),
     )
     use_sdk_creds: Optional[bool] = Field(
-        None,
+        default=None,
         alias="useSDKCreds",
         description="UseSDKCreds tells the driver to figure out credentials based on sdk defaults.",
     )
 
 
 class Backoff(BaseModel):
     duration: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Duration is the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m",'
             ' "1h")'
         ),
     )
     factor: Optional[intstr.IntOrString] = Field(
-        None, description="Factor is a factor to multiply the base duration after each failed retry"
+        default=None, description="Factor is a factor to multiply the base duration after each failed retry"
     )
     max_duration: Optional[str] = Field(
-        None,
+        default=None,
         alias="maxDuration",
         description="MaxDuration is the maximum amount of time allowed for the backoff strategy",
     )
 
 
 class BasicAuth(BaseModel):
     password_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="passwordSecret", description="PasswordSecret is the secret selector to the repository password"
+        default=None,
+        alias="passwordSecret",
+        description="PasswordSecret is the secret selector to the repository password",
     )
     username_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="usernameSecret", description="UsernameSecret is the secret selector to the repository username"
+        default=None,
+        alias="usernameSecret",
+        description="UsernameSecret is the secret selector to the repository username",
     )
 
 
 class Cache(BaseModel):
     config_map: v1.ConfigMapKeySelector = Field(
         ..., alias="configMap", description="ConfigMap sets a ConfigMap-based cache"
     )
 
 
 class ClientCertAuth(BaseModel):
-    client_cert_secret: Optional[v1.SecretKeySelector] = Field(None, alias="clientCertSecret")
-    client_key_secret: Optional[v1.SecretKeySelector] = Field(None, alias="clientKeySecret")
+    client_cert_secret: Optional[v1.SecretKeySelector] = Field(default=None, alias="clientCertSecret")
+    client_key_secret: Optional[v1.SecretKeySelector] = Field(default=None, alias="clientKeySecret")
 
 
 class ContainerSetRetryStrategy(BaseModel):
     duration: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Duration is the time between each retry, examples values are "300ms", "1s" or "5m". Valid time units are'
             ' "ns", "us" (or "s"), "ms", "s", "m", "h".'
         ),
     )
     retries: intstr.IntOrString = Field(..., description="Nbr of retries")
 
@@ -659,200 +682,210 @@
     )
     last_scheduled_time: v1_1.Time = Field(
         ..., alias="lastScheduledTime", description="LastScheduleTime is the last time the CronWorkflow was scheduled"
     )
 
 
 class GCSArtifact(BaseModel):
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     key: str = Field(..., description="Key is the path in the bucket where the artifact resides")
     service_account_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="serviceAccountKeySecret",
         description="ServiceAccountKeySecret is the secret selector to the bucket's service account key",
     )
 
 
 class GCSArtifactRepository(BaseModel):
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     key_format: Optional[str] = Field(
-        None,
+        default=None,
         alias="keyFormat",
         description="KeyFormat is defines the format of how to store keys. Can reference workflow variables",
     )
     service_account_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="serviceAccountKeySecret",
         description="ServiceAccountKeySecret is the secret selector to the bucket's service account key",
     )
 
 
 class GitArtifact(BaseModel):
-    branch: Optional[str] = Field(None, description="Branch is the branch to fetch when `SingleBranch` is enabled")
+    branch: Optional[str] = Field(
+        default=None, description="Branch is the branch to fetch when `SingleBranch` is enabled"
+    )
     depth: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Depth specifies clones/fetches should be shallow and include the given number of commits from the"
             " branch tip"
         ),
     )
     disable_submodules: Optional[bool] = Field(
-        None, alias="disableSubmodules", description="DisableSubmodules disables submodules during git clone"
+        default=None, alias="disableSubmodules", description="DisableSubmodules disables submodules during git clone"
     )
     fetch: Optional[List[str]] = Field(
-        None, description="Fetch specifies a number of refs that should be fetched before checkout"
+        default=None, description="Fetch specifies a number of refs that should be fetched before checkout"
     )
     insecure_ignore_host_key: Optional[bool] = Field(
-        None,
+        default=None,
         alias="insecureIgnoreHostKey",
         description="InsecureIgnoreHostKey disables SSH strict host key checking during git clone",
     )
     password_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="passwordSecret", description="PasswordSecret is the secret selector to the repository password"
+        default=None,
+        alias="passwordSecret",
+        description="PasswordSecret is the secret selector to the repository password",
     )
     repo: str = Field(..., description="Repo is the git repository")
-    revision: Optional[str] = Field(None, description="Revision is the git commit, tag, branch to checkout")
+    revision: Optional[str] = Field(default=None, description="Revision is the git commit, tag, branch to checkout")
     single_branch: Optional[bool] = Field(
-        None,
+        default=None,
         alias="singleBranch",
         description="SingleBranch enables single branch clone, using the `branch` parameter",
     )
     ssh_private_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="sshPrivateKeySecret",
         description="SSHPrivateKeySecret is the secret selector to the repository ssh private key",
     )
     username_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="usernameSecret", description="UsernameSecret is the secret selector to the repository username"
+        default=None,
+        alias="usernameSecret",
+        description="UsernameSecret is the secret selector to the repository username",
     )
 
 
 class HDFSArtifact(BaseModel):
-    addresses: Optional[List[str]] = Field(None, description="Addresses is accessible addresses of HDFS name nodes")
-    force: Optional[bool] = Field(None, description="Force copies a file forcibly even if it exists")
+    addresses: Optional[List[str]] = Field(
+        default=None, description="Addresses is accessible addresses of HDFS name nodes"
+    )
+    force: Optional[bool] = Field(default=None, description="Force copies a file forcibly even if it exists")
     hdfs_user: Optional[str] = Field(
-        None,
+        default=None,
         alias="hdfsUser",
         description=(
             "HDFSUser is the user to access HDFS file system. It is ignored if either ccache or keytab is used."
         ),
     )
     krb_c_cache_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbCCacheSecret",
         description=(
             "KrbCCacheSecret is the secret selector for Kerberos ccache Either ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_config_config_map: Optional[v1.ConfigMapKeySelector] = Field(
-        None,
+        default=None,
         alias="krbConfigConfigMap",
         description=(
             "KrbConfig is the configmap selector for Kerberos config as string It must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_keytab_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbKeytabSecret",
         description=(
             "KrbKeytabSecret is the secret selector for Kerberos keytab Either ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_realm: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbRealm",
         description="KrbRealm is the Kerberos realm used with Kerberos keytab It must be set if keytab is used.",
     )
     krb_service_principal_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbServicePrincipalName",
         description=(
             "KrbServicePrincipalName is the principal name of Kerberos service It must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_username: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbUsername",
         description="KrbUsername is the Kerberos username used with Kerberos keytab It must be set if keytab is used.",
     )
     path: str = Field(..., description="Path is a file path in HDFS")
 
 
 class HDFSArtifactRepository(BaseModel):
-    addresses: Optional[List[str]] = Field(None, description="Addresses is accessible addresses of HDFS name nodes")
-    force: Optional[bool] = Field(None, description="Force copies a file forcibly even if it exists")
+    addresses: Optional[List[str]] = Field(
+        default=None, description="Addresses is accessible addresses of HDFS name nodes"
+    )
+    force: Optional[bool] = Field(default=None, description="Force copies a file forcibly even if it exists")
     hdfs_user: Optional[str] = Field(
-        None,
+        default=None,
         alias="hdfsUser",
         description=(
             "HDFSUser is the user to access HDFS file system. It is ignored if either ccache or keytab is used."
         ),
     )
     krb_c_cache_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbCCacheSecret",
         description=(
             "KrbCCacheSecret is the secret selector for Kerberos ccache Either ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_config_config_map: Optional[v1.ConfigMapKeySelector] = Field(
-        None,
+        default=None,
         alias="krbConfigConfigMap",
         description=(
             "KrbConfig is the configmap selector for Kerberos config as string It must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_keytab_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbKeytabSecret",
         description=(
             "KrbKeytabSecret is the secret selector for Kerberos keytab Either ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_realm: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbRealm",
         description="KrbRealm is the Kerberos realm used with Kerberos keytab It must be set if keytab is used.",
     )
     krb_service_principal_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbServicePrincipalName",
         description=(
             "KrbServicePrincipalName is the principal name of Kerberos service It must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_username: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbUsername",
         description="KrbUsername is the Kerberos username used with Kerberos keytab It must be set if keytab is used.",
     )
     path_format: Optional[str] = Field(
-        None,
+        default=None,
         alias="pathFormat",
         description="PathFormat is defines the format of path to store a file. Can reference workflow variables",
     )
 
 
 class HTTPHeaderSource(BaseModel):
-    secret_key_ref: Optional[v1.SecretKeySelector] = Field(None, alias="secretKeyRef")
+    secret_key_ref: Optional[v1.SecretKeySelector] = Field(default=None, alias="secretKeyRef")
 
 
 class InfoResponse(BaseModel):
     links: Optional[List[Link]] = None
-    managed_namespace: Optional[str] = Field(None, alias="managedNamespace")
-    modals: Optional[Dict[str, bool]] = Field(None, title="which modals to show")
-    nav_color: Optional[str] = Field(None, alias="navColor")
+    managed_namespace: Optional[str] = Field(default=None, alias="managedNamespace")
+    modals: Optional[Dict[str, bool]] = Field(default=None, title="which modals to show")
+    nav_color: Optional[str] = Field(default=None, alias="navColor")
 
 
 class Memoize(BaseModel):
     cache: Cache = Field(..., description="Cache sets and configures the kind of cache")
     key: str = Field(..., description="Key is the key to use as the caching key")
     max_age: str = Field(
         ...,
@@ -865,953 +898,994 @@
 
 
 class Metrics(BaseModel):
     prometheus: List[Prometheus] = Field(..., description="Prometheus is a list of prometheus metrics to be emitted")
 
 
 class OAuth2Auth(BaseModel):
-    client_id_secret: Optional[v1.SecretKeySelector] = Field(None, alias="clientIDSecret")
-    client_secret_secret: Optional[v1.SecretKeySelector] = Field(None, alias="clientSecretSecret")
-    endpoint_params: Optional[List[OAuth2EndpointParam]] = Field(None, alias="endpointParams")
+    client_id_secret: Optional[v1.SecretKeySelector] = Field(default=None, alias="clientIDSecret")
+    client_secret_secret: Optional[v1.SecretKeySelector] = Field(default=None, alias="clientSecretSecret")
+    endpoint_params: Optional[List[OAuth2EndpointParam]] = Field(default=None, alias="endpointParams")
     scopes: Optional[List[str]] = None
-    token_url_secret: Optional[v1.SecretKeySelector] = Field(None, alias="tokenURLSecret")
+    token_url_secret: Optional[v1.SecretKeySelector] = Field(default=None, alias="tokenURLSecret")
 
 
 class OSSArtifact(BaseModel):
     access_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKeySecret", description="AccessKeySecret is the secret selector to the bucket's access key"
+        default=None,
+        alias="accessKeySecret",
+        description="AccessKeySecret is the secret selector to the bucket's access key",
     )
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     create_bucket_if_not_present: Optional[bool] = Field(
-        None,
+        default=None,
         alias="createBucketIfNotPresent",
         description=(
             "CreateBucketIfNotPresent tells the driver to attempt to create the OSS bucket for output artifacts, if it"
             " doesn't exist"
         ),
     )
-    endpoint: Optional[str] = Field(None, description="Endpoint is the hostname of the bucket endpoint")
+    endpoint: Optional[str] = Field(default=None, description="Endpoint is the hostname of the bucket endpoint")
     key: str = Field(..., description="Key is the path in the bucket where the artifact resides")
     lifecycle_rule: Optional[OSSLifecycleRule] = Field(
-        None, alias="lifecycleRule", description="LifecycleRule specifies how to manage bucket's lifecycle"
+        default=None, alias="lifecycleRule", description="LifecycleRule specifies how to manage bucket's lifecycle"
     )
     secret_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKeySecret", description="SecretKeySecret is the secret selector to the bucket's secret key"
+        default=None,
+        alias="secretKeySecret",
+        description="SecretKeySecret is the secret selector to the bucket's secret key",
     )
     security_token: Optional[str] = Field(
-        None,
+        default=None,
         alias="securityToken",
         description=(
             "SecurityToken is the user's temporary security token. For more details, check out:"
             " https://www.alibabacloud.com/help/doc-detail/100624.htm"
         ),
     )
 
 
 class OSSArtifactRepository(BaseModel):
     access_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKeySecret", description="AccessKeySecret is the secret selector to the bucket's access key"
+        default=None,
+        alias="accessKeySecret",
+        description="AccessKeySecret is the secret selector to the bucket's access key",
     )
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     create_bucket_if_not_present: Optional[bool] = Field(
-        None,
+        default=None,
         alias="createBucketIfNotPresent",
         description=(
             "CreateBucketIfNotPresent tells the driver to attempt to create the OSS bucket for output artifacts, if it"
             " doesn't exist"
         ),
     )
-    endpoint: Optional[str] = Field(None, description="Endpoint is the hostname of the bucket endpoint")
+    endpoint: Optional[str] = Field(default=None, description="Endpoint is the hostname of the bucket endpoint")
     key_format: Optional[str] = Field(
-        None,
+        default=None,
         alias="keyFormat",
         description="KeyFormat is defines the format of how to store keys. Can reference workflow variables",
     )
     lifecycle_rule: Optional[OSSLifecycleRule] = Field(
-        None, alias="lifecycleRule", description="LifecycleRule specifies how to manage bucket's lifecycle"
+        default=None, alias="lifecycleRule", description="LifecycleRule specifies how to manage bucket's lifecycle"
     )
     secret_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKeySecret", description="SecretKeySecret is the secret selector to the bucket's secret key"
+        default=None,
+        alias="secretKeySecret",
+        description="SecretKeySecret is the secret selector to the bucket's secret key",
     )
     security_token: Optional[str] = Field(
-        None,
+        default=None,
         alias="securityToken",
         description=(
             "SecurityToken is the user's temporary security token. For more details, check out:"
             " https://www.alibabacloud.com/help/doc-detail/100624.htm"
         ),
     )
 
 
 class RetryAffinity(BaseModel):
-    node_anti_affinity: Optional[RetryNodeAntiAffinity] = Field(None, alias="nodeAntiAffinity")
+    node_anti_affinity: Optional[RetryNodeAntiAffinity] = Field(default=None, alias="nodeAntiAffinity")
 
 
 class RetryStrategy(BaseModel):
     affinity: Optional[RetryAffinity] = Field(
-        None, description="Affinity prevents running workflow's step on the same host"
+        default=None, description="Affinity prevents running workflow's step on the same host"
     )
-    backoff: Optional[Backoff] = Field(None, description="Backoff is a backoff strategy")
+    backoff: Optional[Backoff] = Field(default=None, description="Backoff is a backoff strategy")
     expression: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Expression is a condition expression for when a node will be retried. If it evaluates to false, the node"
             " will not be retried and the retry strategy will be ignored"
         ),
     )
     limit: Optional[intstr.IntOrString] = Field(
-        None,
+        default=None,
         description=(
             "Limit is the maximum number of retry attempts when retrying a container. It does not include the original"
             " container; the maximum number of total attempts will be `limit + 1`."
         ),
     )
     retry_policy: Optional[str] = Field(
-        None, alias="retryPolicy", description="RetryPolicy is a policy of NodePhase statuses that will be retried"
+        default=None,
+        alias="retryPolicy",
+        description="RetryPolicy is a policy of NodePhase statuses that will be retried",
     )
 
 
 class S3EncryptionOptions(BaseModel):
     enable_encryption: Optional[bool] = Field(
-        None,
+        default=None,
         alias="enableEncryption",
         description=(
             "EnableEncryption tells the driver to encrypt objects if set to true. If kmsKeyId and"
             " serverSideCustomerKeySecret are not set, SSE-S3 will be used"
         ),
     )
     kms_encryption_context: Optional[str] = Field(
-        None,
+        default=None,
         alias="kmsEncryptionContext",
         description=(
             "KmsEncryptionContext is a json blob that contains an encryption context. See"
             " https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context for more information"
         ),
     )
     kms_key_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="kmsKeyId",
         description="KMSKeyId tells the driver to encrypt the object using the specified KMS Key.",
     )
     server_side_customer_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="serverSideCustomerKeySecret",
         description=(
             "ServerSideCustomerKeySecret tells the driver to encrypt the output artifacts using SSE-C with the"
             " specified secret."
         ),
     )
 
 
 class SemaphoreRef(BaseModel):
     config_map_key_ref: Optional[v1.ConfigMapKeySelector] = Field(
-        None, alias="configMapKeyRef", description="ConfigMapKeyRef is configmap selector for Semaphore configuration"
+        default=None,
+        alias="configMapKeyRef",
+        description="ConfigMapKeyRef is configmap selector for Semaphore configuration",
     )
 
 
 class Sequence(BaseModel):
     count: Optional[intstr.IntOrString] = Field(
-        None, description="Count is number of elements in the sequence (default: 0). Not to be used with end"
+        default=None, description="Count is number of elements in the sequence (default: 0). Not to be used with end"
     )
     end: Optional[intstr.IntOrString] = Field(
-        None, description="Number at which to end the sequence (default: 0). Not to be used with Count"
+        default=None, description="Number at which to end the sequence (default: 0). Not to be used with Count"
     )
     format: Optional[str] = Field(
-        None, description="Format is a printf format string to format the value in the sequence"
+        default=None, description="Format is a printf format string to format the value in the sequence"
+    )
+    start: Optional[intstr.IntOrString] = Field(
+        default=None, description="Number at which to start the sequence (default: 0)"
     )
-    start: Optional[intstr.IntOrString] = Field(None, description="Number at which to start the sequence (default: 0)")
 
 
 class SubmitOpts(BaseModel):
-    annotations: Optional[str] = Field(None, description="Annotations adds to metadata.labels")
+    annotations: Optional[str] = Field(default=None, description="Annotations adds to metadata.labels")
     dry_run: Optional[bool] = Field(
-        None,
+        default=None,
         alias="dryRun",
         description=(
             "DryRun validates the workflow on the client-side without creating it. This option is not supported in API"
         ),
     )
-    entry_point: Optional[str] = Field(None, alias="entryPoint", description="Entrypoint overrides spec.entrypoint")
+    entry_point: Optional[str] = Field(
+        default=None, alias="entryPoint", description="Entrypoint overrides spec.entrypoint"
+    )
     generate_name: Optional[str] = Field(
-        None, alias="generateName", description="GenerateName overrides metadata.generateName"
+        default=None, alias="generateName", description="GenerateName overrides metadata.generateName"
     )
-    labels: Optional[str] = Field(None, description="Labels adds to metadata.labels")
-    name: Optional[str] = Field(None, description="Name overrides metadata.name")
+    labels: Optional[str] = Field(default=None, description="Labels adds to metadata.labels")
+    name: Optional[str] = Field(default=None, description="Name overrides metadata.name")
     owner_reference: Optional[v1_1.OwnerReference] = Field(
-        None, alias="ownerReference", description="OwnerReference creates a metadata.ownerReference"
+        default=None, alias="ownerReference", description="OwnerReference creates a metadata.ownerReference"
     )
-    parameters: Optional[List[str]] = Field(None, description="Parameters passes input parameters to workflow")
+    parameters: Optional[List[str]] = Field(default=None, description="Parameters passes input parameters to workflow")
     pod_priority_class_name: Optional[str] = Field(
-        None, alias="podPriorityClassName", description="Set the podPriorityClassName of the workflow"
+        default=None, alias="podPriorityClassName", description="Set the podPriorityClassName of the workflow"
     )
     priority: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Priority is used if controller is configured to process limited number of workflows in parallel, higher"
             " priority workflows are processed first."
         ),
     )
     server_dry_run: Optional[bool] = Field(
-        None,
+        default=None,
         alias="serverDryRun",
         description="ServerDryRun validates the workflow on the server-side without creating it",
     )
     service_account: Optional[str] = Field(
-        None,
+        default=None,
         alias="serviceAccount",
         description="ServiceAccount runs all pods in the workflow using specified ServiceAccount.",
     )
 
 
 class Synchronization(BaseModel):
-    mutex: Optional[Mutex] = Field(None, description="Mutex holds the Mutex lock details")
-    semaphore: Optional[SemaphoreRef] = Field(None, description="Semaphore holds the Semaphore configuration")
+    mutex: Optional[Mutex] = Field(default=None, description="Mutex holds the Mutex lock details")
+    semaphore: Optional[SemaphoreRef] = Field(default=None, description="Semaphore holds the Semaphore configuration")
 
 
 class ValueFrom(BaseModel):
     config_map_key_ref: Optional[v1.ConfigMapKeySelector] = Field(
-        None,
+        default=None,
         alias="configMapKeyRef",
         description="ConfigMapKeyRef is configmap selector for input parameter configuration",
     )
     default: Optional[str] = Field(
-        None,
+        default=None,
         description="Default specifies a value to be used if retrieving the value from the specified source fails",
     )
     event: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Selector (https://github.com/antonmedv/expr) that is evaluated against the event to get the value of the"
             " parameter. E.g. `payload.message`"
         ),
     )
     expression: Optional[str] = Field(
-        None, description="Expression, if defined, is evaluated to specify the value for the parameter"
+        default=None, description="Expression, if defined, is evaluated to specify the value for the parameter"
     )
     jq_filter: Optional[str] = Field(
-        None, alias="jqFilter", description="JQFilter expression against the resource object in resource templates"
+        default=None,
+        alias="jqFilter",
+        description="JQFilter expression against the resource object in resource templates",
     )
     json_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="jsonPath",
         description="JSONPath of a resource to retrieve an output parameter value from in resource templates",
     )
     parameter: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Parameter reference to a step or dag task in which to retrieve an output parameter value from (e.g."
             " '{{steps.mystep.outputs.myparam}}')"
         ),
     )
     path: Optional[str] = Field(
-        None, description="Path in the container to retrieve an output parameter value from in container templates"
+        default=None,
+        description="Path in the container to retrieve an output parameter value from in container templates",
     )
     supplied: Optional[SuppliedValueFrom] = Field(
-        None, description="Supplied value to be filled in directly, either through the CLI, API, etc."
+        default=None, description="Supplied value to be filled in directly, either through the CLI, API, etc."
     )
 
 
 class WorkflowSubmitRequest(BaseModel):
     namespace: Optional[str] = None
-    resource_kind: Optional[str] = Field(None, alias="resourceKind")
-    resource_name: Optional[str] = Field(None, alias="resourceName")
-    submit_options: Optional[SubmitOpts] = Field(None, alias="submitOptions")
+    resource_kind: Optional[str] = Field(default=None, alias="resourceKind")
+    resource_name: Optional[str] = Field(default=None, alias="resourceName")
+    submit_options: Optional[SubmitOpts] = Field(default=None, alias="submitOptions")
 
 
 class HTTPAuth(BaseModel):
-    basic_auth: Optional[BasicAuth] = Field(None, alias="basicAuth")
-    client_cert: Optional[ClientCertAuth] = Field(None, alias="clientCert")
+    basic_auth: Optional[BasicAuth] = Field(default=None, alias="basicAuth")
+    client_cert: Optional[ClientCertAuth] = Field(default=None, alias="clientCert")
     oauth2: Optional[OAuth2Auth] = None
 
 
 class HTTPHeader(BaseModel):
     name: str
     value: Optional[str] = None
-    value_from: Optional[HTTPHeaderSource] = Field(None, alias="valueFrom")
+    value_from: Optional[HTTPHeaderSource] = Field(default=None, alias="valueFrom")
 
 
 class Parameter(BaseModel):
     default: Optional[str] = Field(
-        None, description="Default is the default value to use for an input parameter if a value was not supplied"
+        default=None,
+        description="Default is the default value to use for an input parameter if a value was not supplied",
     )
-    description: Optional[str] = Field(None, description="Description is the parameter description")
+    description: Optional[str] = Field(default=None, description="Description is the parameter description")
     enum: Optional[List[str]] = Field(
-        None, description="Enum holds a list of string values to choose from, for the actual value of the parameter"
+        default=None,
+        description="Enum holds a list of string values to choose from, for the actual value of the parameter",
     )
     global_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="globalName",
         description=(
             "GlobalName exports an output parameter to the global scope, making it available as"
             " '{{io.argoproj.workflow.v1alpha1.outputs.parameters.XXXX}} and in workflow.status.outputs.parameters"
         ),
     )
     name: str = Field(..., description="Name is the parameter name")
     value: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Value is the literal value to use for the parameter. If specified in the context of an input parameter,"
             " the value takes precedence over any passed values"
         ),
     )
     value_from: Optional[ValueFrom] = Field(
-        None, alias="valueFrom", description="ValueFrom is the source for the output parameter's value"
+        default=None, alias="valueFrom", description="ValueFrom is the source for the output parameter's value"
     )
 
 
 class PodGC(BaseModel):
     label_selector: Optional[v1_1.LabelSelector] = Field(
-        None,
+        default=None,
         alias="labelSelector",
         description=(
             "LabelSelector is the label selector to check if the pods match the labels before being added to the pod"
             " GC queue."
         ),
     )
     strategy: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Strategy is the strategy to use. One of "OnPodCompletion", "OnPodSuccess", "OnWorkflowCompletion",'
             ' "OnWorkflowSuccess"'
         ),
     )
 
 
 class S3Artifact(BaseModel):
     access_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKeySecret", description="AccessKeySecret is the secret selector to the bucket's access key"
+        default=None,
+        alias="accessKeySecret",
+        description="AccessKeySecret is the secret selector to the bucket's access key",
     )
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     create_bucket_if_not_present: Optional[CreateS3BucketOptions] = Field(
-        None,
+        default=None,
         alias="createBucketIfNotPresent",
         description=(
             "CreateBucketIfNotPresent tells the driver to attempt to create the S3 bucket for output artifacts, if it"
             " doesn't exist. Setting Enabled Encryption will apply either SSE-S3 to the bucket if KmsKeyId is not set"
             " or SSE-KMS if it is."
         ),
     )
-    encryption_options: Optional[S3EncryptionOptions] = Field(None, alias="encryptionOptions")
-    endpoint: Optional[str] = Field(None, description="Endpoint is the hostname of the bucket endpoint")
-    insecure: Optional[bool] = Field(None, description="Insecure will connect to the service with TLS")
-    key: Optional[str] = Field(None, description="Key is the key in the bucket where the artifact resides")
-    region: Optional[str] = Field(None, description="Region contains the optional bucket region")
+    encryption_options: Optional[S3EncryptionOptions] = Field(default=None, alias="encryptionOptions")
+    endpoint: Optional[str] = Field(default=None, description="Endpoint is the hostname of the bucket endpoint")
+    insecure: Optional[bool] = Field(default=None, description="Insecure will connect to the service with TLS")
+    key: Optional[str] = Field(default=None, description="Key is the key in the bucket where the artifact resides")
+    region: Optional[str] = Field(default=None, description="Region contains the optional bucket region")
     role_arn: Optional[str] = Field(
-        None, alias="roleARN", description="RoleARN is the Amazon Resource Name (ARN) of the role to assume."
+        default=None, alias="roleARN", description="RoleARN is the Amazon Resource Name (ARN) of the role to assume."
     )
     secret_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKeySecret", description="SecretKeySecret is the secret selector to the bucket's secret key"
+        default=None,
+        alias="secretKeySecret",
+        description="SecretKeySecret is the secret selector to the bucket's secret key",
     )
     use_sdk_creds: Optional[bool] = Field(
-        None,
+        default=None,
         alias="useSDKCreds",
         description="UseSDKCreds tells the driver to figure out credentials based on sdk defaults.",
     )
 
 
 class S3ArtifactRepository(BaseModel):
     access_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKeySecret", description="AccessKeySecret is the secret selector to the bucket's access key"
+        default=None,
+        alias="accessKeySecret",
+        description="AccessKeySecret is the secret selector to the bucket's access key",
     )
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     create_bucket_if_not_present: Optional[CreateS3BucketOptions] = Field(
-        None,
+        default=None,
         alias="createBucketIfNotPresent",
         description=(
             "CreateBucketIfNotPresent tells the driver to attempt to create the S3 bucket for output artifacts, if it"
             " doesn't exist. Setting Enabled Encryption will apply either SSE-S3 to the bucket if KmsKeyId is not set"
             " or SSE-KMS if it is."
         ),
     )
-    encryption_options: Optional[S3EncryptionOptions] = Field(None, alias="encryptionOptions")
-    endpoint: Optional[str] = Field(None, description="Endpoint is the hostname of the bucket endpoint")
-    insecure: Optional[bool] = Field(None, description="Insecure will connect to the service with TLS")
+    encryption_options: Optional[S3EncryptionOptions] = Field(default=None, alias="encryptionOptions")
+    endpoint: Optional[str] = Field(default=None, description="Endpoint is the hostname of the bucket endpoint")
+    insecure: Optional[bool] = Field(default=None, description="Insecure will connect to the service with TLS")
     key_format: Optional[str] = Field(
-        None,
+        default=None,
         alias="keyFormat",
         description="KeyFormat is defines the format of how to store keys. Can reference workflow variables",
     )
     key_prefix: Optional[str] = Field(
-        None,
+        default=None,
         alias="keyPrefix",
         description=(
             "KeyPrefix is prefix used as part of the bucket key in which the controller will store artifacts."
             " DEPRECATED. Use KeyFormat instead"
         ),
     )
-    region: Optional[str] = Field(None, description="Region contains the optional bucket region")
+    region: Optional[str] = Field(default=None, description="Region contains the optional bucket region")
     role_arn: Optional[str] = Field(
-        None, alias="roleARN", description="RoleARN is the Amazon Resource Name (ARN) of the role to assume."
+        default=None, alias="roleARN", description="RoleARN is the Amazon Resource Name (ARN) of the role to assume."
     )
     secret_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKeySecret", description="SecretKeySecret is the secret selector to the bucket's secret key"
+        default=None,
+        alias="secretKeySecret",
+        description="SecretKeySecret is the secret selector to the bucket's secret key",
     )
     use_sdk_creds: Optional[bool] = Field(
-        None,
+        default=None,
         alias="useSDKCreds",
         description="UseSDKCreds tells the driver to figure out credentials based on sdk defaults.",
     )
 
 
 class ArtifactRepository(BaseModel):
-    archive_logs: Optional[bool] = Field(None, alias="archiveLogs", description="ArchiveLogs enables log archiving")
+    archive_logs: Optional[bool] = Field(
+        default=None, alias="archiveLogs", description="ArchiveLogs enables log archiving"
+    )
     artifactory: Optional[ArtifactoryArtifactRepository] = Field(
-        None, description="Artifactory stores artifacts to JFrog Artifactory"
+        default=None, description="Artifactory stores artifacts to JFrog Artifactory"
     )
     azure: Optional[AzureArtifactRepository] = Field(
-        None, description="Azure stores artifact in an Azure Storage account"
+        default=None, description="Azure stores artifact in an Azure Storage account"
     )
-    gcs: Optional[GCSArtifactRepository] = Field(None, description="GCS stores artifact in a GCS object store")
-    hdfs: Optional[HDFSArtifactRepository] = Field(None, description="HDFS stores artifacts in HDFS")
+    gcs: Optional[GCSArtifactRepository] = Field(default=None, description="GCS stores artifact in a GCS object store")
+    hdfs: Optional[HDFSArtifactRepository] = Field(default=None, description="HDFS stores artifacts in HDFS")
     oss: Optional[OSSArtifactRepository] = Field(
-        None, description="OSS stores artifact in a OSS-compliant object store"
+        default=None, description="OSS stores artifact in a OSS-compliant object store"
+    )
+    s3: Optional[S3ArtifactRepository] = Field(
+        default=None, description="S3 stores artifact in a S3-compliant object store"
     )
-    s3: Optional[S3ArtifactRepository] = Field(None, description="S3 stores artifact in a S3-compliant object store")
 
 
 class ArtifactRepositoryRefStatus(BaseModel):
     artifact_repository: Optional[ArtifactRepository] = Field(
-        None,
+        default=None,
         alias="artifactRepository",
         description="The repository the workflow will use. This maybe empty before v3.1.",
     )
     config_map: Optional[str] = Field(
-        None, alias="configMap", description='The name of the config map. Defaults to "artifact-repositories".'
+        default=None, alias="configMap", description='The name of the config map. Defaults to "artifact-repositories".'
     )
     default: Optional[bool] = Field(
-        None, description="If this ref represents the default artifact repository, rather than a config map."
+        default=None, description="If this ref represents the default artifact repository, rather than a config map."
     )
     key: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'The config map key. Defaults to the value of the "workflows.argoproj.io/default-artifact-repository"'
             " annotation."
         ),
     )
     namespace: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "The namespace of the config map. Defaults to the workflow's namespace, or the controller's namespace (if"
             " found)."
         ),
     )
 
 
 class HTTP(BaseModel):
-    body: Optional[str] = Field(None, description="Body is content of the HTTP Request")
+    body: Optional[str] = Field(default=None, description="Body is content of the HTTP Request")
     body_from: Optional[HTTPBodySource] = Field(
-        None, alias="bodyFrom", description="BodyFrom is  content of the HTTP Request as Bytes"
+        default=None, alias="bodyFrom", description="BodyFrom is  content of the HTTP Request as Bytes"
     )
     headers: Optional[List[HTTPHeader]] = Field(
-        None, description="Headers are an optional list of headers to send with HTTP requests"
+        default=None, description="Headers are an optional list of headers to send with HTTP requests"
     )
     insecure_skip_verify: Optional[bool] = Field(
-        None,
+        default=None,
         alias="insecureSkipVerify",
         description="InsecureSkipVerify is a bool when if set to true will skip TLS verification for the HTTP client",
     )
-    method: Optional[str] = Field(None, description="Method is HTTP methods for HTTP Request")
+    method: Optional[str] = Field(default=None, description="Method is HTTP methods for HTTP Request")
     success_condition: Optional[str] = Field(
-        None,
+        default=None,
         alias="successCondition",
         description="SuccessCondition is an expression if evaluated to true is considered successful",
     )
     timeout_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="timeoutSeconds",
         description="TimeoutSeconds is request timeout for HTTP Request. Default is 30 seconds",
     )
     url: str = Field(..., description="URL of the HTTP Request")
 
 
 class HTTPArtifact(BaseModel):
-    auth: Optional[HTTPAuth] = Field(None, description="Auth contains information for client authentication")
+    auth: Optional[HTTPAuth] = Field(default=None, description="Auth contains information for client authentication")
     headers: Optional[List[Header]] = Field(
-        None, description="Headers are an optional list of headers to send with HTTP requests for artifacts"
+        default=None, description="Headers are an optional list of headers to send with HTTP requests for artifacts"
     )
     url: str = Field(..., description="URL of the artifact")
 
 
 class Artifact(BaseModel):
     archive: Optional[ArchiveStrategy] = Field(
-        None, description="Archive controls how the artifact will be saved to the artifact repository."
+        default=None, description="Archive controls how the artifact will be saved to the artifact repository."
     )
     archive_logs: Optional[bool] = Field(
-        None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
+        default=None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
     )
     artifact_gc: Optional[ArtifactGC] = Field(
-        None,
+        default=None,
         alias="artifactGC",
         description=(
             "ArtifactGC describes the strategy to use when to deleting an artifact from completed or deleted workflows"
         ),
     )
     artifactory: Optional[ArtifactoryArtifact] = Field(
-        None, description="Artifactory contains artifactory artifact location details"
+        default=None, description="Artifactory contains artifactory artifact location details"
     )
-    azure: Optional[AzureArtifact] = Field(None, description="Azure contains Azure Storage artifact location details")
-    deleted: Optional[bool] = Field(None, description="Has this been deleted?")
+    azure: Optional[AzureArtifact] = Field(
+        default=None, description="Azure contains Azure Storage artifact location details"
+    )
+    deleted: Optional[bool] = Field(default=None, description="Has this been deleted?")
     from_: Optional[str] = Field(
-        None, alias="from", description="From allows an artifact to reference an artifact from a previous step"
+        default=None, alias="from", description="From allows an artifact to reference an artifact from a previous step"
     )
     from_expression: Optional[str] = Field(
-        None,
+        default=None,
         alias="fromExpression",
         description="FromExpression, if defined, is evaluated to specify the value for the artifact",
     )
-    gcs: Optional[GCSArtifact] = Field(None, description="GCS contains GCS artifact location details")
-    git: Optional[GitArtifact] = Field(None, description="Git contains git artifact location details")
+    gcs: Optional[GCSArtifact] = Field(default=None, description="GCS contains GCS artifact location details")
+    git: Optional[GitArtifact] = Field(default=None, description="Git contains git artifact location details")
     global_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="globalName",
         description=(
             "GlobalName exports an output artifact to the global scope, making it available as"
             " '{{io.argoproj.workflow.v1alpha1.outputs.artifacts.XXXX}} and in workflow.status.outputs.artifacts"
         ),
     )
-    hdfs: Optional[HDFSArtifact] = Field(None, description="HDFS contains HDFS artifact location details")
-    http: Optional[HTTPArtifact] = Field(None, description="HTTP contains HTTP artifact location details")
+    hdfs: Optional[HDFSArtifact] = Field(default=None, description="HDFS contains HDFS artifact location details")
+    http: Optional[HTTPArtifact] = Field(default=None, description="HTTP contains HTTP artifact location details")
     mode: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "mode bits to use on this file, must be a value between 0 and 0777 set when loading input artifacts."
         ),
     )
     name: str = Field(..., description="name of the artifact. must be unique within a template's inputs/outputs.")
     optional: Optional[bool] = Field(
-        None, description="Make Artifacts optional, if Artifacts doesn't generate or exist"
+        default=None, description="Make Artifacts optional, if Artifacts doesn't generate or exist"
     )
-    oss: Optional[OSSArtifact] = Field(None, description="OSS contains OSS artifact location details")
-    path: Optional[str] = Field(None, description="Path is the container path to the artifact")
-    raw: Optional[RawArtifact] = Field(None, description="Raw contains raw artifact location details")
+    oss: Optional[OSSArtifact] = Field(default=None, description="OSS contains OSS artifact location details")
+    path: Optional[str] = Field(default=None, description="Path is the container path to the artifact")
+    raw: Optional[RawArtifact] = Field(default=None, description="Raw contains raw artifact location details")
     recurse_mode: Optional[bool] = Field(
-        None,
+        default=None,
         alias="recurseMode",
         description="If mode is set, apply the permission recursively into the artifact if it is a folder",
     )
-    s3: Optional[S3Artifact] = Field(None, description="S3 contains S3 artifact location details")
+    s3: Optional[S3Artifact] = Field(default=None, description="S3 contains S3 artifact location details")
     sub_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="subPath",
         description="SubPath allows an artifact to be sourced from a subpath within the specified source",
     )
 
 
 class ArtifactLocation(BaseModel):
     archive_logs: Optional[bool] = Field(
-        None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
+        default=None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
     )
     artifactory: Optional[ArtifactoryArtifact] = Field(
-        None, description="Artifactory contains artifactory artifact location details"
+        default=None, description="Artifactory contains artifactory artifact location details"
+    )
+    azure: Optional[AzureArtifact] = Field(
+        default=None, description="Azure contains Azure Storage artifact location details"
     )
-    azure: Optional[AzureArtifact] = Field(None, description="Azure contains Azure Storage artifact location details")
-    gcs: Optional[GCSArtifact] = Field(None, description="GCS contains GCS artifact location details")
-    git: Optional[GitArtifact] = Field(None, description="Git contains git artifact location details")
-    hdfs: Optional[HDFSArtifact] = Field(None, description="HDFS contains HDFS artifact location details")
-    http: Optional[HTTPArtifact] = Field(None, description="HTTP contains HTTP artifact location details")
-    oss: Optional[OSSArtifact] = Field(None, description="OSS contains OSS artifact location details")
-    raw: Optional[RawArtifact] = Field(None, description="Raw contains raw artifact location details")
-    s3: Optional[S3Artifact] = Field(None, description="S3 contains S3 artifact location details")
+    gcs: Optional[GCSArtifact] = Field(default=None, description="GCS contains GCS artifact location details")
+    git: Optional[GitArtifact] = Field(default=None, description="Git contains git artifact location details")
+    hdfs: Optional[HDFSArtifact] = Field(default=None, description="HDFS contains HDFS artifact location details")
+    http: Optional[HTTPArtifact] = Field(default=None, description="HTTP contains HTTP artifact location details")
+    oss: Optional[OSSArtifact] = Field(default=None, description="OSS contains OSS artifact location details")
+    raw: Optional[RawArtifact] = Field(default=None, description="Raw contains raw artifact location details")
+    s3: Optional[S3Artifact] = Field(default=None, description="S3 contains S3 artifact location details")
 
 
 class ArtifactNodeSpec(BaseModel):
     archive_location: Optional[ArtifactLocation] = Field(
-        None,
+        default=None,
         alias="archiveLocation",
         description="ArchiveLocation is the template-level Artifact location specification",
     )
     artifacts: Optional[Dict[str, Artifact]] = Field(
-        None, description="Artifacts maps artifact name to Artifact description"
+        default=None, description="Artifacts maps artifact name to Artifact description"
     )
 
 
 class ArtifactPaths(BaseModel):
     archive: Optional[ArchiveStrategy] = Field(
-        None, description="Archive controls how the artifact will be saved to the artifact repository."
+        default=None, description="Archive controls how the artifact will be saved to the artifact repository."
     )
     archive_logs: Optional[bool] = Field(
-        None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
+        default=None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
     )
     artifact_gc: Optional[ArtifactGC] = Field(
-        None,
+        default=None,
         alias="artifactGC",
         description=(
             "ArtifactGC describes the strategy to use when to deleting an artifact from completed or deleted workflows"
         ),
     )
     artifactory: Optional[ArtifactoryArtifact] = Field(
-        None, description="Artifactory contains artifactory artifact location details"
+        default=None, description="Artifactory contains artifactory artifact location details"
     )
-    azure: Optional[AzureArtifact] = Field(None, description="Azure contains Azure Storage artifact location details")
-    deleted: Optional[bool] = Field(None, description="Has this been deleted?")
+    azure: Optional[AzureArtifact] = Field(
+        default=None, description="Azure contains Azure Storage artifact location details"
+    )
+    deleted: Optional[bool] = Field(default=None, description="Has this been deleted?")
     from_: Optional[str] = Field(
-        None, alias="from", description="From allows an artifact to reference an artifact from a previous step"
+        default=None, alias="from", description="From allows an artifact to reference an artifact from a previous step"
     )
     from_expression: Optional[str] = Field(
-        None,
+        default=None,
         alias="fromExpression",
         description="FromExpression, if defined, is evaluated to specify the value for the artifact",
     )
-    gcs: Optional[GCSArtifact] = Field(None, description="GCS contains GCS artifact location details")
-    git: Optional[GitArtifact] = Field(None, description="Git contains git artifact location details")
+    gcs: Optional[GCSArtifact] = Field(default=None, description="GCS contains GCS artifact location details")
+    git: Optional[GitArtifact] = Field(default=None, description="Git contains git artifact location details")
     global_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="globalName",
         description=(
             "GlobalName exports an output artifact to the global scope, making it available as"
             " '{{io.argoproj.workflow.v1alpha1.outputs.artifacts.XXXX}} and in workflow.status.outputs.artifacts"
         ),
     )
-    hdfs: Optional[HDFSArtifact] = Field(None, description="HDFS contains HDFS artifact location details")
-    http: Optional[HTTPArtifact] = Field(None, description="HTTP contains HTTP artifact location details")
+    hdfs: Optional[HDFSArtifact] = Field(default=None, description="HDFS contains HDFS artifact location details")
+    http: Optional[HTTPArtifact] = Field(default=None, description="HTTP contains HTTP artifact location details")
     mode: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "mode bits to use on this file, must be a value between 0 and 0777 set when loading input artifacts."
         ),
     )
     name: str = Field(..., description="name of the artifact. must be unique within a template's inputs/outputs.")
     optional: Optional[bool] = Field(
-        None, description="Make Artifacts optional, if Artifacts doesn't generate or exist"
+        default=None, description="Make Artifacts optional, if Artifacts doesn't generate or exist"
     )
-    oss: Optional[OSSArtifact] = Field(None, description="OSS contains OSS artifact location details")
-    path: Optional[str] = Field(None, description="Path is the container path to the artifact")
-    raw: Optional[RawArtifact] = Field(None, description="Raw contains raw artifact location details")
+    oss: Optional[OSSArtifact] = Field(default=None, description="OSS contains OSS artifact location details")
+    path: Optional[str] = Field(default=None, description="Path is the container path to the artifact")
+    raw: Optional[RawArtifact] = Field(default=None, description="Raw contains raw artifact location details")
     recurse_mode: Optional[bool] = Field(
-        None,
+        default=None,
         alias="recurseMode",
         description="If mode is set, apply the permission recursively into the artifact if it is a folder",
     )
-    s3: Optional[S3Artifact] = Field(None, description="S3 contains S3 artifact location details")
+    s3: Optional[S3Artifact] = Field(default=None, description="S3 contains S3 artifact location details")
     sub_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="subPath",
         description="SubPath allows an artifact to be sourced from a subpath within the specified source",
     )
 
 
 class ContainerNode(BaseModel):
     args: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Arguments to the entrypoint. The container image's CMD is used if this is not provided. Variable"
             " references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be"
             " resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $,"
             ' which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal'
             ' "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable exists or'
             " not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     command: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Entrypoint array. Not executed within a shell. The container image's ENTRYPOINT is used if this is not"
             " provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable"
             " cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a"
             ' single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string'
             ' literal "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable'
             " exists or not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     dependencies: Optional[List[str]] = None
     env: Optional[List[v1.EnvVar]] = Field(
-        None, description="List of environment variables to set in the container. Cannot be updated."
+        default=None, description="List of environment variables to set in the container. Cannot be updated."
     )
     env_from: Optional[List[v1.EnvFromSource]] = Field(
-        None,
+        default=None,
         alias="envFrom",
         description=(
             "List of sources to populate environment variables in the container. The keys defined within a source must"
             " be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a"
             " key exists in multiple sources, the value associated with the last source will take precedence. Values"
             " defined by an Env with a duplicate key will take precedence. Cannot be updated."
         ),
     )
     image: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Container image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is"
             " optional to allow higher level config management to default or override container images in workload"
             " controllers like Deployments and StatefulSets."
         ),
     )
     image_pull_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="imagePullPolicy",
         description=(
             "Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or"
             " IfNotPresent otherwise. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/containers/images#updating-images"
         ),
     )
     lifecycle: Optional[v1.Lifecycle] = Field(
-        None,
+        default=None,
         description=(
             "Actions that the management system should take in response to container lifecycle events. Cannot be"
             " updated."
         ),
     )
     liveness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="livenessProbe",
         description=(
             "Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated."
             " More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     name: str = Field(
         ...,
         description=(
             "Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name"
             " (DNS_LABEL). Cannot be updated."
         ),
     )
     ports: Optional[List[v1.ContainerPort]] = Field(
-        None,
+        default=None,
         description=(
             "List of ports to expose from the container. Exposing a port here gives the system additional information"
             " about the network connections a container uses, but is primarily informational. Not specifying a port"
             ' here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0"'
             " address inside a container will be accessible from the network. Cannot be updated."
         ),
     )
     readiness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="readinessProbe",
         description=(
             "Periodic probe of container service readiness. Container will be removed from service endpoints if the"
             " probe fails. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     resources: Optional[v1.ResourceRequirements] = Field(
-        None,
+        default=None,
         description=(
             "Compute Resources required by this container. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
     security_context: Optional[v1.SecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext defines the security options the container should be run with. If set, the fields of"
             " SecurityContext override the equivalent fields of PodSecurityContext. More info:"
             " https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"
         ),
     )
     startup_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="startupProbe",
         description=(
             "StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are"
             " executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if"
             " the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a"
             " Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state"
             " operation. This cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     stdin: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a buffer for stdin in the container runtime. If this is not set,"
             " reads from stdin in the container will always result in EOF. Default is false."
         ),
     )
     stdin_once: Optional[bool] = Field(
-        None,
+        default=None,
         alias="stdinOnce",
         description=(
             "Whether the container runtime should close the stdin channel after it has been opened by a single attach."
             " When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is"
             " set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and"
             " then remains open and accepts data until the client disconnects, at which time stdin is closed and"
             " remains closed until the container is restarted. If this flag is false, a container processes that reads"
             " from stdin will never receive an EOF. Default is false"
         ),
     )
     termination_message_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePath",
         description=(
             "Optional: Path at which the file to which the container's termination message will be written is mounted"
             " into the container's filesystem. Message written is intended to be brief final status, such as an"
             " assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message"
             " length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be"
             " updated."
         ),
     )
     termination_message_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePolicy",
         description=(
             "Indicate how the termination message should be populated. File will use the contents of"
             " terminationMessagePath to populate the container status message on both success and failure."
             " FallbackToLogsOnError will use the last chunk of container log output if the termination message file is"
             " empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines,"
             " whichever is smaller. Defaults to File. Cannot be updated."
         ),
     )
     tty: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is"
             " false."
         ),
     )
     volume_devices: Optional[List[v1.VolumeDevice]] = Field(
-        None,
+        default=None,
         alias="volumeDevices",
         description="volumeDevices is the list of block devices to be used by the container.",
     )
     volume_mounts: Optional[List[v1.VolumeMount]] = Field(
-        None,
+        default=None,
         alias="volumeMounts",
         description="Pod volumes to mount into the container's filesystem. Cannot be updated.",
     )
     working_dir: Optional[str] = Field(
-        None,
+        default=None,
         alias="workingDir",
         description=(
             "Container's working directory. If not specified, the container runtime's default will be used, which"
             " might be configured in the container image. Cannot be updated."
         ),
     )
 
 
 class ContainerSetTemplate(BaseModel):
     containers: List[ContainerNode]
     retry_strategy: Optional[ContainerSetRetryStrategy] = Field(
-        None,
+        default=None,
         alias="retryStrategy",
         description=(
             "RetryStrategy describes how to retry a container nodes in the container set if it fails. Nbr of"
             " retries(default 0) and sleep duration between retries(default 0s, instant retry) can be set."
         ),
     )
-    volume_mounts: Optional[List[v1.VolumeMount]] = Field(None, alias="volumeMounts")
+    volume_mounts: Optional[List[v1.VolumeMount]] = Field(default=None, alias="volumeMounts")
 
 
 class DataSource(BaseModel):
     artifact_paths: Optional[ArtifactPaths] = Field(
-        None,
+        default=None,
         alias="artifactPaths",
         description="ArtifactPaths is a data transformation that collects a list of artifact paths",
     )
 
 
 class Inputs(BaseModel):
-    artifacts: Optional[List[Artifact]] = Field(None, description="Artifact are a list of artifacts passed as inputs")
+    artifacts: Optional[List[Artifact]] = Field(
+        default=None, description="Artifact are a list of artifacts passed as inputs"
+    )
     parameters: Optional[List[Parameter]] = Field(
-        None, description="Parameters are a list of parameters passed as inputs"
+        default=None, description="Parameters are a list of parameters passed as inputs"
     )
 
 
 class ManifestFrom(BaseModel):
     artifact: Artifact = Field(..., description="Artifact contains the artifact to use")
 
 
 class Outputs(BaseModel):
     artifacts: Optional[List[Artifact]] = Field(
-        None, description="Artifacts holds the list of output artifacts produced by a step"
+        default=None, description="Artifacts holds the list of output artifacts produced by a step"
     )
     exit_code: Optional[str] = Field(
-        None, alias="exitCode", description="ExitCode holds the exit code of a script template"
+        default=None, alias="exitCode", description="ExitCode holds the exit code of a script template"
     )
     parameters: Optional[List[Parameter]] = Field(
-        None, description="Parameters holds the list of output parameters produced by a step"
+        default=None, description="Parameters holds the list of output parameters produced by a step"
     )
-    result: Optional[str] = Field(None, description="Result holds the result (stdout) of a script template")
+    result: Optional[str] = Field(default=None, description="Result holds the result (stdout) of a script template")
 
 
 class ResourceTemplate(BaseModel):
     action: str = Field(
         ...,
         description=(
             "Action is the action to perform to the resource. Must be one of: get, create, apply, delete, replace,"
             " patch"
         ),
     )
     failure_condition: Optional[str] = Field(
-        None,
+        default=None,
         alias="failureCondition",
         description=(
             "FailureCondition is a label selector expression which describes the conditions of the k8s resource in"
             " which the step was considered failed"
         ),
     )
     flags: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Flags is a set of additional options passed to kubectl before submitting a resource I.e. to disable"
             ' resource validation: flags: [\n\t"--validate=false"  # disable resource validation\n]'
         ),
     )
-    manifest: Optional[str] = Field(None, description="Manifest contains the kubernetes manifest")
+    manifest: Optional[str] = Field(default=None, description="Manifest contains the kubernetes manifest")
     manifest_from: Optional[ManifestFrom] = Field(
-        None, alias="manifestFrom", description="ManifestFrom is the source for a single kubernetes manifest"
+        default=None, alias="manifestFrom", description="ManifestFrom is the source for a single kubernetes manifest"
     )
     merge_strategy: Optional[str] = Field(
-        None,
+        default=None,
         alias="mergeStrategy",
         description=(
             'MergeStrategy is the strategy used to merge a patch. It defaults to "strategic" Must be one of:'
             " strategic, merge, json"
         ),
     )
     set_owner_reference: Optional[bool] = Field(
-        None,
+        default=None,
         alias="setOwnerReference",
         description=(
             "SetOwnerReference sets the reference to the workflow on the OwnerReference of generated resource."
         ),
     )
     success_condition: Optional[str] = Field(
-        None,
+        default=None,
         alias="successCondition",
         description=(
             "SuccessCondition is a label selector expression which describes the conditions of the k8s resource in"
             " which it is acceptable to proceed to the following step"
         ),
     )
 
 
 class ScriptTemplate(BaseModel):
     args: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Arguments to the entrypoint. The container image's CMD is used if this is not provided. Variable"
             " references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be"
             " resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $,"
             ' which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal'
             ' "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable exists or'
             " not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     command: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Entrypoint array. Not executed within a shell. The container image's ENTRYPOINT is used if this is not"
             " provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable"
             " cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a"
             ' single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string'
             ' literal "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable'
             " exists or not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     env: Optional[List[v1.EnvVar]] = Field(
-        None, description="List of environment variables to set in the container. Cannot be updated."
+        default=None, description="List of environment variables to set in the container. Cannot be updated."
     )
     env_from: Optional[List[v1.EnvFromSource]] = Field(
-        None,
+        default=None,
         alias="envFrom",
         description=(
             "List of sources to populate environment variables in the container. The keys defined within a source must"
             " be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a"
             " key exists in multiple sources, the value associated with the last source will take precedence. Values"
             " defined by an Env with a duplicate key will take precedence. Cannot be updated."
         ),
@@ -1821,231 +1895,231 @@
         description=(
             "Container image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is"
             " optional to allow higher level config management to default or override container images in workload"
             " controllers like Deployments and StatefulSets."
         ),
     )
     image_pull_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="imagePullPolicy",
         description=(
             "Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or"
             " IfNotPresent otherwise. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/containers/images#updating-images"
         ),
     )
     lifecycle: Optional[v1.Lifecycle] = Field(
-        None,
+        default=None,
         description=(
             "Actions that the management system should take in response to container lifecycle events. Cannot be"
             " updated."
         ),
     )
     liveness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="livenessProbe",
         description=(
             "Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated."
             " More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name"
             " (DNS_LABEL). Cannot be updated."
         ),
     )
     ports: Optional[List[v1.ContainerPort]] = Field(
-        None,
+        default=None,
         description=(
             "List of ports to expose from the container. Exposing a port here gives the system additional information"
             " about the network connections a container uses, but is primarily informational. Not specifying a port"
             ' here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0"'
             " address inside a container will be accessible from the network. Cannot be updated."
         ),
     )
     readiness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="readinessProbe",
         description=(
             "Periodic probe of container service readiness. Container will be removed from service endpoints if the"
             " probe fails. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     resources: Optional[v1.ResourceRequirements] = Field(
-        None,
+        default=None,
         description=(
             "Compute Resources required by this container. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
     security_context: Optional[v1.SecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext defines the security options the container should be run with. If set, the fields of"
             " SecurityContext override the equivalent fields of PodSecurityContext. More info:"
             " https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"
         ),
     )
     source: str = Field(..., description="Source contains the source code of the script to execute")
     startup_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="startupProbe",
         description=(
             "StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are"
             " executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if"
             " the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a"
             " Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state"
             " operation. This cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     stdin: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a buffer for stdin in the container runtime. If this is not set,"
             " reads from stdin in the container will always result in EOF. Default is false."
         ),
     )
     stdin_once: Optional[bool] = Field(
-        None,
+        default=None,
         alias="stdinOnce",
         description=(
             "Whether the container runtime should close the stdin channel after it has been opened by a single attach."
             " When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is"
             " set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and"
             " then remains open and accepts data until the client disconnects, at which time stdin is closed and"
             " remains closed until the container is restarted. If this flag is false, a container processes that reads"
             " from stdin will never receive an EOF. Default is false"
         ),
     )
     termination_message_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePath",
         description=(
             "Optional: Path at which the file to which the container's termination message will be written is mounted"
             " into the container's filesystem. Message written is intended to be brief final status, such as an"
             " assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message"
             " length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be"
             " updated."
         ),
     )
     termination_message_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePolicy",
         description=(
             "Indicate how the termination message should be populated. File will use the contents of"
             " terminationMessagePath to populate the container status message on both success and failure."
             " FallbackToLogsOnError will use the last chunk of container log output if the termination message file is"
             " empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines,"
             " whichever is smaller. Defaults to File. Cannot be updated."
         ),
     )
     tty: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is"
             " false."
         ),
     )
     volume_devices: Optional[List[v1.VolumeDevice]] = Field(
-        None,
+        default=None,
         alias="volumeDevices",
         description="volumeDevices is the list of block devices to be used by the container.",
     )
     volume_mounts: Optional[List[v1.VolumeMount]] = Field(
-        None,
+        default=None,
         alias="volumeMounts",
         description="Pod volumes to mount into the container's filesystem. Cannot be updated.",
     )
     working_dir: Optional[str] = Field(
-        None,
+        default=None,
         alias="workingDir",
         description=(
             "Container's working directory. If not specified, the container runtime's default will be used, which"
             " might be configured in the container image. Cannot be updated."
         ),
     )
 
 
 class UserContainer(BaseModel):
     args: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Arguments to the entrypoint. The container image's CMD is used if this is not provided. Variable"
             " references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be"
             " resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $,"
             ' which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal'
             ' "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable exists or'
             " not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     command: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Entrypoint array. Not executed within a shell. The container image's ENTRYPOINT is used if this is not"
             " provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable"
             " cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a"
             ' single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string'
             ' literal "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable'
             " exists or not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     env: Optional[List[v1.EnvVar]] = Field(
-        None, description="List of environment variables to set in the container. Cannot be updated."
+        default=None, description="List of environment variables to set in the container. Cannot be updated."
     )
     env_from: Optional[List[v1.EnvFromSource]] = Field(
-        None,
+        default=None,
         alias="envFrom",
         description=(
             "List of sources to populate environment variables in the container. The keys defined within a source must"
             " be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a"
             " key exists in multiple sources, the value associated with the last source will take precedence. Values"
             " defined by an Env with a duplicate key will take precedence. Cannot be updated."
         ),
     )
     image: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Container image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is"
             " optional to allow higher level config management to default or override container images in workload"
             " controllers like Deployments and StatefulSets."
         ),
     )
     image_pull_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="imagePullPolicy",
         description=(
             "Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or"
             " IfNotPresent otherwise. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/containers/images#updating-images"
         ),
     )
     lifecycle: Optional[v1.Lifecycle] = Field(
-        None,
+        default=None,
         description=(
             "Actions that the management system should take in response to container lifecycle events. Cannot be"
             " updated."
         ),
     )
     liveness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="livenessProbe",
         description=(
             "Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated."
             " More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     mirror_volume_mounts: Optional[bool] = Field(
-        None,
+        default=None,
         alias="mirrorVolumeMounts",
         description=(
             "MirrorVolumeMounts will mount the same volumes specified in the main container to the container"
             " (including artifacts), at the same mountPaths. This enables dind daemon to partially see the same"
             " filesystem as the main container in order to use features such as docker volume binding"
         ),
     )
@@ -2053,223 +2127,228 @@
         ...,
         description=(
             "Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name"
             " (DNS_LABEL). Cannot be updated."
         ),
     )
     ports: Optional[List[v1.ContainerPort]] = Field(
-        None,
+        default=None,
         description=(
             "List of ports to expose from the container. Exposing a port here gives the system additional information"
             " about the network connections a container uses, but is primarily informational. Not specifying a port"
             ' here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0"'
             " address inside a container will be accessible from the network. Cannot be updated."
         ),
     )
     readiness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="readinessProbe",
         description=(
             "Periodic probe of container service readiness. Container will be removed from service endpoints if the"
             " probe fails. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     resources: Optional[v1.ResourceRequirements] = Field(
-        None,
+        default=None,
         description=(
             "Compute Resources required by this container. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
     security_context: Optional[v1.SecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext defines the security options the container should be run with. If set, the fields of"
             " SecurityContext override the equivalent fields of PodSecurityContext. More info:"
             " https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"
         ),
     )
     startup_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="startupProbe",
         description=(
             "StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are"
             " executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if"
             " the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a"
             " Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state"
             " operation. This cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     stdin: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a buffer for stdin in the container runtime. If this is not set,"
             " reads from stdin in the container will always result in EOF. Default is false."
         ),
     )
     stdin_once: Optional[bool] = Field(
-        None,
+        default=None,
         alias="stdinOnce",
         description=(
             "Whether the container runtime should close the stdin channel after it has been opened by a single attach."
             " When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is"
             " set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and"
             " then remains open and accepts data until the client disconnects, at which time stdin is closed and"
             " remains closed until the container is restarted. If this flag is false, a container processes that reads"
             " from stdin will never receive an EOF. Default is false"
         ),
     )
     termination_message_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePath",
         description=(
             "Optional: Path at which the file to which the container's termination message will be written is mounted"
             " into the container's filesystem. Message written is intended to be brief final status, such as an"
             " assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message"
             " length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be"
             " updated."
         ),
     )
     termination_message_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePolicy",
         description=(
             "Indicate how the termination message should be populated. File will use the contents of"
             " terminationMessagePath to populate the container status message on both success and failure."
             " FallbackToLogsOnError will use the last chunk of container log output if the termination message file is"
             " empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines,"
             " whichever is smaller. Defaults to File. Cannot be updated."
         ),
     )
     tty: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is"
             " false."
         ),
     )
     volume_devices: Optional[List[v1.VolumeDevice]] = Field(
-        None,
+        default=None,
         alias="volumeDevices",
         description="volumeDevices is the list of block devices to be used by the container.",
     )
     volume_mounts: Optional[List[v1.VolumeMount]] = Field(
-        None,
+        default=None,
         alias="volumeMounts",
         description="Pod volumes to mount into the container's filesystem. Cannot be updated.",
     )
     working_dir: Optional[str] = Field(
-        None,
+        default=None,
         alias="workingDir",
         description=(
             "Container's working directory. If not specified, the container runtime's default will be used, which"
             " might be configured in the container image. Cannot be updated."
         ),
     )
 
 
 class Arguments(BaseModel):
     artifacts: Optional[List[Artifact]] = Field(
-        None, description="Artifacts is the list of artifacts to pass to the template or workflow"
+        default=None, description="Artifacts is the list of artifacts to pass to the template or workflow"
     )
     parameters: Optional[List[Parameter]] = Field(
-        None, description="Parameters is the list of parameters to pass to the template or workflow"
+        default=None, description="Parameters is the list of parameters to pass to the template or workflow"
     )
 
 
 class ArtifactGCSpec(BaseModel):
     artifacts_by_node: Optional[Dict[str, ArtifactNodeSpec]] = Field(
-        None,
+        default=None,
         alias="artifactsByNode",
         description="ArtifactsByNode maps Node name to information pertaining to Artifacts on that Node",
     )
 
 
 class Data(BaseModel):
     source: DataSource = Field(..., description="Source sources external data into a data template")
     transformation: List[TransformationStep] = Field(
         ..., description="Transformation applies a set of transformations"
     )
 
 
 class LifecycleHook(BaseModel):
-    arguments: Optional[Arguments] = Field(None, description="Arguments hold arguments to the template")
+    arguments: Optional[Arguments] = Field(default=None, description="Arguments hold arguments to the template")
     expression: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Expression is a condition expression for when a node will be retried. If it evaluates to false, the node"
             " will not be retried and the retry strategy will be ignored"
         ),
     )
-    template: Optional[str] = Field(None, description="Template is the name of the template to execute by the hook")
+    template: Optional[str] = Field(
+        default=None, description="Template is the name of the template to execute by the hook"
+    )
     template_ref: Optional[TemplateRef] = Field(
-        None,
+        default=None,
         alias="templateRef",
         description="TemplateRef is the reference to the template resource to execute by the hook",
     )
 
 
 class NodeResult(BaseModel):
     message: Optional[str] = None
     outputs: Optional[Outputs] = None
     phase: Optional[str] = None
     progress: Optional[str] = None
 
 
 class NodeStatus(BaseModel):
     boundary_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="boundaryID",
         description=(
             "BoundaryID indicates the node ID of the associated template root node in which this node belongs to"
         ),
     )
-    children: Optional[List[str]] = Field(None, description="Children is a list of child node IDs")
+    children: Optional[List[str]] = Field(default=None, description="Children is a list of child node IDs")
     daemoned: Optional[bool] = Field(
-        None, description="Daemoned tracks whether or not this node was daemoned and need to be terminated"
+        default=None, description="Daemoned tracks whether or not this node was daemoned and need to be terminated"
     )
     display_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="displayName",
         description="DisplayName is a human readable representation of the node. Unique within a template boundary",
     )
     estimated_duration: Optional[int] = Field(
-        None, alias="estimatedDuration", description="EstimatedDuration in seconds."
+        default=None, alias="estimatedDuration", description="EstimatedDuration in seconds."
+    )
+    finished_at: Optional[v1_1.Time] = Field(
+        default=None, alias="finishedAt", description="Time at which this node completed"
     )
-    finished_at: Optional[v1_1.Time] = Field(None, alias="finishedAt", description="Time at which this node completed")
     host_node_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="hostNodeName",
         description="HostNodeName name of the Kubernetes node on which the Pod is running, if applicable",
     )
     id: str = Field(
         ...,
         description=(
             "ID is a unique identifier of a node within the worklow It is implemented as a hash of the node name,"
             " which makes the ID deterministic"
         ),
     )
     inputs: Optional[Inputs] = Field(
-        None,
+        default=None,
         description=(
             "Inputs captures input parameter values and artifact locations supplied to this template invocation"
         ),
     )
     memoization_status: Optional[MemoizationStatus] = Field(
-        None, alias="memoizationStatus", description="MemoizationStatus holds information about cached nodes"
+        default=None, alias="memoizationStatus", description="MemoizationStatus holds information about cached nodes"
     )
     message: Optional[str] = Field(
-        None, description="A human readable message indicating details about why the node is in this condition."
+        default=None,
+        description="A human readable message indicating details about why the node is in this condition.",
     )
     name: str = Field(..., description="Name is unique name in the node tree used to generate the node ID")
     outbound_nodes: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="outboundNodes",
         description=(
             'OutboundNodes tracks the node IDs which are considered "outbound" nodes to a template invocation. For'
             ' every invocation of a template, there are nodes which we considered as "outbound". Essentially, these'
             " are last nodes in the execution sequence to run, before the template is considered completed. These"
             " nodes are then connected as parents to a following step.\n\nIn the case of single pod steps (i.e."
             " container, script, resource templates), this list will be nil since the pod itself is already considered"
@@ -2277,1048 +2356,1080 @@
             " children). In the case of steps, outbound nodes are all the containers involved in the last step group."
             " NOTE: since templates are composable, the list of outbound nodes are carried upwards when a DAG/steps"
             " template invokes another DAG/steps template. In other words, the outbound nodes of a template, will be a"
             " superset of the outbound nodes of its last children."
         ),
     )
     outputs: Optional[Outputs] = Field(
-        None,
+        default=None,
         description=(
             "Outputs captures output parameter values and artifact locations produced by this template invocation"
         ),
     )
     phase: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Phase a simple, high-level summary of where the node is in its lifecycle. Can be used as a state machine."
         ),
     )
     pod_ip: Optional[str] = Field(
-        None, alias="podIP", description="PodIP captures the IP of the pod for daemoned steps"
+        default=None, alias="podIP", description="PodIP captures the IP of the pod for daemoned steps"
     )
-    progress: Optional[str] = Field(None, description="Progress to completion")
+    progress: Optional[str] = Field(default=None, description="Progress to completion")
     resources_duration: Optional[Dict[str, int]] = Field(
-        None,
+        default=None,
         alias="resourcesDuration",
         description=(
             "ResourcesDuration is indicative, but not accurate, resource duration. This is populated when the nodes"
             " completes."
         ),
     )
-    started_at: Optional[v1_1.Time] = Field(None, alias="startedAt", description="Time at which this node started")
+    started_at: Optional[v1_1.Time] = Field(
+        default=None, alias="startedAt", description="Time at which this node started"
+    )
     synchronization_status: Optional[NodeSynchronizationStatus] = Field(
-        None,
+        default=None,
         alias="synchronizationStatus",
         description="SynchronizationStatus is the synchronization status of the node",
     )
     template_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="templateName",
         description=(
             "TemplateName is the template name which this node corresponds to. Not applicable to virtual nodes (e.g."
             " Retry, StepGroup)"
         ),
     )
     template_ref: Optional[TemplateRef] = Field(
-        None,
+        default=None,
         alias="templateRef",
         description=(
             "TemplateRef is the reference to the template resource which this node corresponds to. Not applicable to"
             " virtual nodes (e.g. Retry, StepGroup)"
         ),
     )
     template_scope: Optional[str] = Field(
-        None,
+        default=None,
         alias="templateScope",
         description="TemplateScope is the template scope in which the template of this node was retrieved.",
     )
     type: str = Field(..., description="Type indicates type of node")
 
 
 class Submit(BaseModel):
     arguments: Optional[Arguments] = Field(
-        None, description="Arguments extracted from the event and then set as arguments to the workflow created."
+        default=None,
+        description="Arguments extracted from the event and then set as arguments to the workflow created.",
     )
     metadata: Optional[v1_1.ObjectMeta] = Field(
-        None, description="Metadata optional means to customize select fields of the workflow metadata"
+        default=None, description="Metadata optional means to customize select fields of the workflow metadata"
     )
     workflow_template_ref: WorkflowTemplateRef = Field(
         ..., alias="workflowTemplateRef", description="WorkflowTemplateRef the workflow template to submit"
     )
 
 
 class WorkflowEventBindingSpec(BaseModel):
     event: Event = Field(..., description="Event is the event to bind to")
-    submit: Optional[Submit] = Field(None, description="Submit is the workflow template to submit")
+    submit: Optional[Submit] = Field(default=None, description="Submit is the workflow template to submit")
 
 
 class WorkflowTaskSetStatus(BaseModel):
     nodes: Optional[Dict[str, NodeResult]] = None
 
 
 class WorkflowEventBinding(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ObjectMeta
     spec: WorkflowEventBindingSpec
 
 
 class WorkflowEventBindingList(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     items: List[WorkflowEventBinding]
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ListMeta
 
 
 class ClusterWorkflowTemplate(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ObjectMeta
     spec: WorkflowSpec
 
 
 class ClusterWorkflowTemplateCreateRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
     template: Optional[ClusterWorkflowTemplate] = None
 
 
 class ClusterWorkflowTemplateLintRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
     template: Optional[ClusterWorkflowTemplate] = None
 
 
 class ClusterWorkflowTemplateList(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     items: List[ClusterWorkflowTemplate]
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ListMeta
 
 
 class ClusterWorkflowTemplateUpdateRequest(BaseModel):
-    name: Optional[str] = Field(None, description="DEPRECATED: This field is ignored.")
+    name: Optional[str] = Field(default=None, description="DEPRECATED: This field is ignored.")
     template: Optional[ClusterWorkflowTemplate] = None
 
 
 class CreateCronWorkflowRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
-    cron_workflow: Optional[CronWorkflow] = Field(None, alias="cronWorkflow")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
+    cron_workflow: Optional[CronWorkflow] = Field(default=None, alias="cronWorkflow")
     namespace: Optional[str] = None
 
 
 class CronWorkflow(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ObjectMeta
     spec: CronWorkflowSpec
     status: Optional[CronWorkflowStatus] = None
 
 
 class CronWorkflowList(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     items: List[CronWorkflow]
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ListMeta
 
 
 class CronWorkflowSpec(BaseModel):
     concurrency_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="concurrencyPolicy",
         description="ConcurrencyPolicy is the K8s-style concurrency policy that will be used",
     )
     failed_jobs_history_limit: Optional[int] = Field(
-        None,
+        default=None,
         alias="failedJobsHistoryLimit",
         description="FailedJobsHistoryLimit is the number of failed jobs to be kept at a time",
     )
     schedule: str = Field(..., description="Schedule is a schedule to run the Workflow in Cron format")
     starting_deadline_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="startingDeadlineSeconds",
         description=(
             "StartingDeadlineSeconds is the K8s-style deadline that will limit the time a CronWorkflow will be run"
             " after its original scheduled time if it is missed."
         ),
     )
     successful_jobs_history_limit: Optional[int] = Field(
-        None,
+        default=None,
         alias="successfulJobsHistoryLimit",
         description="SuccessfulJobsHistoryLimit is the number of successful jobs to be kept at a time",
     )
     suspend: Optional[bool] = Field(
-        None, description="Suspend is a flag that will stop new CronWorkflows from running if set to true"
+        default=None, description="Suspend is a flag that will stop new CronWorkflows from running if set to true"
     )
     timezone: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Timezone is the timezone against which the cron schedule will be calculated, e.g. "Asia/Tokyo". Default'
             " is machine's local time."
         ),
     )
     workflow_metadata: Optional[v1_1.ObjectMeta] = Field(
-        None, alias="workflowMetadata", description="WorkflowMetadata contains some metadata of the workflow to be run"
+        default=None,
+        alias="workflowMetadata",
+        description="WorkflowMetadata contains some metadata of the workflow to be run",
     )
     workflow_spec: WorkflowSpec = Field(
         ..., alias="workflowSpec", description="WorkflowSpec is the spec of the workflow to be run"
     )
 
 
 class DAGTask(BaseModel):
     arguments: Optional[Arguments] = Field(
-        None, description="Arguments are the parameter and artifact arguments to the template"
+        default=None, description="Arguments are the parameter and artifact arguments to the template"
     )
     continue_on: Optional[ContinueOn] = Field(
-        None,
+        default=None,
         alias="continueOn",
         description=(
             "ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed"
             " states can be specified"
         ),
     )
     dependencies: Optional[List[str]] = Field(
-        None, description="Dependencies are name of other targets which this depends on"
+        default=None, description="Dependencies are name of other targets which this depends on"
     )
-    depends: Optional[str] = Field(None, description="Depends are name of other targets which this depends on")
+    depends: Optional[str] = Field(default=None, description="Depends are name of other targets which this depends on")
     hooks: Optional[Dict[str, LifecycleHook]] = Field(
-        None,
+        default=None,
         description=(
             "Hooks hold the lifecycle hook which is invoked at lifecycle of task, irrespective of the success,"
             " failure, or error status of the primary task"
         ),
     )
     inline: Optional[Template] = Field(
-        None, description="Inline is the template. Template must be empty if this is declared (and vice-versa)."
+        default=None,
+        description="Inline is the template. Template must be empty if this is declared (and vice-versa).",
     )
     name: str = Field(..., description="Name is the name of the target")
     on_exit: Optional[str] = Field(
-        None,
+        default=None,
         alias="onExit",
         description=(
             "OnExit is a template reference which is invoked at the end of the template, irrespective of the success,"
             " failure, or error of the primary template. DEPRECATED: Use Hooks[exit].Template instead."
         ),
     )
-    template: Optional[str] = Field(None, description="Name of template to execute")
+    template: Optional[str] = Field(default=None, description="Name of template to execute")
     template_ref: Optional[TemplateRef] = Field(
-        None, alias="templateRef", description="TemplateRef is the reference to the template resource to execute."
+        default=None,
+        alias="templateRef",
+        description="TemplateRef is the reference to the template resource to execute.",
     )
     when: Optional[str] = Field(
-        None, description="When is an expression in which the task should conditionally execute"
+        default=None, description="When is an expression in which the task should conditionally execute"
     )
     with_items: Optional[List[Item]] = Field(
-        None,
+        default=None,
         alias="withItems",
         description="WithItems expands a task into multiple parallel tasks from the items in the list",
     )
     with_param: Optional[str] = Field(
-        None,
+        default=None,
         alias="withParam",
         description=(
             "WithParam expands a task into multiple parallel tasks from the value in the parameter, which is expected"
             " to be a JSON list."
         ),
     )
     with_sequence: Optional[Sequence] = Field(
-        None, alias="withSequence", description="WithSequence expands a task into a numeric sequence"
+        default=None, alias="withSequence", description="WithSequence expands a task into a numeric sequence"
     )
 
 
 class DAGTemplate(BaseModel):
     fail_fast: Optional[bool] = Field(
-        None,
+        default=None,
         alias="failFast",
         description=(
             'This flag is for DAG logic. The DAG logic has a built-in "fail fast" feature to stop scheduling new'
             " steps, as soon as it detects that one of the DAG nodes is failed. Then it waits until all DAG nodes are"
             " completed before failing the DAG itself. The FailFast flag default is true,  if set to false, it will"
             " allow a DAG to run all branches of the DAG to completion (either success or failure), regardless of the"
             " failed outcomes of branches in the DAG. More info and example about this feature at"
             " https://github.com/argoproj/argo-workflows/issues/1442"
         ),
     )
-    target: Optional[str] = Field(None, description="Target are one or more names of targets to execute in a DAG")
+    target: Optional[str] = Field(
+        default=None, description="Target are one or more names of targets to execute in a DAG"
+    )
     tasks: List[DAGTask] = Field(..., description="Tasks are a list of DAG tasks")
 
 
 class LintCronWorkflowRequest(BaseModel):
-    cron_workflow: Optional[CronWorkflow] = Field(None, alias="cronWorkflow")
+    cron_workflow: Optional[CronWorkflow] = Field(default=None, alias="cronWorkflow")
     namespace: Optional[str] = None
 
 
 class ParallelSteps(BaseModel):
     __root__: List[WorkflowStep]
 
 
 class Template(BaseModel):
     active_deadline_seconds: Optional[intstr.IntOrString] = Field(
-        None,
+        default=None,
         alias="activeDeadlineSeconds",
         description=(
             "Optional duration in seconds relative to the StartTime that the pod may be active on a node before the"
             " system actively tries to terminate the pod; value must be positive integer This field is only applicable"
             " to container and script templates."
         ),
     )
     affinity: Optional[v1.Affinity] = Field(
-        None,
+        default=None,
         description=(
             "Affinity sets the pod's scheduling constraints Overrides the affinity set at the workflow level (if any)"
         ),
     )
     archive_location: Optional[ArtifactLocation] = Field(
-        None,
+        default=None,
         alias="archiveLocation",
         description=(
             "Location in which all files related to the step will be stored (logs, artifacts, etc...). Can be"
             " overridden by individual items in Outputs. If omitted, will use the default artifact repository location"
             " configured in the controller, appended with the <workflowname>/<nodename> in the key."
         ),
     )
     automount_service_account_token: Optional[bool] = Field(
-        None,
+        default=None,
         alias="automountServiceAccountToken",
         description=(
             "AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in"
             " pods. ServiceAccountName of ExecutorConfig must be specified if this value is false."
         ),
     )
     container: Optional[v1.Container] = Field(
-        None, description="Container is the main container image to run in the pod"
+        default=None, description="Container is the main container image to run in the pod"
     )
     container_set: Optional[ContainerSetTemplate] = Field(
-        None, alias="containerSet", description="ContainerSet groups multiple containers within a single pod."
+        default=None, alias="containerSet", description="ContainerSet groups multiple containers within a single pod."
     )
     daemon: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Deamon will allow a workflow to proceed to the next step so long as the container reaches readiness"
         ),
     )
-    dag: Optional[DAGTemplate] = Field(None, description="DAG template subtype which runs a DAG")
-    data: Optional[Data] = Field(None, description="Data is a data template")
+    dag: Optional[DAGTemplate] = Field(default=None, description="DAG template subtype which runs a DAG")
+    data: Optional[Data] = Field(default=None, description="Data is a data template")
     executor: Optional[ExecutorConfig] = Field(
-        None, description="Executor holds configurations of the executor container."
+        default=None, description="Executor holds configurations of the executor container."
     )
     fail_fast: Optional[bool] = Field(
-        None,
+        default=None,
         alias="failFast",
         description=(
             "FailFast, if specified, will fail this template if any of its child pods has failed. This is useful for"
             " when this template is expanded with `withItems`, etc."
         ),
     )
     host_aliases: Optional[List[v1.HostAlias]] = Field(
-        None,
+        default=None,
         alias="hostAliases",
         description="HostAliases is an optional list of hosts and IPs that will be injected into the pod spec",
     )
-    http: Optional[HTTP] = Field(None, description="HTTP makes a HTTP request")
+    http: Optional[HTTP] = Field(default=None, description="HTTP makes a HTTP request")
     init_containers: Optional[List[UserContainer]] = Field(
-        None,
+        default=None,
         alias="initContainers",
         description="InitContainers is a list of containers which run before the main container.",
     )
     inputs: Optional[Inputs] = Field(
-        None, description="Inputs describe what inputs parameters and artifacts are supplied to this template"
+        default=None, description="Inputs describe what inputs parameters and artifacts are supplied to this template"
     )
     memoize: Optional[Memoize] = Field(
-        None, description="Memoize allows templates to use outputs generated from already executed templates"
+        default=None, description="Memoize allows templates to use outputs generated from already executed templates"
     )
     metadata: Optional[Metadata] = Field(
-        None, description="Metdata sets the pods's metadata, i.e. annotations and labels"
+        default=None, description="Metdata sets the pods's metadata, i.e. annotations and labels"
+    )
+    metrics: Optional[Metrics] = Field(
+        default=None, description="Metrics are a list of metrics emitted from this template"
     )
-    metrics: Optional[Metrics] = Field(None, description="Metrics are a list of metrics emitted from this template")
-    name: Optional[str] = Field(None, description="Name is the name of the template")
+    name: Optional[str] = Field(default=None, description="Name is the name of the template")
     node_selector: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         alias="nodeSelector",
         description=(
             "NodeSelector is a selector to schedule this step of the workflow to be run on the selected node(s)."
             " Overrides the selector set at the workflow level."
         ),
     )
     outputs: Optional[Outputs] = Field(
-        None, description="Outputs describe the parameters and artifacts that this template produces"
+        default=None, description="Outputs describe the parameters and artifacts that this template produces"
     )
     parallelism: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Parallelism limits the max total parallel pods that can execute at the same time within the boundaries of"
             " this template invocation. If additional steps/dag templates are invoked, the pods created by those"
             " templates will not be counted towards this total."
         ),
     )
-    plugin: Optional[Plugin] = Field(None, description="Plugin is a plugin template")
+    plugin: Optional[Plugin] = Field(default=None, description="Plugin is a plugin template")
     pod_spec_patch: Optional[str] = Field(
-        None,
+        default=None,
         alias="podSpecPatch",
         description=(
             "PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of"
             " container fields which are not strings (e.g. resource limits)."
         ),
     )
-    priority: Optional[int] = Field(None, description="Priority to apply to workflow pods.")
+    priority: Optional[int] = Field(default=None, description="Priority to apply to workflow pods.")
     priority_class_name: Optional[str] = Field(
-        None, alias="priorityClassName", description="PriorityClassName to apply to workflow pods."
+        default=None, alias="priorityClassName", description="PriorityClassName to apply to workflow pods."
     )
     resource: Optional[ResourceTemplate] = Field(
-        None, description="Resource template subtype which can run k8s resources"
+        default=None, description="Resource template subtype which can run k8s resources"
     )
     retry_strategy: Optional[RetryStrategy] = Field(
-        None, alias="retryStrategy", description="RetryStrategy describes how to retry a template when it fails"
+        default=None,
+        alias="retryStrategy",
+        description="RetryStrategy describes how to retry a template when it fails",
     )
     scheduler_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="schedulerName",
         description=(
             "If specified, the pod will be dispatched by specified scheduler. Or it will be dispatched by workflow"
             " scope scheduler if specified. If neither specified, the pod will be dispatched by default scheduler."
         ),
     )
-    script: Optional[ScriptTemplate] = Field(None, description="Script runs a portion of code against an interpreter")
+    script: Optional[ScriptTemplate] = Field(
+        default=None, description="Script runs a portion of code against an interpreter"
+    )
     security_context: Optional[v1.PodSecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to"
             " empty.  See type description for default values of each field."
         ),
     )
     service_account_name: Optional[str] = Field(
-        None, alias="serviceAccountName", description="ServiceAccountName to apply to workflow pods"
+        default=None, alias="serviceAccountName", description="ServiceAccountName to apply to workflow pods"
     )
     sidecars: Optional[List[UserContainer]] = Field(
-        None,
+        default=None,
         description=(
             "Sidecars is a list of containers which run alongside the main container Sidecars are automatically killed"
             " when the main container completes"
         ),
     )
     steps: Optional[List[ParallelSteps]] = Field(
-        None, description="Steps define a series of sequential/parallel workflow steps"
+        default=None, description="Steps define a series of sequential/parallel workflow steps"
     )
     suspend: Optional[SuspendTemplate] = Field(
-        None, description="Suspend template subtype which can suspend a workflow when reaching the step"
+        default=None, description="Suspend template subtype which can suspend a workflow when reaching the step"
     )
     synchronization: Optional[Synchronization] = Field(
-        None, description="Synchronization holds synchronization lock configuration for this template"
+        default=None, description="Synchronization holds synchronization lock configuration for this template"
     )
     timeout: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Timeout allows to set the total node execution timeout duration counting from the node's start time. This"
             " duration also includes time in which the node spends in Pending state. This duration may not be applied"
             " to Step or DAG templates."
         ),
     )
-    tolerations: Optional[List[v1.Toleration]] = Field(None, description="Tolerations to apply to workflow pods.")
+    tolerations: Optional[List[v1.Toleration]] = Field(
+        default=None, description="Tolerations to apply to workflow pods."
+    )
     volumes: Optional[List[v1.Volume]] = Field(
-        None, description="Volumes is a list of volumes that can be mounted by containers in a template."
+        default=None, description="Volumes is a list of volumes that can be mounted by containers in a template."
     )
 
 
 class UpdateCronWorkflowRequest(BaseModel):
-    cron_workflow: Optional[CronWorkflow] = Field(None, alias="cronWorkflow")
-    name: Optional[str] = Field(None, description="DEPRECATED: This field is ignored.")
+    cron_workflow: Optional[CronWorkflow] = Field(default=None, alias="cronWorkflow")
+    name: Optional[str] = Field(default=None, description="DEPRECATED: This field is ignored.")
     namespace: Optional[str] = None
 
 
 class Workflow(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ObjectMeta
     spec: WorkflowSpec
     status: Optional[WorkflowStatus] = None
 
 
 class WorkflowCreateRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
-    instance_id: Optional[str] = Field(None, alias="instanceID", description="This field is no longer used.")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
+    instance_id: Optional[str] = Field(default=None, alias="instanceID", description="This field is no longer used.")
     namespace: Optional[str] = None
-    server_dry_run: Optional[bool] = Field(None, alias="serverDryRun")
+    server_dry_run: Optional[bool] = Field(default=None, alias="serverDryRun")
     workflow: Optional[Workflow] = None
 
 
 class WorkflowLintRequest(BaseModel):
     namespace: Optional[str] = None
     workflow: Optional[Workflow] = None
 
 
 class WorkflowList(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     items: List[Workflow]
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ListMeta
 
 
 class WorkflowSpec(BaseModel):
     active_deadline_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="activeDeadlineSeconds",
         description=(
             "Optional duration in seconds relative to the workflow start time which the workflow is allowed to run"
             " before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate"
             " a Running workflow"
         ),
     )
     affinity: Optional[v1.Affinity] = Field(
-        None,
+        default=None,
         description=(
             "Affinity sets the scheduling constraints for all pods in the io.argoproj.workflow.v1alpha1. Can be"
             " overridden by an affinity specified in the template"
         ),
     )
     archive_logs: Optional[bool] = Field(
-        None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
+        default=None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
     )
     arguments: Optional[Arguments] = Field(
-        None,
+        default=None,
         description=(
             "Arguments contain the parameters and artifacts sent to the workflow entrypoint Parameters are"
             " referencable globally using the 'workflow' variable prefix. e.g."
             " {{io.argoproj.workflow.v1alpha1.parameters.myparam}}"
         ),
     )
     artifact_gc: Optional[ArtifactGC] = Field(
-        None,
+        default=None,
         alias="artifactGC",
         description=(
             "ArtifactGC describes the strategy to use when deleting artifacts from completed or deleted workflows"
             " (applies to all output Artifacts unless Artifact.ArtifactGC is specified, which overrides this)"
         ),
     )
     artifact_repository_ref: Optional[ArtifactRepositoryRef] = Field(
-        None,
+        default=None,
         alias="artifactRepositoryRef",
         description=(
             "ArtifactRepositoryRef specifies the configMap name and key containing the artifact repository config."
         ),
     )
     automount_service_account_token: Optional[bool] = Field(
-        None,
+        default=None,
         alias="automountServiceAccountToken",
         description=(
             "AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in"
             " pods. ServiceAccountName of ExecutorConfig must be specified if this value is false."
         ),
     )
     dns_config: Optional[v1.PodDNSConfig] = Field(
-        None,
+        default=None,
         alias="dnsConfig",
         description="PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy.",
     )
     dns_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="dnsPolicy",
         description=(
             "Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet',"
             " 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the"
             " policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS"
             " policy explicitly to 'ClusterFirstWithHostNet'."
         ),
     )
     entrypoint: Optional[str] = Field(
-        None,
+        default=None,
         description="Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1.",
     )
     executor: Optional[ExecutorConfig] = Field(
-        None, description="Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1."
+        default=None,
+        description="Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1.",
     )
     hooks: Optional[Dict[str, LifecycleHook]] = Field(
-        None,
+        default=None,
         description=(
             "Hooks holds the lifecycle hook which is invoked at lifecycle of step, irrespective of the success,"
             " failure, or error status of the primary step"
         ),
     )
-    host_aliases: Optional[List[v1.HostAlias]] = Field(None, alias="hostAliases")
+    host_aliases: Optional[List[v1.HostAlias]] = Field(default=None, alias="hostAliases")
     host_network: Optional[bool] = Field(
-        None, alias="hostNetwork", description="Host networking requested for this workflow pod. Default to false."
+        default=None,
+        alias="hostNetwork",
+        description="Host networking requested for this workflow pod. Default to false.",
     )
     image_pull_secrets: Optional[List[v1.LocalObjectReference]] = Field(
-        None,
+        default=None,
         alias="imagePullSecrets",
         description=(
             "ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images"
             " in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets"
             " can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info:"
             " https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod"
         ),
     )
-    metrics: Optional[Metrics] = Field(None, description="Metrics are a list of metrics emitted from this Workflow")
+    metrics: Optional[Metrics] = Field(
+        default=None, description="Metrics are a list of metrics emitted from this Workflow"
+    )
     node_selector: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         alias="nodeSelector",
         description=(
             "NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected"
             " node(s). This is able to be overridden by a nodeSelector specified in the template."
         ),
     )
     on_exit: Optional[str] = Field(
-        None,
+        default=None,
         alias="onExit",
         description=(
             "OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success,"
             " failure, or error of the primary io.argoproj.workflow.v1alpha1."
         ),
     )
     parallelism: Optional[int] = Field(
-        None,
+        default=None,
         description="Parallelism limits the max total parallel pods that can execute at the same time in a workflow",
     )
     pod_disruption_budget: Optional[v1beta1.PodDisruptionBudgetSpec] = Field(
-        None,
+        default=None,
         alias="podDisruptionBudget",
         description=(
             "PodDisruptionBudget holds the number of concurrent disruptions that you allow for Workflow's Pods."
             " Controller will automatically add the selector with workflow name, if selector is empty. Optional:"
             " Defaults to empty."
         ),
     )
     pod_gc: Optional[PodGC] = Field(
-        None, alias="podGC", description="PodGC describes the strategy to use when deleting completed pods"
+        default=None, alias="podGC", description="PodGC describes the strategy to use when deleting completed pods"
     )
     pod_metadata: Optional[Metadata] = Field(
-        None,
+        default=None,
         alias="podMetadata",
         description="PodMetadata defines additional metadata that should be applied to workflow pods",
     )
     pod_priority: Optional[int] = Field(
-        None,
+        default=None,
         alias="podPriority",
         description="Priority to apply to workflow pods. DEPRECATED: Use PodPriorityClassName instead.",
     )
     pod_priority_class_name: Optional[str] = Field(
-        None, alias="podPriorityClassName", description="PriorityClassName to apply to workflow pods."
+        default=None, alias="podPriorityClassName", description="PriorityClassName to apply to workflow pods."
     )
     pod_spec_patch: Optional[str] = Field(
-        None,
+        default=None,
         alias="podSpecPatch",
         description=(
             "PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of"
             " container fields which are not strings (e.g. resource limits)."
         ),
     )
     priority: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Priority is used if controller is configured to process limited number of workflows in parallel."
             " Workflows with higher priority are processed first."
         ),
     )
     retry_strategy: Optional[RetryStrategy] = Field(
-        None,
+        default=None,
         alias="retryStrategy",
         description="RetryStrategy for all templates in the io.argoproj.workflow.v1alpha1.",
     )
     scheduler_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="schedulerName",
         description=(
             "Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set."
             " Default scheduler will be used if neither specified."
         ),
     )
     security_context: Optional[v1.PodSecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to"
             " empty.  See type description for default values of each field."
         ),
     )
     service_account_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="serviceAccountName",
         description="ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as.",
     )
     shutdown: Optional[str] = Field(
-        None, description="Shutdown will shutdown the workflow according to its ShutdownStrategy"
+        default=None, description="Shutdown will shutdown the workflow according to its ShutdownStrategy"
     )
     suspend: Optional[bool] = Field(
-        None, description="Suspend will suspend the workflow and prevent execution of any future steps in the workflow"
+        default=None,
+        description="Suspend will suspend the workflow and prevent execution of any future steps in the workflow",
     )
     synchronization: Optional[Synchronization] = Field(
-        None, description="Synchronization holds synchronization lock configuration for this Workflow"
+        default=None, description="Synchronization holds synchronization lock configuration for this Workflow"
     )
     template_defaults: Optional[Template] = Field(
-        None,
+        default=None,
         alias="templateDefaults",
         description=(
             "TemplateDefaults holds default template values that will apply to all templates in the Workflow, unless"
             " overridden on the template-level"
         ),
     )
     templates: Optional[List[Template]] = Field(
-        None, description="Templates is a list of workflow templates used in a workflow"
+        default=None, description="Templates is a list of workflow templates used in a workflow"
+    )
+    tolerations: Optional[List[v1.Toleration]] = Field(
+        default=None, description="Tolerations to apply to workflow pods."
     )
-    tolerations: Optional[List[v1.Toleration]] = Field(None, description="Tolerations to apply to workflow pods.")
     ttl_strategy: Optional[TTLStrategy] = Field(
-        None,
+        default=None,
         alias="ttlStrategy",
         description=(
             "TTLStrategy limits the lifetime of a Workflow that has finished execution depending on if it Succeeded or"
             " Failed. If this struct is set, once the Workflow finishes, it will be deleted after the time to live"
             " expires. If this field is unset, the controller config map will hold the default values."
         ),
     )
     volume_claim_gc: Optional[VolumeClaimGC] = Field(
-        None,
+        default=None,
         alias="volumeClaimGC",
         description="VolumeClaimGC describes the strategy to use when deleting volumes from completed workflows",
     )
     volume_claim_templates: Optional[List[v1.PersistentVolumeClaim]] = Field(
-        None,
+        default=None,
         alias="volumeClaimTemplates",
         description=(
             "VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow"
             " controller will create the claims at the beginning of the workflow and delete the claims upon completion"
             " of the workflow"
         ),
     )
     volumes: Optional[List[v1.Volume]] = Field(
-        None,
+        default=None,
         description=(
             "Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1."
         ),
     )
     workflow_metadata: Optional[WorkflowMetadata] = Field(
-        None,
+        default=None,
         alias="workflowMetadata",
         description="WorkflowMetadata contains some metadata of the workflow to refer to",
     )
     workflow_template_ref: Optional[WorkflowTemplateRef] = Field(
-        None,
+        default=None,
         alias="workflowTemplateRef",
         description="WorkflowTemplateRef holds a reference to a WorkflowTemplate for execution",
     )
 
 
 class WorkflowStatus(BaseModel):
     artifact_gc_status: Optional[ArtGCStatus] = Field(
-        None,
+        default=None,
         alias="artifactGCStatus",
         description="ArtifactGCStatus maintains the status of Artifact Garbage Collection",
     )
     artifact_repository_ref: Optional[ArtifactRepositoryRefStatus] = Field(
-        None,
+        default=None,
         alias="artifactRepositoryRef",
         description=(
             "ArtifactRepositoryRef is used to cache the repository to use so we do not need to determine it everytime"
             " we reconcile."
         ),
     )
     compressed_nodes: Optional[str] = Field(
-        None, alias="compressedNodes", description="Compressed and base64 decoded Nodes map"
+        default=None, alias="compressedNodes", description="Compressed and base64 decoded Nodes map"
     )
     conditions: Optional[List[Condition]] = Field(
-        None, description="Conditions is a list of conditions the Workflow may have"
+        default=None, description="Conditions is a list of conditions the Workflow may have"
     )
     estimated_duration: Optional[int] = Field(
-        None, alias="estimatedDuration", description="EstimatedDuration in seconds."
+        default=None, alias="estimatedDuration", description="EstimatedDuration in seconds."
     )
     finished_at: Optional[v1_1.Time] = Field(
-        None, alias="finishedAt", description="Time at which this workflow completed"
+        default=None, alias="finishedAt", description="Time at which this workflow completed"
     )
     message: Optional[str] = Field(
-        None, description="A human readable message indicating details about why the workflow is in this condition."
+        default=None,
+        description="A human readable message indicating details about why the workflow is in this condition.",
     )
     nodes: Optional[Dict[str, NodeStatus]] = Field(
-        None, description="Nodes is a mapping between a node ID and the node's status."
+        default=None, description="Nodes is a mapping between a node ID and the node's status."
     )
     offload_node_status_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="offloadNodeStatusVersion",
         description=(
             "Whether on not node status has been offloaded to a database. If exists, then Nodes and CompressedNodes"
             " will be empty. This will actually be populated with a hash of the offloaded data."
         ),
     )
     outputs: Optional[Outputs] = Field(
-        None,
+        default=None,
         description=(
             "Outputs captures output values and artifact locations produced by the workflow via global outputs"
         ),
     )
     persistent_volume_claims: Optional[List[v1.Volume]] = Field(
-        None,
+        default=None,
         alias="persistentVolumeClaims",
         description=(
             "PersistentVolumeClaims tracks all PVCs that were created as part of the io.argoproj.workflow.v1alpha1."
             " The contents of this list are drained at the end of the workflow."
         ),
     )
     phase: Optional[str] = Field(
-        None, description="Phase a simple, high-level summary of where the workflow is in its lifecycle."
+        default=None, description="Phase a simple, high-level summary of where the workflow is in its lifecycle."
     )
-    progress: Optional[str] = Field(None, description="Progress to completion")
+    progress: Optional[str] = Field(default=None, description="Progress to completion")
     resources_duration: Optional[Dict[str, int]] = Field(
-        None, alias="resourcesDuration", description="ResourcesDuration is the total for the workflow"
+        default=None, alias="resourcesDuration", description="ResourcesDuration is the total for the workflow"
+    )
+    started_at: Optional[v1_1.Time] = Field(
+        default=None, alias="startedAt", description="Time at which this workflow started"
     )
-    started_at: Optional[v1_1.Time] = Field(None, alias="startedAt", description="Time at which this workflow started")
     stored_templates: Optional[Dict[str, Template]] = Field(
-        None,
+        default=None,
         alias="storedTemplates",
         description="StoredTemplates is a mapping between a template ref and the node's status.",
     )
     stored_workflow_template_spec: Optional[WorkflowSpec] = Field(
-        None,
+        default=None,
         alias="storedWorkflowTemplateSpec",
         description="StoredWorkflowSpec stores the WorkflowTemplate spec for future execution.",
     )
     synchronization: Optional[SynchronizationStatus] = Field(
-        None, description="Synchronization stores the status of synchronization locks"
+        default=None, description="Synchronization stores the status of synchronization locks"
     )
 
 
 class WorkflowStep(BaseModel):
-    arguments: Optional[Arguments] = Field(None, description="Arguments hold arguments to the template")
+    arguments: Optional[Arguments] = Field(default=None, description="Arguments hold arguments to the template")
     continue_on: Optional[ContinueOn] = Field(
-        None,
+        default=None,
         alias="continueOn",
         description=(
             "ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed"
             " states can be specified"
         ),
     )
     hooks: Optional[Dict[str, LifecycleHook]] = Field(
-        None,
+        default=None,
         description=(
             "Hooks holds the lifecycle hook which is invoked at lifecycle of step, irrespective of the success,"
             " failure, or error status of the primary step"
         ),
     )
     inline: Optional[Template] = Field(
-        None, description="Inline is the template. Template must be empty if this is declared (and vice-versa)."
+        default=None,
+        description="Inline is the template. Template must be empty if this is declared (and vice-versa).",
     )
-    name: Optional[str] = Field(None, description="Name of the step")
+    name: Optional[str] = Field(default=None, description="Name of the step")
     on_exit: Optional[str] = Field(
-        None,
+        default=None,
         alias="onExit",
         description=(
             "OnExit is a template reference which is invoked at the end of the template, irrespective of the success,"
             " failure, or error of the primary template. DEPRECATED: Use Hooks[exit].Template instead."
         ),
     )
-    template: Optional[str] = Field(None, description="Template is the name of the template to execute as the step")
+    template: Optional[str] = Field(
+        default=None, description="Template is the name of the template to execute as the step"
+    )
     template_ref: Optional[TemplateRef] = Field(
-        None,
+        default=None,
         alias="templateRef",
         description="TemplateRef is the reference to the template resource to execute as the step.",
     )
     when: Optional[str] = Field(
-        None, description="When is an expression in which the step should conditionally execute"
+        default=None, description="When is an expression in which the step should conditionally execute"
     )
     with_items: Optional[List[Item]] = Field(
-        None,
+        default=None,
         alias="withItems",
         description="WithItems expands a step into multiple parallel steps from the items in the list",
     )
     with_param: Optional[str] = Field(
-        None,
+        default=None,
         alias="withParam",
         description=(
             "WithParam expands a step into multiple parallel steps from the value in the parameter, which is expected"
             " to be a JSON list."
         ),
     )
     with_sequence: Optional[Sequence] = Field(
-        None, alias="withSequence", description="WithSequence expands a step into a numeric sequence"
+        default=None, alias="withSequence", description="WithSequence expands a step into a numeric sequence"
     )
 
 
 class WorkflowTaskSetSpec(BaseModel):
     tasks: Optional[Dict[str, Template]] = None
 
 
 class WorkflowTemplate(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ObjectMeta
     spec: WorkflowSpec
 
 
 class WorkflowTemplateCreateRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
     namespace: Optional[str] = None
     template: Optional[WorkflowTemplate] = None
 
 
 class WorkflowTemplateLintRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
     namespace: Optional[str] = None
     template: Optional[WorkflowTemplate] = None
 
 
 class WorkflowTemplateList(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     items: List[WorkflowTemplate]
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ListMeta
 
 
 class WorkflowTemplateUpdateRequest(BaseModel):
-    name: Optional[str] = Field(None, description="DEPRECATED: This field is ignored.")
+    name: Optional[str] = Field(default=None, description="DEPRECATED: This field is ignored.")
     namespace: Optional[str] = None
     template: Optional[WorkflowTemplate] = None
 
 
 class WorkflowWatchEvent(BaseModel):
-    object: Optional[Workflow] = Field(None, title="the workflow")
-    type: Optional[str] = Field(None, title="the type of change")
+    object: Optional[Workflow] = Field(default=None, title="the workflow")
+    type: Optional[str] = Field(default=None, title="the type of change")
 
 
 ClusterWorkflowTemplate.update_forward_refs()
 CreateCronWorkflowRequest.update_forward_refs()
 CronWorkflow.update_forward_refs()
 CronWorkflowSpec.update_forward_refs()
 DAGTask.update_forward_refs()
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/io/argoproj/workflow/v1alpha1.pyi` & `hera_workflows-5.6.0/src/hera/events/models/io/argoproj/workflow/v1alpha1.pyi`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,13 @@
-from typing import Any, Dict, List, Optional
-
-from hera.shared._base_model import BaseModel as BaseModel
-
 from ...k8s.api.core import v1 as v1
 from ...k8s.api.policy import v1beta1 as v1beta1
 from ...k8s.apimachinery.pkg.apis.meta import v1 as v1_1
 from ...k8s.apimachinery.pkg.util import intstr as intstr
+from hera.shared._base_model import BaseModel as BaseModel
+from typing import Any, Dict, List, Optional
 
 class Amount(BaseModel):
     __root__: float
 
 class ArchivedWorkflowDeletedResponse(BaseModel): ...
 
 class ArtGCStatus(BaseModel):
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/io/k8s/api/core/v1.py` & `hera_workflows-5.6.0/src/hera/events/models/io/k8s/api/core/v1.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,43 +2,42 @@
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Dict, List, Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from ...apimachinery.pkg.api import resource
 from ...apimachinery.pkg.apis.meta import v1
 from ...apimachinery.pkg.util import intstr
 
 
 class AWSElasticBlockStoreVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported"
             ' by the host operating system. Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if'
             " unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore"
         ),
     )
     partition: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "The partition in the volume that you want to mount. If omitted, the default is to mount by volume name."
             ' Examples: For volume /dev/sda1, you specify the partition as "1". Similarly, the volume partition for'
             ' /dev/sda is "0" (or you can leave the property empty).'
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             'Specify "true" to force and set the ReadOnly property in VolumeMounts to "true". If omitted, the default'
             ' is "false". More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore'
         ),
     )
     volume_id: str = Field(
@@ -49,78 +48,80 @@
             " https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore"
         ),
     )
 
 
 class AzureDiskVolumeSource(BaseModel):
     caching_mode: Optional[str] = Field(
-        None, alias="cachingMode", description="Host Caching mode: None, Read Only, Read Write."
+        default=None, alias="cachingMode", description="Host Caching mode: None, Read Only, Read Write."
     )
     disk_name: str = Field(..., alias="diskName", description="The Name of the data disk in the blob storage")
     disk_uri: str = Field(..., alias="diskURI", description="The URI the data disk in the blob storage")
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Expected values Shared: multiple blob disks per storage account  Dedicated: single blob disk per storage"
             " account  Managed: azure managed data disk (only in managed availability set). defaults to shared"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.",
     )
 
 
 class AzureFileVolumeSource(BaseModel):
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.",
     )
     secret_name: str = Field(
         ..., alias="secretName", description="the name of secret that contains Azure Storage Account Name and Key"
     )
     share_name: str = Field(..., alias="shareName", description="Share Name")
 
 
 class Capabilities(BaseModel):
-    add: Optional[List[str]] = Field(None, description="Added capabilities")
-    drop: Optional[List[str]] = Field(None, description="Removed capabilities")
+    add: Optional[List[str]] = Field(default=None, description="Added capabilities")
+    drop: Optional[List[str]] = Field(default=None, description="Removed capabilities")
 
 
 class ConfigMapEnvSource(BaseModel):
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the ConfigMap must be defined")
+    optional: Optional[bool] = Field(default=None, description="Specify whether the ConfigMap must be defined")
 
 
 class ConfigMapKeySelector(BaseModel):
     key: str = Field(..., description="The key to select.")
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the ConfigMap or its key must be defined")
+    optional: Optional[bool] = Field(
+        default=None, description="Specify whether the ConfigMap or its key must be defined"
+    )
 
 
 class ImagePullPolicy(Enum):
     always = "Always"
     if_not_present = "IfNotPresent"
     never = "Never"
 
@@ -140,111 +141,115 @@
     container_port: int = Field(
         ...,
         alias="containerPort",
         description=(
             "Number of port to expose on the pod's IP address. This must be a valid port number, 0 < x < 65536."
         ),
     )
-    host_ip: Optional[str] = Field(None, alias="hostIP", description="What host IP to bind the external port to.")
+    host_ip: Optional[str] = Field(
+        default=None, alias="hostIP", description="What host IP to bind the external port to."
+    )
     host_port: Optional[int] = Field(
-        None,
+        default=None,
         alias="hostPort",
         description=(
             "Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. If"
             " HostNetwork is specified, this must match ContainerPort. Most containers do not need this."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "If specified, this must be an IANA_SVC_NAME and unique within the pod. Each named port in a pod must have"
             " a unique name. Name for the port that can be referred to by services."
         ),
     )
     protocol: Optional[Protocol] = Field(
-        None,
+        default=None,
         description=(
             'Protocol for port. Must be UDP, TCP, or SCTP. Defaults to "TCP".\n\nPossible enum values:\n - `"SCTP"` is'
             ' the SCTP protocol.\n - `"TCP"` is the TCP protocol.\n - `"UDP"` is the UDP protocol.'
         ),
     )
 
 
 class EventSource(BaseModel):
-    component: Optional[str] = Field(None, description="Component from which the event is generated.")
-    host: Optional[str] = Field(None, description="Node name on which the event is generated.")
+    component: Optional[str] = Field(default=None, description="Component from which the event is generated.")
+    host: Optional[str] = Field(default=None, description="Node name on which the event is generated.")
 
 
 class ExecAction(BaseModel):
     command: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Command is the command line to execute inside the container, the working directory for the command  is"
             " root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so"
             " traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to"
             " that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy."
         ),
     )
 
 
 class FCVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
-    lun: Optional[int] = Field(None, description="Optional: FC target lun number")
+    lun: Optional[int] = Field(default=None, description="Optional: FC target lun number")
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts."
         ),
     )
     target_ww_ns: Optional[List[str]] = Field(
-        None, alias="targetWWNs", description="Optional: FC target worldwide names (WWNs)"
+        default=None, alias="targetWWNs", description="Optional: FC target worldwide names (WWNs)"
     )
     wwids: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Optional: FC volume world wide identifiers (wwids) Either wwids or combination of targetWWNs and lun must"
             " be set, but not both simultaneously."
         ),
     )
 
 
 class FlockerVolumeSource(BaseModel):
     dataset_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="datasetName",
         description=(
             "Name of the dataset stored as metadata -> name on the dataset for Flocker should be considered as"
             " deprecated"
         ),
     )
     dataset_uuid: Optional[str] = Field(
-        None, alias="datasetUUID", description="UUID of the dataset. This is unique identifier of a Flocker dataset"
+        default=None,
+        alias="datasetUUID",
+        description="UUID of the dataset. This is unique identifier of a Flocker dataset",
     )
 
 
 class GCEPersistentDiskVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported"
             ' by the host operating system. Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if'
             " unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk"
         ),
     )
     partition: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "The partition in the volume that you want to mount. If omitted, the default is to mount by volume name."
             ' Examples: For volume /dev/sda1, you specify the partition as "1". Similarly, the volume partition for'
             ' /dev/sda is "0" (or you can leave the property empty). More info:'
             " https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk"
         ),
     )
@@ -253,46 +258,46 @@
         alias="pdName",
         description=(
             "Unique name of the PD resource in GCE. Used to identify the disk in GCE. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk"
         ),
     )
 
 
 class GRPCAction(BaseModel):
     port: int = Field(..., description="Port number of the gRPC service. Number must be in the range 1 to 65535.")
     service: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Service is the name of the service to place in the gRPC HealthCheckRequest (see"
             " https://github.com/grpc/grpc/blob/master/doc/health-checking.md).\n\nIf this is not specified, the"
             " default behavior is defined by gRPC."
         ),
     )
 
 
 class GitRepoVolumeSource(BaseModel):
     directory: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Target directory name. Must not contain or start with '..'.  If '.' is supplied, the volume directory"
             " will be the git repository.  Otherwise, if specified, the volume will contain the git repository in the"
             " subdirectory with the given name."
         ),
     )
     repository: str = Field(..., description="Repository URL")
-    revision: Optional[str] = Field(None, description="Commit hash for the specified revision.")
+    revision: Optional[str] = Field(default=None, description="Commit hash for the specified revision.")
 
 
 class GlusterfsVolumeSource(BaseModel):
     endpoints: str = Field(
         ...,
         description=(
             "EndpointsName is the endpoint name that details Glusterfs topology. More info:"
@@ -303,15 +308,15 @@
         ...,
         description=(
             "Path is the Glusterfs volume path. More info:"
             " https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "ReadOnly here will force the Glusterfs volume to be mounted with read-only permissions. Defaults to"
             " false. More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod"
         ),
     )
 
@@ -323,39 +328,39 @@
 
 class HTTPHeader(BaseModel):
     name: str = Field(..., description="The header field name")
     value: str = Field(..., description="The header field value")
 
 
 class HostAlias(BaseModel):
-    hostnames: Optional[List[str]] = Field(None, description="Hostnames for the above IP address.")
-    ip: Optional[str] = Field(None, description="IP address of the host file entry.")
+    hostnames: Optional[List[str]] = Field(default=None, description="Hostnames for the above IP address.")
+    ip: Optional[str] = Field(default=None, description="IP address of the host file entry.")
 
 
 class HostPathVolumeSource(BaseModel):
     path: str = Field(
         ...,
         description=(
             "Path of the directory on the host. If the path is a symlink, it will follow the link to the real path."
             " More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath"
         ),
     )
     type: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Type for HostPath Volume Defaults to "" More info:'
             " https://kubernetes.io/docs/concepts/storage/volumes#hostpath"
         ),
     )
 
 
 class KeyToPath(BaseModel):
     key: str = Field(..., description="The key to project.")
     mode: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Optional: mode bits used to set permissions on this file. Must be an octal value between 0000 and 0777 or"
             " a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal"
             " values for mode bits. If not specified, the volume defaultMode will be used. This might be in conflict"
             " with other options that affect the file mode, like fsGroup, and the result can be other mode bits set."
         ),
     )
@@ -366,15 +371,15 @@
             " element '..'. May not start with the string '..'."
         ),
     )
 
 
 class LocalObjectReference(BaseModel):
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
 
 
@@ -383,15 +388,15 @@
         ...,
         description=(
             "Path that is exported by the NFS server. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#nfs"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "ReadOnly here will force the NFS export to be mounted with read-only permissions. Defaults to false. More"
             " info: https://kubernetes.io/docs/concepts/storage/volumes#nfs"
         ),
     )
     server: str = Field(
@@ -419,89 +424,89 @@
         description=(
             "Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist."
             ' Gt, and Lt.\n\nPossible enum values:\n - `"DoesNotExist"`\n - `"Exists"`\n - `"Gt"`\n - `"In"`\n -'
             ' `"Lt"`\n - `"NotIn"`'
         ),
     )
     values: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the"
             " operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the"
             " values array must have a single element, which will be interpreted as an integer. This array is replaced"
             " during a strategic merge patch."
         ),
     )
 
 
 class NodeSelectorTerm(BaseModel):
     match_expressions: Optional[List[NodeSelectorRequirement]] = Field(
-        None, alias="matchExpressions", description="A list of node selector requirements by node's labels."
+        default=None, alias="matchExpressions", description="A list of node selector requirements by node's labels."
     )
     match_fields: Optional[List[NodeSelectorRequirement]] = Field(
-        None, alias="matchFields", description="A list of node selector requirements by node's fields."
+        default=None, alias="matchFields", description="A list of node selector requirements by node's fields."
     )
 
 
 class ObjectFieldSelector(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description='Version of the schema the FieldPath is written in terms of, defaults to "v1".',
     )
     field_path: str = Field(
         ..., alias="fieldPath", description="Path of the field to select in the specified API version."
     )
 
 
 class ObjectReference(BaseModel):
-    api_version: Optional[str] = Field(None, alias="apiVersion", description="API version of the referent.")
+    api_version: Optional[str] = Field(default=None, alias="apiVersion", description="API version of the referent.")
     field_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="fieldPath",
         description=(
             "If referring to a piece of an object instead of an entire object, this string should contain a valid"
             " JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object"
             ' reference is to a container within a pod, this would take on a value like: "spec.containers{name}"'
             ' (where "name" refers to the name of the container that triggered the event) or if no container name is'
             ' specified "spec.containers[2]" (container with index 2 in this pod). This syntax is chosen only to have'
             " some well-defined way of referencing a part of an object."
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind of the referent. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
     namespace: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Namespace of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"
         ),
     )
     resource_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="resourceVersion",
         description=(
             "Specific resourceVersion to which this reference is made, if any. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"
         ),
     )
     uid: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "UID of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids"
         ),
     )
 
 
@@ -522,46 +527,46 @@
         alias="claimName",
         description=(
             "ClaimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume. More"
             " info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims"
         ),
     )
     read_only: Optional[bool] = Field(
-        None, alias="readOnly", description="Will force the ReadOnly setting in VolumeMounts. Default false."
+        default=None, alias="readOnly", description="Will force the ReadOnly setting in VolumeMounts. Default false."
     )
 
 
 class PhotonPersistentDiskVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
     pd_id: str = Field(..., alias="pdID", description="ID that identifies Photon Controller persistent disk")
 
 
 class PodDNSConfigOption(BaseModel):
-    name: Optional[str] = Field(None, description="Required.")
+    name: Optional[str] = Field(default=None, description="Required.")
     value: Optional[str] = None
 
 
 class PortworxVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "FSType represents the filesystem type to mount Must be a filesystem type supported by the host operating"
             ' system. Ex. "ext4", "xfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.",
     )
     volume_id: str = Field(..., alias="volumeID", description="VolumeID uniquely identifies a Portworx volume")
 
 
 class PreferredSchedulingTerm(BaseModel):
@@ -570,152 +575,166 @@
     )
     weight: int = Field(
         ..., description="Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100."
     )
 
 
 class QuobyteVolumeSource(BaseModel):
-    group: Optional[str] = Field(None, description="Group to map volume access to Default is no group")
+    group: Optional[str] = Field(default=None, description="Group to map volume access to Default is no group")
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "ReadOnly here will force the Quobyte volume to be mounted with read-only permissions. Defaults to false."
         ),
     )
     registry: str = Field(
         ...,
         description=(
             "Registry represents a single or multiple Quobyte Registry services specified as a string as host:port"
             " pair (multiple entries are separated with commas) which acts as the central registry for volumes"
         ),
     )
     tenant: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Tenant owning the given Quobyte volume in the Backend Used with dynamically provisioned Quobyte volumes,"
             " value is set by the plugin"
         ),
     )
-    user: Optional[str] = Field(None, description="User to map volume access to Defaults to serivceaccount user")
+    user: Optional[str] = Field(
+        default=None, description="User to map volume access to Defaults to serivceaccount user"
+    )
     volume: str = Field(
         ..., description="Volume is a string that references an already created Quobyte volume by name."
     )
 
 
 class RBDVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported"
             ' by the host operating system. Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if'
             " unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#rbd"
         ),
     )
     image: str = Field(
         ..., description="The rados image name. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
     )
     keyring: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Keyring is the path to key ring for RBDUser. Default is /etc/ceph/keyring. More info:"
             " https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
     monitors: List[str] = Field(
         ...,
         description=(
             "A collection of Ceph monitors. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
     pool: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "The rados pool name. Default is rbd. More info:"
             " https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info:"
             " https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="secretRef",
         description=(
             "SecretRef is name of the authentication secret for RBDUser. If provided overrides keyring. Default is"
             " nil. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
     user: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "The rados user name. Default is admin. More info:"
             " https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
 
 
 class SELinuxOptions(BaseModel):
-    level: Optional[str] = Field(None, description="Level is SELinux level label that applies to the container.")
-    role: Optional[str] = Field(None, description="Role is a SELinux role label that applies to the container.")
-    type: Optional[str] = Field(None, description="Type is a SELinux type label that applies to the container.")
-    user: Optional[str] = Field(None, description="User is a SELinux user label that applies to the container.")
+    level: Optional[str] = Field(
+        default=None, description="Level is SELinux level label that applies to the container."
+    )
+    role: Optional[str] = Field(
+        default=None, description="Role is a SELinux role label that applies to the container."
+    )
+    type: Optional[str] = Field(
+        default=None, description="Type is a SELinux type label that applies to the container."
+    )
+    user: Optional[str] = Field(
+        default=None, description="User is a SELinux user label that applies to the container."
+    )
 
 
 class ScaleIOVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Default is "xfs".'
         ),
     )
     gateway: str = Field(..., description="The host address of the ScaleIO API Gateway.")
     protection_domain: Optional[str] = Field(
-        None,
+        default=None,
         alias="protectionDomain",
         description="The name of the ScaleIO Protection Domain for the configured storage.",
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.",
     )
     secret_ref: LocalObjectReference = Field(
         ...,
         alias="secretRef",
         description=(
             "SecretRef references to the secret for ScaleIO user and other sensitive information. If this is not"
             " provided, Login operation will fail."
         ),
     )
     ssl_enabled: Optional[bool] = Field(
-        None, alias="sslEnabled", description="Flag to enable/disable SSL communication with Gateway, default false"
+        default=None,
+        alias="sslEnabled",
+        description="Flag to enable/disable SSL communication with Gateway, default false",
     )
     storage_mode: Optional[str] = Field(
-        None,
+        default=None,
         alias="storageMode",
         description=(
             "Indicates whether the storage for a volume should be ThickProvisioned or ThinProvisioned. Default is"
             " ThinProvisioned."
         ),
     )
     storage_pool: Optional[str] = Field(
-        None, alias="storagePool", description="The ScaleIO Storage Pool associated with the protection domain."
+        default=None,
+        alias="storagePool",
+        description="The ScaleIO Storage Pool associated with the protection domain.",
     )
     system: str = Field(..., description="The name of the storage system as configured in ScaleIO.")
     volume_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="volumeName",
         description=(
             "The name of a volume already created in the ScaleIO system that is associated with this volume source."
         ),
     )
 
 
@@ -723,15 +742,15 @@
     localhost = "Localhost"
     runtime_default = "RuntimeDefault"
     unconfined = "Unconfined"
 
 
 class SeccompProfile(BaseModel):
     localhost_profile: Optional[str] = Field(
-        None,
+        default=None,
         alias="localhostProfile",
         description=(
             "localhostProfile indicates a profile defined in a file on the node should be used. The profile must be"
             " preconfigured on the node to work. Must be a descending path, relative to the kubelet's configured"
             ' seccomp profile location. Must only be set if type is "Localhost".'
         ),
     )
@@ -746,100 +765,102 @@
             ' profile.\n - `"Unconfined"` indicates no seccomp profile is applied (A.K.A. unconfined).'
         ),
     )
 
 
 class SecretEnvSource(BaseModel):
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the Secret must be defined")
+    optional: Optional[bool] = Field(default=None, description="Specify whether the Secret must be defined")
 
 
 class SecretKeySelector(BaseModel):
     key: str = Field(..., description="The key of the secret to select from.  Must be a valid secret key.")
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the Secret or its key must be defined")
+    optional: Optional[bool] = Field(default=None, description="Specify whether the Secret or its key must be defined")
 
 
 class SecretProjection(BaseModel):
     items: Optional[List[KeyToPath]] = Field(
-        None,
+        default=None,
         description=(
             "If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the"
             " volume as a file whose name is the key and content is the value. If specified, the listed keys will be"
             " projected into the specified paths, and unlisted keys will not be present. If a key is specified which"
             " is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be"
             " relative and may not contain the '..' path or start with '..'."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the Secret or its key must be defined")
+    optional: Optional[bool] = Field(default=None, description="Specify whether the Secret or its key must be defined")
 
 
 class SecretVolumeSource(BaseModel):
     default_mode: Optional[int] = Field(
-        None,
+        default=None,
         alias="defaultMode",
         description=(
             "Optional: mode bits used to set permissions on created files by default. Must be an octal value between"
             " 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON"
             " requires decimal values for mode bits. Defaults to 0644. Directories within the path are not affected by"
             " this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and"
             " the result can be other mode bits set."
         ),
     )
     items: Optional[List[KeyToPath]] = Field(
-        None,
+        default=None,
         description=(
             "If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the"
             " volume as a file whose name is the key and content is the value. If specified, the listed keys will be"
             " projected into the specified paths, and unlisted keys will not be present. If a key is specified which"
             " is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be"
             " relative and may not contain the '..' path or start with '..'."
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the Secret or its keys must be defined")
+    optional: Optional[bool] = Field(
+        default=None, description="Specify whether the Secret or its keys must be defined"
+    )
     secret_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="secretName",
         description=(
             "Name of the secret in the pod's namespace to use. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#secret"
         ),
     )
 
 
 class ServiceAccountTokenProjection(BaseModel):
     audience: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Audience is the intended audience of the token. A recipient of a token must identify itself with an"
             " identifier specified in the audience of the token, and otherwise should reject the token. The audience"
             " defaults to the identifier of the apiserver."
         ),
     )
     expiration_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="expirationSeconds",
         description=(
             "ExpirationSeconds is the requested duration of validity of the service account token. As the token"
             " approaches expiration, the kubelet volume plugin will proactively rotate the service account token. The"
             " kubelet will start trying to rotate the token if the token is older than 80 percent of its time to live"
             " or if the token is older than 24 hours.Defaults to 1 hour and must be at least 10 minutes."
         ),
@@ -847,44 +868,44 @@
     path: str = Field(
         ..., description="Path is the path relative to the mount point of the file to project the token into."
     )
 
 
 class StorageOSVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.",
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="secretRef",
         description=(
             "SecretRef specifies the secret to use for obtaining the StorageOS API credentials.  If not specified,"
             " default values will be attempted."
         ),
     )
     volume_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="volumeName",
         description=(
             "VolumeName is the human-readable name of the StorageOS volume.  Volume names are only unique within a"
             " namespace."
         ),
     )
     volume_namespace: Optional[str] = Field(
-        None,
+        default=None,
         alias="volumeNamespace",
         description=(
             "VolumeNamespace specifies the scope of the volume within StorageOS.  If no namespace is specified then"
             " the Pod's namespace will be used.  This allows the Kubernetes name scoping to be mirrored within"
             " StorageOS for tighter integration. Set VolumeName to any name to override the default behaviour. Set to"
             ' "default" if you are not using namespaces within StorageOS. Namespaces that do not pre-exist within'
             " StorageOS will be created."
@@ -906,63 +927,63 @@
 class OperatorModel(Enum):
     equal = "Equal"
     exists = "Exists"
 
 
 class Toleration(BaseModel):
     effect: Optional[Effect] = Field(
-        None,
+        default=None,
         description=(
             "Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed"
             ' values are NoSchedule, PreferNoSchedule and NoExecute.\n\nPossible enum values:\n - `"NoExecute"` Evict'
             " any already-running pods that do not tolerate the taint. Currently enforced by NodeController.\n -"
             ' `"NoSchedule"` Do not allow new pods to schedule onto the node unless they tolerate the taint, but allow'
             " all pods submitted to Kubelet without going through the scheduler to start, and allow all"
             ' already-running pods to continue running. Enforced by the scheduler.\n - `"PreferNoSchedule"` Like'
             " TaintEffectNoSchedule, but the scheduler tries not to schedule new pods onto the node, rather than"
             " prohibiting new pods from scheduling onto the node entirely. Enforced by the scheduler."
         ),
     )
     key: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is"
             " empty, operator must be Exists; this combination means to match all values and all keys."
         ),
     )
     operator: Optional[OperatorModel] = Field(
-        None,
+        default=None,
         description=(
             "Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to"
             " Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular"
             ' category.\n\nPossible enum values:\n - `"Equal"`\n - `"Exists"`'
         ),
     )
     toleration_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="tolerationSeconds",
         description=(
             "TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute,"
             " otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate"
             " the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by"
             " the system."
         ),
     )
     value: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty,"
             " otherwise just a regular string."
         ),
     )
 
 
 class TypedLocalObjectReference(BaseModel):
     api_group: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiGroup",
         description=(
             "APIGroup is the group for the resource being referenced. If APIGroup is not specified, the specified Kind"
             " must be in the core API group. For any other third-party types, APIGroup is required."
         ),
     )
     kind: str = Field(..., description="Kind is the type of resource being referenced")
@@ -981,94 +1002,94 @@
 class VolumeMount(BaseModel):
     mount_path: str = Field(
         ...,
         alias="mountPath",
         description="Path within the container at which the volume should be mounted.  Must not contain ':'.",
     )
     mount_propagation: Optional[str] = Field(
-        None,
+        default=None,
         alias="mountPropagation",
         description=(
             "mountPropagation determines how mounts are propagated from the host to container and the other way"
             " around. When not set, MountPropagationNone is used. This field is beta in 1.10."
         ),
     )
     name: str = Field(..., description="This must match the Name of a Volume.")
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false.",
     )
     sub_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="subPath",
         description=(
             "Path within the volume from which the container's volume should be mounted. Defaults to \"\" (volume's"
             " root)."
         ),
     )
     sub_path_expr: Optional[str] = Field(
-        None,
+        default=None,
         alias="subPathExpr",
         description=(
             "Expanded path within the volume from which the container's volume should be mounted. Behaves similarly"
             " to SubPath but environment variable references $(VAR_NAME) are expanded using the container's"
             ' environment. Defaults to "" (volume\'s root). SubPathExpr and SubPath are mutually exclusive.'
         ),
     )
 
 
 class VsphereVirtualDiskVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
     storage_policy_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="storagePolicyID",
         description="Storage Policy Based Management (SPBM) profile ID associated with the StoragePolicyName.",
     )
     storage_policy_name: Optional[str] = Field(
-        None, alias="storagePolicyName", description="Storage Policy Based Management (SPBM) profile name."
+        default=None, alias="storagePolicyName", description="Storage Policy Based Management (SPBM) profile name."
     )
     volume_path: str = Field(..., alias="volumePath", description="Path that identifies vSphere volume vmdk")
 
 
 class WindowsSecurityContextOptions(BaseModel):
     gmsa_credential_spec: Optional[str] = Field(
-        None,
+        default=None,
         alias="gmsaCredentialSpec",
         description=(
             "GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa)"
             " inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field."
         ),
     )
     gmsa_credential_spec_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="gmsaCredentialSpecName",
         description="GMSACredentialSpecName is the name of the GMSA credential spec to use.",
     )
     host_process: Optional[bool] = Field(
-        None,
+        default=None,
         alias="hostProcess",
         description=(
             "HostProcess determines if a container should be run as a 'Host Process' container. This field is"
             " alpha-level and will only be honored by components that enable the WindowsHostProcessContainers feature"
             " flag. Setting this field without the feature flag will result in errors when validating the Pod. All of"
             " a Pod's containers must have the same effective HostProcess value (it is not allowed to have a mix of"
             " HostProcess containers and non-HostProcess containers).  In addition, if HostProcess is true then"
             " HostNetwork must also be set to true."
         ),
     )
     run_as_user_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="runAsUserName",
         description=(
             "The UserName in Windows to run the entrypoint of the container process. Defaults to the user specified in"
             " image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and"
             " PodSecurityContext, the value specified in SecurityContext takes precedence."
         ),
     )
@@ -1079,38 +1100,38 @@
         ...,
         description=(
             "Driver is the name of the CSI driver that handles this volume. Consult with your admin for the correct"
             " name as registered in the cluster."
         ),
     )
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Ex. "ext4", "xfs", "ntfs". If not provided, the empty value is passed to the'
             " associated CSI driver which will determine the default filesystem to apply."
         ),
     )
     node_publish_secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="nodePublishSecretRef",
         description=(
             "NodePublishSecretRef is a reference to the secret object containing sensitive information to pass to the"
             " CSI driver to complete the CSI NodePublishVolume and NodeUnpublishVolume calls. This field is optional,"
             " and  may be empty if no secret is required. If the secret object contains more than one secret, all"
             " secret references are passed."
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Specifies a read-only configuration for the volume. Defaults to false (read/write).",
     )
     volume_attributes: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         alias="volumeAttributes",
         description=(
             "VolumeAttributes stores driver-specific properties that are passed to the CSI driver. Consult your"
             " driver's documentation for supported values."
         ),
     )
 
@@ -1120,69 +1141,69 @@
         ...,
         description=(
             "Required: Monitors is a collection of Ceph monitors More info:"
             " https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it"
         ),
     )
     path: Optional[str] = Field(
-        None, description="Optional: Used as the mounted root, rather than the full Ceph tree, default is /"
+        default=None, description="Optional: Used as the mounted root, rather than the full Ceph tree, default is /"
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts."
             " More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it"
         ),
     )
     secret_file: Optional[str] = Field(
-        None,
+        default=None,
         alias="secretFile",
         description=(
             "Optional: SecretFile is the path to key ring for User, default is /etc/ceph/user.secret More info:"
             " https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it"
         ),
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="secretRef",
         description=(
             "Optional: SecretRef is reference to the authentication secret for User, default is empty. More info:"
             " https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it"
         ),
     )
     user: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Optional: User is the rados user name, default is admin More info:"
             " https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it"
         ),
     )
 
 
 class CinderVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "Filesystem type to mount. Must be a filesystem type supported by the host operating system. Examples:"
             ' "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified. More info:'
             " https://examples.k8s.io/mysql-cinder-pd/README.md"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts."
             " More info: https://examples.k8s.io/mysql-cinder-pd/README.md"
         ),
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="secretRef",
         description="Optional: points to a secret object containing parameters used to connect to OpenStack.",
     )
     volume_id: str = Field(
         ...,
         alias="volumeID",
         description=(
@@ -1190,207 +1211,216 @@
             " https://examples.k8s.io/mysql-cinder-pd/README.md"
         ),
     )
 
 
 class ConfigMapProjection(BaseModel):
     items: Optional[List[KeyToPath]] = Field(
-        None,
+        default=None,
         description=(
             "If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into"
             " the volume as a file whose name is the key and content is the value. If specified, the listed keys will"
             " be projected into the specified paths, and unlisted keys will not be present. If a key is specified"
             " which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths"
             " must be relative and may not contain the '..' path or start with '..'."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the ConfigMap or its keys must be defined")
+    optional: Optional[bool] = Field(
+        default=None, description="Specify whether the ConfigMap or its keys must be defined"
+    )
 
 
 class ConfigMapVolumeSource(BaseModel):
     default_mode: Optional[int] = Field(
-        None,
+        default=None,
         alias="defaultMode",
         description=(
             "Optional: mode bits used to set permissions on created files by default. Must be an octal value between"
             " 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON"
             " requires decimal values for mode bits. Defaults to 0644. Directories within the path are not affected by"
             " this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and"
             " the result can be other mode bits set."
         ),
     )
     items: Optional[List[KeyToPath]] = Field(
-        None,
+        default=None,
         description=(
             "If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into"
             " the volume as a file whose name is the key and content is the value. If specified, the listed keys will"
             " be projected into the specified paths, and unlisted keys will not be present. If a key is specified"
             " which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths"
             " must be relative and may not contain the '..' path or start with '..'."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the ConfigMap or its keys must be defined")
+    optional: Optional[bool] = Field(
+        default=None, description="Specify whether the ConfigMap or its keys must be defined"
+    )
 
 
 class EmptyDirVolumeSource(BaseModel):
     medium: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'What type of storage medium should back this directory. The default is "" which means to use the node\'s'
             " default medium. Must be an empty string (default) or Memory. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#emptydir"
         ),
     )
     size_limit: Optional[resource.Quantity] = Field(
-        None,
+        default=None,
         alias="sizeLimit",
         description=(
             "Total amount of local storage required for this EmptyDir volume. The size limit is also applicable for"
             " memory medium. The maximum usage on memory medium EmptyDir would be the minimum value between the"
             " SizeLimit specified here and the sum of memory limits of all containers in a pod. The default is nil"
             " which means that the limit is undefined. More info:"
             " http://kubernetes.io/docs/user-guide/volumes#emptydir"
         ),
     )
 
 
 class EnvFromSource(BaseModel):
     config_map_ref: Optional[ConfigMapEnvSource] = Field(
-        None, alias="configMapRef", description="The ConfigMap to select from"
+        default=None, alias="configMapRef", description="The ConfigMap to select from"
     )
     prefix: Optional[str] = Field(
-        None, description="An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER."
+        default=None,
+        description="An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.",
+    )
+    secret_ref: Optional[SecretEnvSource] = Field(
+        default=None, alias="secretRef", description="The Secret to select from"
     )
-    secret_ref: Optional[SecretEnvSource] = Field(None, alias="secretRef", description="The Secret to select from")
 
 
 class EventSeries(BaseModel):
     count: Optional[int] = Field(
-        None, description="Number of occurrences in this series up to the last heartbeat time"
+        default=None, description="Number of occurrences in this series up to the last heartbeat time"
     )
     last_observed_time: Optional[v1.MicroTime] = Field(
-        None, alias="lastObservedTime", description="Time of the last occurrence observed"
+        default=None, alias="lastObservedTime", description="Time of the last occurrence observed"
     )
 
 
 class FlexVolumeSource(BaseModel):
     driver: str = Field(..., description="Driver is the name of the driver to use for this volume.")
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". The default filesystem depends on FlexVolume script.'
         ),
     )
-    options: Optional[Dict[str, str]] = Field(None, description="Optional: Extra command options if any.")
+    options: Optional[Dict[str, str]] = Field(default=None, description="Optional: Extra command options if any.")
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts."
         ),
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="secretRef",
         description=(
             "Optional: SecretRef is reference to the secret object containing sensitive information to pass to the"
             " plugin scripts. This may be empty if no secret object is specified. If the secret object contains more"
             " than one secret, all secrets are passed to the plugin scripts."
         ),
     )
 
 
 class HTTPGetAction(BaseModel):
     host: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Host name to connect to, defaults to the pod IP. You probably want to set "Host" in httpHeaders instead.'
         ),
     )
     http_headers: Optional[List[HTTPHeader]] = Field(
-        None, alias="httpHeaders", description="Custom headers to set in the request. HTTP allows repeated headers."
+        default=None,
+        alias="httpHeaders",
+        description="Custom headers to set in the request. HTTP allows repeated headers.",
     )
-    path: Optional[str] = Field(None, description="Path to access on the HTTP server.")
+    path: Optional[str] = Field(default=None, description="Path to access on the HTTP server.")
     port: intstr.IntOrString = Field(
         ...,
         description=(
             "Name or number of the port to access on the container. Number must be in the range 1 to 65535. Name must"
             " be an IANA_SVC_NAME."
         ),
     )
     scheme: Optional[Scheme] = Field(
-        None,
+        default=None,
         description=(
             'Scheme to use for connecting to the host. Defaults to HTTP.\n\nPossible enum values:\n - `"HTTP"` means'
             ' that the scheme used will be http://\n - `"HTTPS"` means that the scheme used will be https://'
         ),
     )
 
 
 class ISCSIVolumeSource(BaseModel):
     chap_auth_discovery: Optional[bool] = Field(
-        None, alias="chapAuthDiscovery", description="whether support iSCSI Discovery CHAP authentication"
+        default=None, alias="chapAuthDiscovery", description="whether support iSCSI Discovery CHAP authentication"
     )
     chap_auth_session: Optional[bool] = Field(
-        None, alias="chapAuthSession", description="whether support iSCSI Session CHAP authentication"
+        default=None, alias="chapAuthSession", description="whether support iSCSI Session CHAP authentication"
     )
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported"
             ' by the host operating system. Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if'
             " unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#iscsi"
         ),
     )
     initiator_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="initiatorName",
         description=(
             "Custom iSCSI Initiator Name. If initiatorName is specified with iscsiInterface simultaneously, new iSCSI"
             " interface <target portal>:<volume name> will be created for the connection."
         ),
     )
     iqn: str = Field(..., description="Target iSCSI Qualified Name.")
     iscsi_interface: Optional[str] = Field(
-        None,
+        default=None,
         alias="iscsiInterface",
         description="iSCSI Interface Name that uses an iSCSI transport. Defaults to 'default' (tcp).",
     )
     lun: int = Field(..., description="iSCSI Target Lun number.")
     portals: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "iSCSI Target Portal List. The portal is either an IP or ip_addr:port if the port is other than default"
             " (typically TCP ports 860 and 3260)."
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false.",
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None, alias="secretRef", description="CHAP Secret for iSCSI target and initiator authentication"
+        default=None, alias="secretRef", description="CHAP Secret for iSCSI target and initiator authentication"
     )
     target_portal: str = Field(
         ...,
         alias="targetPortal",
         description=(
             "iSCSI Target Portal. The Portal is either an IP or ip_addr:port if the port is other than default"
             " (typically TCP ports 860 and 3260)."
@@ -1402,26 +1432,26 @@
     node_selector_terms: List[NodeSelectorTerm] = Field(
         ..., alias="nodeSelectorTerms", description="Required. A list of node selector terms. The terms are ORed."
     )
 
 
 class PersistentVolumeClaimCondition(BaseModel):
     last_probe_time: Optional[v1.Time] = Field(
-        None, alias="lastProbeTime", description="Last time we probed the condition."
+        default=None, alias="lastProbeTime", description="Last time we probed the condition."
     )
     last_transition_time: Optional[v1.Time] = Field(
-        None,
+        default=None,
         alias="lastTransitionTime",
         description="Last time the condition transitioned from one status to another.",
     )
     message: Optional[str] = Field(
-        None, description="Human-readable message indicating details about last transition."
+        default=None, description="Human-readable message indicating details about last transition."
     )
     reason: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Unique, this should be a short, machine understandable string that gives the reason for condition's last"
             ' transition. If it reports "ResizeStarted" that means the underlying persistent volume is being resized.'
         ),
     )
     status: str
     type: Type = Field(
@@ -1431,394 +1461,394 @@
             ' system resize is pending on node\n - `"Resizing"` - a user trigger resize of pvc has been started'
         ),
     )
 
 
 class PersistentVolumeClaimStatus(BaseModel):
     access_modes: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="accessModes",
         description=(
             "AccessModes contains the actual access modes the volume backing the PVC has. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"
         ),
     )
     allocated_resources: Optional[Dict[str, resource.Quantity]] = Field(
-        None,
+        default=None,
         alias="allocatedResources",
         description=(
             "The storage resource within AllocatedResources tracks the capacity allocated to a PVC. It may be larger"
             " than the actual capacity when a volume expansion operation is requested. For storage quota, the larger"
             " value from allocatedResources and PVC.spec.resources is used. If allocatedResources is not set,"
             " PVC.spec.resources alone is used for quota calculation. If a volume expansion capacity request is"
             " lowered, allocatedResources is only lowered if there are no expansion operations in progress and if the"
             " actual volume capacity is equal or lower than the requested capacity. This is an alpha field and"
             " requires enabling RecoverVolumeExpansionFailure feature."
         ),
     )
     capacity: Optional[Dict[str, resource.Quantity]] = Field(
-        None, description="Represents the actual resources of the underlying volume."
+        default=None, description="Represents the actual resources of the underlying volume."
     )
     conditions: Optional[List[PersistentVolumeClaimCondition]] = Field(
-        None,
+        default=None,
         description=(
             "Current Condition of persistent volume claim. If underlying persistent volume is being resized then the"
             " Condition will be set to 'ResizeStarted'."
         ),
     )
     phase: Optional[Phase] = Field(
-        None,
+        default=None,
         description=(
             'Phase represents the current phase of PersistentVolumeClaim.\n\nPossible enum values:\n - `"Bound"` used'
             ' for PersistentVolumeClaims that are bound\n - `"Lost"` used for PersistentVolumeClaims that lost their'
             " underlying PersistentVolume. The claim was bound to a PersistentVolume and this volume does not exist"
             ' any longer and all data on it was lost.\n - `"Pending"` used for PersistentVolumeClaims that are not yet'
             " bound"
         ),
     )
     resize_status: Optional[str] = Field(
-        None,
+        default=None,
         alias="resizeStatus",
         description=(
             "ResizeStatus stores status of resize operation. ResizeStatus is not set by default but when expansion is"
             " complete resizeStatus is set to empty string by resize controller or kubelet. This is an alpha field and"
             " requires enabling RecoverVolumeExpansionFailure feature."
         ),
     )
 
 
 class PodDNSConfig(BaseModel):
     nameservers: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "A list of DNS name server IP addresses. This will be appended to the base nameservers generated from"
             " DNSPolicy. Duplicated nameservers will be removed."
         ),
     )
     options: Optional[List[PodDNSConfigOption]] = Field(
-        None,
+        default=None,
         description=(
             "A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy."
             " Duplicated entries will be removed. Resolution options given in Options will override those that appear"
             " in the base DNSPolicy."
         ),
     )
     searches: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "A list of DNS search domains for host-name lookup. This will be appended to the base search paths"
             " generated from DNSPolicy. Duplicated search paths will be removed."
         ),
     )
 
 
 class PodSecurityContext(BaseModel):
     fs_group: Optional[int] = Field(
-        None,
+        default=None,
         alias="fsGroup",
         description=(
             "A special supplemental group that applies to all containers in a pod. Some volume types allow the Kubelet"
             " to change the ownership of that volume to be owned by the pod:\n\n1. The owning GID will be the FSGroup"
             " 2. The setgid bit is set (new files created in the volume will be owned by FSGroup) 3. The permission"
             " bits are OR'd with rw-rw----\n\nIf unset, the Kubelet will not modify the ownership and permissions of"
             " any volume. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     fs_group_change_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsGroupChangePolicy",
         description=(
             "fsGroupChangePolicy defines behavior of changing ownership and permission of the volume before being"
             " exposed inside Pod. This field will only apply to volume types which support fsGroup based ownership(and"
             " permissions). It will have no effect on ephemeral volume types such as: secret, configmaps and emptydir."
             ' Valid values are "OnRootMismatch" and "Always". If not specified, "Always" is used. Note that this field'
             " cannot be set when spec.os.name is windows."
         ),
     )
     run_as_group: Optional[int] = Field(
-        None,
+        default=None,
         alias="runAsGroup",
         description=(
             "The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in"
             " SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in"
             " SecurityContext takes precedence for that container. Note that this field cannot be set when"
             " spec.os.name is windows."
         ),
     )
     run_as_non_root: Optional[bool] = Field(
-        None,
+        default=None,
         alias="runAsNonRoot",
         description=(
             "Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at"
             " runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If"
             " unset or false, no such validation will be performed. May also be set in SecurityContext.  If set in"
             " both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."
         ),
     )
     run_as_user: Optional[int] = Field(
-        None,
+        default=None,
         alias="runAsUser",
         description=(
             "The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if"
             " unspecified. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext,"
             " the value specified in SecurityContext takes precedence for that container. Note that this field cannot"
             " be set when spec.os.name is windows."
         ),
     )
     se_linux_options: Optional[SELinuxOptions] = Field(
-        None,
+        default=None,
         alias="seLinuxOptions",
         description=(
             "The SELinux context to be applied to all containers. If unspecified, the container runtime will allocate"
             " a random SELinux context for each container.  May also be set in SecurityContext.  If set in both"
             " SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that"
             " container. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     seccomp_profile: Optional[SeccompProfile] = Field(
-        None,
+        default=None,
         alias="seccompProfile",
         description=(
             "The seccomp options to use by the containers in this pod. Note that this field cannot be set when"
             " spec.os.name is windows."
         ),
     )
     supplemental_groups: Optional[List[int]] = Field(
-        None,
+        default=None,
         alias="supplementalGroups",
         description=(
             "A list of groups applied to the first process run in each container, in addition to the container's"
             " primary GID.  If unspecified, no groups will be added to any container. Note that this field cannot be"
             " set when spec.os.name is windows."
         ),
     )
     sysctls: Optional[List[Sysctl]] = Field(
-        None,
+        default=None,
         description=(
             "Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the"
             " container runtime) might fail to launch. Note that this field cannot be set when spec.os.name is"
             " windows."
         ),
     )
     windows_options: Optional[WindowsSecurityContextOptions] = Field(
-        None,
+        default=None,
         alias="windowsOptions",
         description=(
             "The Windows specific settings applied to all containers. If unspecified, the options within a container's"
             " SecurityContext will be used. If set in both SecurityContext and PodSecurityContext, the value specified"
             " in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is linux."
         ),
     )
 
 
 class ResourceFieldSelector(BaseModel):
     container_name: Optional[str] = Field(
-        None, alias="containerName", description="Container name: required for volumes, optional for env vars"
+        default=None, alias="containerName", description="Container name: required for volumes, optional for env vars"
     )
     divisor: Optional[resource.Quantity] = Field(
-        None, description='Specifies the output format of the exposed resources, defaults to "1"'
+        default=None, description='Specifies the output format of the exposed resources, defaults to "1"'
     )
     resource: str = Field(..., description="Required: resource to select")
 
 
 class ResourceRequirements(BaseModel):
     limits: Optional[Dict[str, resource.Quantity]] = Field(
-        None,
+        default=None,
         description=(
             "Limits describes the maximum amount of compute resources allowed. More info:"
             " https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
     requests: Optional[Dict[str, resource.Quantity]] = Field(
-        None,
+        default=None,
         description=(
             "Requests describes the minimum amount of compute resources required. If Requests is omitted for a"
             " container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined"
             " value. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
 
 
 class SecurityContext(BaseModel):
     allow_privilege_escalation: Optional[bool] = Field(
-        None,
+        default=None,
         alias="allowPrivilegeEscalation",
         description=(
             "AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process."
             " This bool directly controls if the no_new_privs flag will be set on the container process."
             " AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN"
             " Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     capabilities: Optional[Capabilities] = Field(
-        None,
+        default=None,
         description=(
             "The capabilities to add/drop when running containers. Defaults to the default set of capabilities granted"
             " by the container runtime. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     privileged: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Run container in privileged mode. Processes in privileged containers are essentially equivalent to root"
             " on the host. Defaults to false. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     proc_mount: Optional[str] = Field(
-        None,
+        default=None,
         alias="procMount",
         description=(
             "procMount denotes the type of proc mount to use for the containers. The default is DefaultProcMount which"
             " uses the container runtime defaults for readonly paths and masked paths. This requires the ProcMountType"
             " feature flag to be enabled. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     read_only_root_filesystem: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnlyRootFilesystem",
         description=(
             "Whether this container has a read-only root filesystem. Default is false. Note that this field cannot be"
             " set when spec.os.name is windows."
         ),
     )
     run_as_group: Optional[int] = Field(
-        None,
+        default=None,
         alias="runAsGroup",
         description=(
             "The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in"
             " PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in"
             " SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     run_as_non_root: Optional[bool] = Field(
-        None,
+        default=None,
         alias="runAsNonRoot",
         description=(
             "Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at"
             " runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If"
             " unset or false, no such validation will be performed. May also be set in PodSecurityContext.  If set in"
             " both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."
         ),
     )
     run_as_user: Optional[int] = Field(
-        None,
+        default=None,
         alias="runAsUser",
         description=(
             "The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if"
             " unspecified. May also be set in PodSecurityContext.  If set in both SecurityContext and"
             " PodSecurityContext, the value specified in SecurityContext takes precedence. Note that this field cannot"
             " be set when spec.os.name is windows."
         ),
     )
     se_linux_options: Optional[SELinuxOptions] = Field(
-        None,
+        default=None,
         alias="seLinuxOptions",
         description=(
             "The SELinux context to be applied to the container. If unspecified, the container runtime will allocate a"
             " random SELinux context for each container.  May also be set in PodSecurityContext.  If set in both"
             " SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. Note"
             " that this field cannot be set when spec.os.name is windows."
         ),
     )
     seccomp_profile: Optional[SeccompProfile] = Field(
-        None,
+        default=None,
         alias="seccompProfile",
         description=(
             "The seccomp options to use by this container. If seccomp options are provided at both the pod & container"
             " level, the container options override the pod options. Note that this field cannot be set when"
             " spec.os.name is windows."
         ),
     )
     windows_options: Optional[WindowsSecurityContextOptions] = Field(
-        None,
+        default=None,
         alias="windowsOptions",
         description=(
             "The Windows specific settings applied to all containers. If unspecified, the options from the"
             " PodSecurityContext will be used. If set in both SecurityContext and PodSecurityContext, the value"
             " specified in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is"
             " linux."
         ),
     )
 
 
 class ServicePort(BaseModel):
     app_protocol: Optional[str] = Field(
-        None,
+        default=None,
         alias="appProtocol",
         description=(
             "The application protocol for this port. This field follows standard Kubernetes label syntax. Un-prefixed"
             " names are reserved for IANA standard service names (as per RFC-6335 and"
             " http://www.iana.org/assignments/service-names). Non-standard protocols should use prefixed names such as"
             " mycompany.com/my-custom-protocol."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "The name of this port within the service. This must be a DNS_LABEL. All ports within a ServiceSpec must"
             " have unique names. When considering the endpoints for a Service, this must match the 'name' field in the"
             " EndpointPort. Optional if only one ServicePort is defined on this service."
         ),
     )
     node_port: Optional[int] = Field(
-        None,
+        default=None,
         alias="nodePort",
         description=(
             "The port on each node on which this service is exposed when type is NodePort or LoadBalancer.  Usually"
             " assigned by the system. If a value is specified, in-range, and not in use it will be used, otherwise the"
             " operation will fail.  If not specified, a port will be allocated if this Service requires one.  If this"
             " field is specified when creating a Service which does not need it, creation will fail. This field will"
             " be wiped when updating a Service to no longer need it (e.g. changing type from NodePort to ClusterIP)."
             " More info: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport"
         ),
     )
     port: int = Field(..., description="The port that will be exposed by this service.")
     protocol: Optional[Protocol] = Field(
-        None,
+        default=None,
         description=(
             'The IP protocol for this port. Supports "TCP", "UDP", and "SCTP". Default is TCP.\n\nPossible enum'
             ' values:\n - `"SCTP"` is the SCTP protocol.\n - `"TCP"` is the TCP protocol.\n - `"UDP"` is the UDP'
             " protocol."
         ),
     )
     target_port: Optional[intstr.IntOrString] = Field(
-        None,
+        default=None,
         alias="targetPort",
         description=(
             "Number or name of the port to access on the pods targeted by the service. Number must be in the range 1"
             " to 65535. Name must be an IANA_SVC_NAME. If this is a string, it will be looked up as a named port in"
             " the target Pod's container ports. If this is not specified, the value of the 'port' field is used (an"
             " identity map). This field is ignored for services with clusterIP=None, and should be omitted or set"
             " equal to the 'port' field. More info:"
             " https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service"
         ),
     )
 
 
 class TCPSocketAction(BaseModel):
-    host: Optional[str] = Field(None, description="Optional: Host name to connect to, defaults to the pod IP.")
+    host: Optional[str] = Field(default=None, description="Optional: Host name to connect to, defaults to the pod IP.")
     port: intstr.IntOrString = Field(
         ...,
         description=(
             "Number or name of the port to access on the container. Number must be in the range 1 to 65535. Name must"
             " be an IANA_SVC_NAME."
         ),
     )
 
 
 class DownwardAPIVolumeFile(BaseModel):
     field_ref: Optional[ObjectFieldSelector] = Field(
-        None,
+        default=None,
         alias="fieldRef",
         description=(
             "Required: Selects a field of the pod: only annotations, labels, name and namespace are supported."
         ),
     )
     mode: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Optional: mode bits used to set permissions on this file, must be an octal value between 0000 and 0777 or"
             " a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal"
             " values for mode bits. If not specified, the volume defaultMode will be used. This might be in conflict"
             " with other options that affect the file mode, like fsGroup, and the result can be other mode bits set."
         ),
     )
@@ -1826,202 +1856,209 @@
         ...,
         description=(
             "Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the"
             " '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..'"
         ),
     )
     resource_field_ref: Optional[ResourceFieldSelector] = Field(
-        None,
+        default=None,
         alias="resourceFieldRef",
         description=(
             "Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory,"
             " requests.cpu and requests.memory) are currently supported."
         ),
     )
 
 
 class DownwardAPIVolumeSource(BaseModel):
     default_mode: Optional[int] = Field(
-        None,
+        default=None,
         alias="defaultMode",
         description=(
             "Optional: mode bits to use on created files by default. Must be a Optional: mode bits used to set"
             " permissions on created files by default. Must be an octal value between 0000 and 0777 or a decimal value"
             " between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode"
             " bits. Defaults to 0644. Directories within the path are not affected by this setting. This might be in"
             " conflict with other options that affect the file mode, like fsGroup, and the result can be other mode"
             " bits set."
         ),
     )
     items: Optional[List[DownwardAPIVolumeFile]] = Field(
-        None, description="Items is a list of downward API volume file"
+        default=None, description="Items is a list of downward API volume file"
     )
 
 
 class EnvVarSource(BaseModel):
     config_map_key_ref: Optional[ConfigMapKeySelector] = Field(
-        None, alias="configMapKeyRef", description="Selects a key of a ConfigMap."
+        default=None, alias="configMapKeyRef", description="Selects a key of a ConfigMap."
     )
     field_ref: Optional[ObjectFieldSelector] = Field(
-        None,
+        default=None,
         alias="fieldRef",
         description=(
             "Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`,"
             " `metadata.annotations['<KEY>']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP,"
             " status.podIPs."
         ),
     )
     resource_field_ref: Optional[ResourceFieldSelector] = Field(
-        None,
+        default=None,
         alias="resourceFieldRef",
         description=(
             "Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory,"
             " limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently"
             " supported."
         ),
     )
     secret_key_ref: Optional[SecretKeySelector] = Field(
-        None, alias="secretKeyRef", description="Selects a key of a secret in the pod's namespace"
+        default=None, alias="secretKeyRef", description="Selects a key of a secret in the pod's namespace"
     )
 
 
 class Event(BaseModel):
-    action: Optional[str] = Field(None, description="What action was taken/failed regarding to the Regarding object.")
+    action: Optional[str] = Field(
+        default=None, description="What action was taken/failed regarding to the Regarding object."
+    )
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
-    count: Optional[int] = Field(None, description="The number of times this event has occurred.")
+    count: Optional[int] = Field(default=None, description="The number of times this event has occurred.")
     event_time: Optional[v1.MicroTime] = Field(
-        None, alias="eventTime", description="Time when this Event was first observed."
+        default=None, alias="eventTime", description="Time when this Event was first observed."
     )
     first_timestamp: Optional[v1.Time] = Field(
-        None,
+        default=None,
         alias="firstTimestamp",
         description="The time at which the event was first recorded. (Time of server receipt is in TypeMeta.)",
     )
     involved_object: ObjectReference = Field(
         ..., alias="involvedObject", description="The object that this event is about."
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     last_timestamp: Optional[v1.Time] = Field(
-        None,
+        default=None,
         alias="lastTimestamp",
         description="The time at which the most recent occurrence of this event was recorded.",
     )
-    message: Optional[str] = Field(None, description="A human-readable description of the status of this operation.")
+    message: Optional[str] = Field(
+        default=None, description="A human-readable description of the status of this operation."
+    )
     metadata: v1.ObjectMeta = Field(
         ...,
         description=(
             "Standard object's metadata. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata"
         ),
     )
     reason: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "This should be a short, machine understandable string that gives the reason for the transition into the"
             " object's current status."
         ),
     )
-    related: Optional[ObjectReference] = Field(None, description="Optional secondary object for more complex actions.")
+    related: Optional[ObjectReference] = Field(
+        default=None, description="Optional secondary object for more complex actions."
+    )
     reporting_component: Optional[str] = Field(
-        None,
+        default=None,
         alias="reportingComponent",
         description="Name of the controller that emitted this Event, e.g. `kubernetes.io/kubelet`.",
     )
     reporting_instance: Optional[str] = Field(
-        None, alias="reportingInstance", description="ID of the controller instance, e.g. `kubelet-xyzf`."
+        default=None, alias="reportingInstance", description="ID of the controller instance, e.g. `kubelet-xyzf`."
     )
     series: Optional[EventSeries] = Field(
-        None, description="Data about the Event series this event represents or nil if it's a singleton Event."
+        default=None, description="Data about the Event series this event represents or nil if it's a singleton Event."
     )
     source: Optional[EventSource] = Field(
-        None, description="The component reporting this event. Should be a short machine understandable string."
+        default=None,
+        description="The component reporting this event. Should be a short machine understandable string.",
     )
     type: Optional[str] = Field(
-        None, description="Type of this event (Normal, Warning), new types could be added in the future"
+        default=None, description="Type of this event (Normal, Warning), new types could be added in the future"
     )
 
 
 class LifecycleHandler(BaseModel):
-    exec: Optional[ExecAction] = Field(None, description="Exec specifies the action to take.")
+    exec: Optional[ExecAction] = Field(default=None, description="Exec specifies the action to take.")
     http_get: Optional[HTTPGetAction] = Field(
-        None, alias="httpGet", description="HTTPGet specifies the http request to perform."
+        default=None, alias="httpGet", description="HTTPGet specifies the http request to perform."
     )
     tcp_socket: Optional[TCPSocketAction] = Field(
-        None,
+        default=None,
         alias="tcpSocket",
         description=(
             "Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for the backward compatibility."
             " There are no validation of this field and lifecycle hooks will fail in runtime when tcp handler is"
             " specified."
         ),
     )
 
 
 class NodeAffinity(BaseModel):
     preferred_during_scheduling_ignored_during_execution: Optional[List[PreferredSchedulingTerm]] = Field(
-        None,
+        default=None,
         alias="preferredDuringSchedulingIgnoredDuringExecution",
         description=(
             "The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by"
             " this field, but it may choose a node that violates one or more of the expressions. The node that is most"
             " preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the"
             " scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute"
             ' a sum by iterating through the elements of this field and adding "weight" to the sum if the node matches'
             " the corresponding matchExpressions; the node(s) with the highest sum are the most preferred."
         ),
     )
     required_during_scheduling_ignored_during_execution: Optional[NodeSelector] = Field(
-        None,
+        default=None,
         alias="requiredDuringSchedulingIgnoredDuringExecution",
         description=(
             "If the affinity requirements specified by this field are not met at scheduling time, the pod will not be"
             " scheduled onto the node. If the affinity requirements specified by this field cease to be met at some"
             " point during pod execution (e.g. due to an update), the system may or may not try to eventually evict"
             " the pod from its node."
         ),
     )
 
 
 class PersistentVolumeClaimSpec(BaseModel):
     access_modes: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="accessModes",
         description=(
             "AccessModes contains the desired access modes the volume should have. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"
         ),
     )
     data_source: Optional[TypedLocalObjectReference] = Field(
-        None,
+        default=None,
         alias="dataSource",
         description=(
             "This field can be used to specify either: * An existing VolumeSnapshot object"
             " (snapshot.storage.k8s.io/VolumeSnapshot) * An existing PVC (PersistentVolumeClaim) If the provisioner or"
             " an external controller can support the specified data source, it will create a new volume based on the"
             " contents of the specified data source. If the AnyVolumeDataSource feature gate is enabled, this field"
             " will always have the same contents as the DataSourceRef field."
         ),
     )
     data_source_ref: Optional[TypedLocalObjectReference] = Field(
-        None,
+        default=None,
         alias="dataSourceRef",
         description=(
             "Specifies the object from which to populate the volume with data, if a non-empty volume is desired. This"
             " may be any local object from a non-empty API group (non core object) or a PersistentVolumeClaim object."
             " When this field is specified, volume binding will only succeed if the type of the specified object"
             " matches some installed volume populator or dynamic provisioner. This field will replace the"
             " functionality of the DataSource field and as such if both fields are non-empty, they must have the same"
@@ -2031,51 +2068,51 @@
             " objects, DataSourceRef\n  allows any non-core object, as well as PersistentVolumeClaim objects.\n* While"
             " DataSource ignores disallowed values (dropping them), DataSourceRef\n  preserves all values, and"
             " generates an error if a disallowed value is\n  specified.\n(Alpha) Using this field requires the"
             " AnyVolumeDataSource feature gate to be enabled."
         ),
     )
     resources: Optional[ResourceRequirements] = Field(
-        None,
+        default=None,
         description=(
             "Resources represents the minimum resources the volume should have. If RecoverVolumeExpansionFailure"
             " feature is enabled users are allowed to specify resource requirements that are lower than previous value"
             " but must still be higher than capacity recorded in the status field of the claim. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources"
         ),
     )
     selector: Optional[v1.LabelSelector] = Field(
-        None, description="A label query over volumes to consider for binding."
+        default=None, description="A label query over volumes to consider for binding."
     )
     storage_class_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="storageClassName",
         description=(
             "Name of the StorageClass required by the claim. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1"
         ),
     )
     volume_mode: Optional[str] = Field(
-        None,
+        default=None,
         alias="volumeMode",
         description=(
             "volumeMode defines what type of volume is required by the claim. Value of Filesystem is implied when not"
             " included in claim spec."
         ),
     )
     volume_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="volumeName",
         description="VolumeName is the binding reference to the PersistentVolume backing this claim.",
     )
 
 
 class PersistentVolumeClaimTemplate(BaseModel):
     metadata: Optional[v1.ObjectMeta] = Field(
-        None,
+        default=None,
         description=(
             "May contain labels and annotations that will be copied into the PVC when creating it. No other fields are"
             " allowed and will be rejected during validation."
         ),
     )
     spec: PersistentVolumeClaimSpec = Field(
         ...,
@@ -2084,29 +2121,29 @@
             " gets created from this template. The same fields as in a PersistentVolumeClaim are also valid here."
         ),
     )
 
 
 class PodAffinityTerm(BaseModel):
     label_selector: Optional[v1.LabelSelector] = Field(
-        None, alias="labelSelector", description="A label query over a set of resources, in this case pods."
+        default=None, alias="labelSelector", description="A label query over a set of resources, in this case pods."
     )
     namespace_selector: Optional[v1.LabelSelector] = Field(
-        None,
+        default=None,
         alias="namespaceSelector",
         description=(
             "A label query over the set of namespaces that the term applies to. The term is applied to the union of"
             " the namespaces selected by this field and the ones listed in the namespaces field. null selector and"
             ' null or empty namespaces list means "this pod\'s namespace". An empty selector ({}) matches all'
             " namespaces. This field is beta-level and is only honored when PodAffinityNamespaceSelector feature is"
             " enabled."
         ),
     )
     namespaces: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "namespaces specifies a static list of namespace names that the term applies to. The term is applied to"
             " the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or"
             ' empty namespaces list and null namespaceSelector means "this pod\'s namespace"'
         ),
     )
     topology_key: str = Field(
@@ -2118,73 +2155,73 @@
             " of the label with key topologyKey matches that of any node on which any of the selected pods is running."
             " Empty topologyKey is not allowed."
         ),
     )
 
 
 class Probe(BaseModel):
-    exec: Optional[ExecAction] = Field(None, description="Exec specifies the action to take.")
+    exec: Optional[ExecAction] = Field(default=None, description="Exec specifies the action to take.")
     failure_threshold: Optional[int] = Field(
-        None,
+        default=None,
         alias="failureThreshold",
         description=(
             "Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3."
             " Minimum value is 1."
         ),
     )
     grpc: Optional[GRPCAction] = Field(
-        None,
+        default=None,
         description=(
             "GRPC specifies an action involving a GRPC port. This is an alpha field and requires enabling"
             " GRPCContainerProbe feature gate."
         ),
     )
     http_get: Optional[HTTPGetAction] = Field(
-        None, alias="httpGet", description="HTTPGet specifies the http request to perform."
+        default=None, alias="httpGet", description="HTTPGet specifies the http request to perform."
     )
     initial_delay_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="initialDelaySeconds",
         description=(
             "Number of seconds after the container has started before liveness probes are initiated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     period_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="periodSeconds",
         description="How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1.",
     )
     success_threshold: Optional[int] = Field(
-        None,
+        default=None,
         alias="successThreshold",
         description=(
             "Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to"
             " 1. Must be 1 for liveness and startup. Minimum value is 1."
         ),
     )
     tcp_socket: Optional[TCPSocketAction] = Field(
-        None, alias="tcpSocket", description="TCPSocket specifies an action involving a TCP port."
+        default=None, alias="tcpSocket", description="TCPSocket specifies an action involving a TCP port."
     )
     termination_grace_period_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="terminationGracePeriodSeconds",
         description=(
             "Optional duration in seconds the pod needs to terminate gracefully upon probe failure. The grace period"
             " is the duration in seconds after the processes running in the pod are sent a termination signal and the"
             " time when the processes are forcibly halted with a kill signal. Set this value longer than the expected"
             " cleanup time for your process. If this value is nil, the pod's terminationGracePeriodSeconds will be"
             " used. Otherwise, this value overrides the value provided by the pod spec. Value must be non-negative"
             " integer. The value zero indicates stop immediately via the kill signal (no opportunity to shut down)."
             " This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate. Minimum value is 1."
             " spec.terminationGracePeriodSeconds is used if unset."
         ),
     )
     timeout_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="timeoutSeconds",
         description=(
             "Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
 
@@ -2197,39 +2234,41 @@
     )
     weight: int = Field(
         ..., description="weight associated with matching the corresponding podAffinityTerm, in the range 1-100."
     )
 
 
 class DownwardAPIProjection(BaseModel):
-    items: Optional[List[DownwardAPIVolumeFile]] = Field(None, description="Items is a list of DownwardAPIVolume file")
+    items: Optional[List[DownwardAPIVolumeFile]] = Field(
+        default=None, description="Items is a list of DownwardAPIVolume file"
+    )
 
 
 class EnvVar(BaseModel):
     name: str = Field(..., description="Name of the environment variable. Must be a C_IDENTIFIER.")
     value: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the"
             " container and any service environment variables. If a variable cannot be resolved, the reference in the"
             " input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the"
             ' $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)". Escaped'
             ' references will never be expanded, regardless of whether the variable exists or not. Defaults to "".'
         ),
     )
     value_from: Optional[EnvVarSource] = Field(
-        None,
+        default=None,
         alias="valueFrom",
         description="Source for the environment variable's value. Cannot be used if value is not empty.",
     )
 
 
 class EphemeralVolumeSource(BaseModel):
     volume_claim_template: Optional[PersistentVolumeClaimTemplate] = Field(
-        None,
+        default=None,
         alias="volumeClaimTemplate",
         description=(
             "Will be used to create a stand-alone PVC to provision the volume. The pod in which this"
             " EphemeralVolumeSource is embedded will be the owner of the PVC, i.e. the PVC will be deleted together"
             " with the pod.  The name of the PVC will be `<pod name>-<volume name>` where `<volume name>` is the name"
             " from the `PodSpec.Volumes` array entry. Pod validation will reject the pod if the concatenated name is"
             " not valid for a PVC (for example, too long).\n\nAn existing PVC with that name that is not owned by the"
@@ -2241,25 +2280,25 @@
             " must not be nil."
         ),
     )
 
 
 class Lifecycle(BaseModel):
     post_start: Optional[LifecycleHandler] = Field(
-        None,
+        default=None,
         alias="postStart",
         description=(
             "PostStart is called immediately after a container is created. If the handler fails, the container is"
             " terminated and restarted according to its restart policy. Other management of the container blocks until"
             " the hook completes. More info:"
             " https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks"
         ),
     )
     pre_stop: Optional[LifecycleHandler] = Field(
-        None,
+        default=None,
         alias="preStop",
         description=(
             "PreStop is called immediately before a container is terminated due to an API request or management event"
             " such as liveness/startup probe failure, preemption, resource contention, etc. The handler is not called"
             " if the container crashes or exits. The Pod's termination grace period countdown begins before the"
             " PreStop hook is executed. Regardless of the outcome of the handler, the container will eventually"
             " terminate within the Pod's termination grace period (unless delayed by finalizers). Other management of"
@@ -2267,172 +2306,176 @@
             " info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks"
         ),
     )
 
 
 class PersistentVolumeClaim(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: Optional[v1.ObjectMeta] = Field(
-        None,
+        default=None,
         description=(
             "Standard object's metadata. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata"
         ),
     )
     spec: Optional[PersistentVolumeClaimSpec] = Field(
-        None,
+        default=None,
         description=(
             "Spec defines the desired characteristics of a volume requested by a pod author. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims"
         ),
     )
     status: Optional[PersistentVolumeClaimStatus] = Field(
-        None,
+        default=None,
         description=(
             "Status represents the current information/status of a persistent volume claim. Read-only. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims"
         ),
     )
 
 
 class PodAffinity(BaseModel):
     preferred_during_scheduling_ignored_during_execution: Optional[List[WeightedPodAffinityTerm]] = Field(
-        None,
+        default=None,
         alias="preferredDuringSchedulingIgnoredDuringExecution",
         description=(
             "The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by"
             " this field, but it may choose a node that violates one or more of the expressions. The node that is most"
             " preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the"
             " scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute"
             ' a sum by iterating through the elements of this field and adding "weight" to the sum if the node has'
             " pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most"
             " preferred."
         ),
     )
     required_during_scheduling_ignored_during_execution: Optional[List[PodAffinityTerm]] = Field(
-        None,
+        default=None,
         alias="requiredDuringSchedulingIgnoredDuringExecution",
         description=(
             "If the affinity requirements specified by this field are not met at scheduling time, the pod will not be"
             " scheduled onto the node. If the affinity requirements specified by this field cease to be met at some"
             " point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually"
             " evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each"
             " podAffinityTerm are intersected, i.e. all terms must be satisfied."
         ),
     )
 
 
 class PodAntiAffinity(BaseModel):
     preferred_during_scheduling_ignored_during_execution: Optional[List[WeightedPodAffinityTerm]] = Field(
-        None,
+        default=None,
         alias="preferredDuringSchedulingIgnoredDuringExecution",
         description=(
             "The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified"
             " by this field, but it may choose a node that violates one or more of the expressions. The node that is"
             " most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the"
             " scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.),"
             ' compute a sum by iterating through the elements of this field and adding "weight" to the sum if the node'
             " has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most"
             " preferred."
         ),
     )
     required_during_scheduling_ignored_during_execution: Optional[List[PodAffinityTerm]] = Field(
-        None,
+        default=None,
         alias="requiredDuringSchedulingIgnoredDuringExecution",
         description=(
             "If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will"
             " not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be"
             " met at some point during pod execution (e.g. due to a pod label update), the system may or may not try"
             " to eventually evict the pod from its node. When there are multiple elements, the lists of nodes"
             " corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied."
         ),
     )
 
 
 class VolumeProjection(BaseModel):
     config_map: Optional[ConfigMapProjection] = Field(
-        None, alias="configMap", description="information about the configMap data to project"
+        default=None, alias="configMap", description="information about the configMap data to project"
     )
     downward_api: Optional[DownwardAPIProjection] = Field(
-        None, alias="downwardAPI", description="information about the downwardAPI data to project"
+        default=None, alias="downwardAPI", description="information about the downwardAPI data to project"
+    )
+    secret: Optional[SecretProjection] = Field(
+        default=None, description="information about the secret data to project"
     )
-    secret: Optional[SecretProjection] = Field(None, description="information about the secret data to project")
     service_account_token: Optional[ServiceAccountTokenProjection] = Field(
-        None, alias="serviceAccountToken", description="information about the serviceAccountToken data to project"
+        default=None,
+        alias="serviceAccountToken",
+        description="information about the serviceAccountToken data to project",
     )
 
 
 class Affinity(BaseModel):
     node_affinity: Optional[NodeAffinity] = Field(
-        None, alias="nodeAffinity", description="Describes node affinity scheduling rules for the pod."
+        default=None, alias="nodeAffinity", description="Describes node affinity scheduling rules for the pod."
     )
     pod_affinity: Optional[PodAffinity] = Field(
-        None,
+        default=None,
         alias="podAffinity",
         description=(
             "Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some"
             " other pod(s))."
         ),
     )
     pod_anti_affinity: Optional[PodAntiAffinity] = Field(
-        None,
+        default=None,
         alias="podAntiAffinity",
         description=(
             "Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as"
             " some other pod(s))."
         ),
     )
 
 
 class Container(BaseModel):
     args: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references"
             " $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the"
             " reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for"
             ' escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".'
             " Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be"
             " updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     command: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not"
             " provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable"
             " cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a"
             ' single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string'
             ' literal "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable'
             " exists or not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     env: Optional[List[EnvVar]] = Field(
-        None, description="List of environment variables to set in the container. Cannot be updated."
+        default=None, description="List of environment variables to set in the container. Cannot be updated."
     )
     env_from: Optional[List[EnvFromSource]] = Field(
-        None,
+        default=None,
         alias="envFrom",
         description=(
             "List of sources to populate environment variables in the container. The keys defined within a source must"
             " be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a"
             " key exists in multiple sources, the value associated with the last source will take precedence. Values"
             " defined by an Env with a duplicate key will take precedence. Cannot be updated."
         ),
@@ -2442,236 +2485,238 @@
         description=(
             "Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is"
             " optional to allow higher level config management to default or override container images in workload"
             " controllers like Deployments and StatefulSets."
         ),
     )
     image_pull_policy: Optional[ImagePullPolicy] = Field(
-        None,
+        default=None,
         alias="imagePullPolicy",
         description=(
             "Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or"
             " IfNotPresent otherwise. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/containers/images#updating-images\n\nPossible enum values:\n -"
             ' `"Always"` means that kubelet always attempts to pull the latest image. Container will fail If the pull'
             ' fails.\n - `"IfNotPresent"` means that kubelet pulls if the image isn\'t present on disk. Container will'
             ' fail if the image isn\'t present and the pull fails.\n - `"Never"` means that kubelet never pulls an'
             " image, but only uses a local image. Container will fail if the image isn't present"
         ),
     )
     lifecycle: Optional[Lifecycle] = Field(
-        None,
+        default=None,
         description=(
             "Actions that the management system should take in response to container lifecycle events. Cannot be"
             " updated."
         ),
     )
     liveness_probe: Optional[Probe] = Field(
-        None,
+        default=None,
         alias="livenessProbe",
         description=(
             "Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated."
             " More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name"
             " (DNS_LABEL). Cannot be updated."
         ),
     )
     ports: Optional[List[ContainerPort]] = Field(
-        None,
+        default=None,
         description=(
             "List of ports to expose from the container. Exposing a port here gives the system additional information"
             " about the network connections a container uses, but is primarily informational. Not specifying a port"
             ' here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0"'
             " address inside a container will be accessible from the network. Cannot be updated."
         ),
     )
     readiness_probe: Optional[Probe] = Field(
-        None,
+        default=None,
         alias="readinessProbe",
         description=(
             "Periodic probe of container service readiness. Container will be removed from service endpoints if the"
             " probe fails. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     resources: Optional[ResourceRequirements] = Field(
-        None,
+        default=None,
         description=(
             "Compute Resources required by this container. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
     security_context: Optional[SecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext defines the security options the container should be run with. If set, the fields of"
             " SecurityContext override the equivalent fields of PodSecurityContext. More info:"
             " https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"
         ),
     )
     startup_probe: Optional[Probe] = Field(
-        None,
+        default=None,
         alias="startupProbe",
         description=(
             "StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are"
             " executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if"
             " the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a"
             " Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state"
             " operation. This cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     stdin: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a buffer for stdin in the container runtime. If this is not set,"
             " reads from stdin in the container will always result in EOF. Default is false."
         ),
     )
     stdin_once: Optional[bool] = Field(
-        None,
+        default=None,
         alias="stdinOnce",
         description=(
             "Whether the container runtime should close the stdin channel after it has been opened by a single attach."
             " When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is"
             " set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and"
             " then remains open and accepts data until the client disconnects, at which time stdin is closed and"
             " remains closed until the container is restarted. If this flag is false, a container processes that reads"
             " from stdin will never receive an EOF. Default is false"
         ),
     )
     termination_message_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePath",
         description=(
             "Optional: Path at which the file to which the container's termination message will be written is mounted"
             " into the container's filesystem. Message written is intended to be brief final status, such as an"
             " assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message"
             " length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be"
             " updated."
         ),
     )
     termination_message_policy: Optional[TerminationMessagePolicy] = Field(
-        None,
+        default=None,
         alias="terminationMessagePolicy",
         description=(
             "Indicate how the termination message should be populated. File will use the contents of"
             " terminationMessagePath to populate the container status message on both success and failure."
             " FallbackToLogsOnError will use the last chunk of container log output if the termination message file is"
             " empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines,"
             " whichever is smaller. Defaults to File. Cannot be updated.\n\nPossible enum values:\n -"
             ' `"FallbackToLogsOnError"` will read the most recent contents of the container logs for the container'
             " status message when the container exits with an error and the terminationMessagePath has no contents.\n"
             ' - `"File"` is the default behavior and will set the container status message to the contents of the'
             " container's terminationMessagePath when the container exits."
         ),
     )
     tty: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is"
             " false."
         ),
     )
     volume_devices: Optional[List[VolumeDevice]] = Field(
-        None,
+        default=None,
         alias="volumeDevices",
         description="volumeDevices is the list of block devices to be used by the container.",
     )
     volume_mounts: Optional[List[VolumeMount]] = Field(
-        None,
+        default=None,
         alias="volumeMounts",
         description="Pod volumes to mount into the container's filesystem. Cannot be updated.",
     )
     working_dir: Optional[str] = Field(
-        None,
+        default=None,
         alias="workingDir",
         description=(
             "Container's working directory. If not specified, the container runtime's default will be used, which"
             " might be configured in the container image. Cannot be updated."
         ),
     )
 
 
 class ProjectedVolumeSource(BaseModel):
     default_mode: Optional[int] = Field(
-        None,
+        default=None,
         alias="defaultMode",
         description=(
             "Mode bits used to set permissions on created files by default. Must be an octal value between 0000 and"
             " 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires"
             " decimal values for mode bits. Directories within the path are not affected by this setting. This might"
             " be in conflict with other options that affect the file mode, like fsGroup, and the result can be other"
             " mode bits set."
         ),
     )
-    sources: Optional[List[VolumeProjection]] = Field(None, description="list of volume projections")
+    sources: Optional[List[VolumeProjection]] = Field(default=None, description="list of volume projections")
 
 
 class Volume(BaseModel):
     aws_elastic_block_store: Optional[AWSElasticBlockStoreVolumeSource] = Field(
-        None,
+        default=None,
         alias="awsElasticBlockStore",
         description=(
             "AWSElasticBlockStore represents an AWS Disk resource that is attached to a kubelet's host machine and"
             " then exposed to the pod. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore"
         ),
     )
     azure_disk: Optional[AzureDiskVolumeSource] = Field(
-        None,
+        default=None,
         alias="azureDisk",
         description="AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.",
     )
     azure_file: Optional[AzureFileVolumeSource] = Field(
-        None,
+        default=None,
         alias="azureFile",
         description="AzureFile represents an Azure File Service mount on the host and bind mount to the pod.",
     )
     cephfs: Optional[CephFSVolumeSource] = Field(
-        None, description="CephFS represents a Ceph FS mount on the host that shares a pod's lifetime"
+        default=None, description="CephFS represents a Ceph FS mount on the host that shares a pod's lifetime"
     )
     cinder: Optional[CinderVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "Cinder represents a cinder volume attached and mounted on kubelets host machine. More info:"
             " https://examples.k8s.io/mysql-cinder-pd/README.md"
         ),
     )
     config_map: Optional[ConfigMapVolumeSource] = Field(
-        None, alias="configMap", description="ConfigMap represents a configMap that should populate this volume"
+        default=None,
+        alias="configMap",
+        description="ConfigMap represents a configMap that should populate this volume",
     )
     csi: Optional[CSIVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "CSI (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI"
             " drivers (Beta feature)."
         ),
     )
     downward_api: Optional[DownwardAPIVolumeSource] = Field(
-        None,
+        default=None,
         alias="downwardAPI",
         description="DownwardAPI represents downward API about the pod that should populate this volume",
     )
     empty_dir: Optional[EmptyDirVolumeSource] = Field(
-        None,
+        default=None,
         alias="emptyDir",
         description=(
             "EmptyDir represents a temporary directory that shares a pod's lifetime. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#emptydir"
         ),
     )
     ephemeral: Optional[EphemeralVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "Ephemeral represents a volume that is handled by a cluster storage driver. The volume's lifecycle is tied"
             " to the pod that defines it - it will be created before the pod starts, and deleted when the pod is"
             " removed.\n\nUse this if: a) the volume is only needed while the pod runs, b) features of normal volumes"
             " like restoring from snapshot or capacity\n   tracking are needed,\nc) the storage driver is specified"
             " through a storage class, and d) the storage driver supports dynamic volume provisioning through\n   a"
             " PersistentVolumeClaim (see EphemeralVolumeSource for more\n   information on the connection between this"
@@ -2679,137 +2724,137 @@
             " APIs for volumes that persist for longer than the lifecycle of an individual pod.\n\nUse CSI for"
             " light-weight local ephemeral volumes if the CSI driver is meant to be used that way - see the"
             " documentation of the driver for more information.\n\nA pod can use both types of ephemeral volumes and"
             " persistent volumes at the same time."
         ),
     )
     fc: Optional[FCVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "FC represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to"
             " the pod."
         ),
     )
     flex_volume: Optional[FlexVolumeSource] = Field(
-        None,
+        default=None,
         alias="flexVolume",
         description=(
             "FlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin."
         ),
     )
     flocker: Optional[FlockerVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "Flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker"
             " control service being running"
         ),
     )
     gce_persistent_disk: Optional[GCEPersistentDiskVolumeSource] = Field(
-        None,
+        default=None,
         alias="gcePersistentDisk",
         description=(
             "GCEPersistentDisk represents a GCE Disk resource that is attached to a kubelet's host machine and then"
             " exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk"
         ),
     )
     git_repo: Optional[GitRepoVolumeSource] = Field(
-        None,
+        default=None,
         alias="gitRepo",
         description=(
             "GitRepo represents a git repository at a particular revision. DEPRECATED: GitRepo is deprecated. To"
             " provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo"
             " using git, then mount the EmptyDir into the Pod's container."
         ),
     )
     glusterfs: Optional[GlusterfsVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "Glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime. More info:"
             " https://examples.k8s.io/volumes/glusterfs/README.md"
         ),
     )
     host_path: Optional[HostPathVolumeSource] = Field(
-        None,
+        default=None,
         alias="hostPath",
         description=(
             "HostPath represents a pre-existing file or directory on the host machine that is directly exposed to the"
             " container. This is generally used for system agents or other privileged things that are allowed to see"
             " the host machine. Most containers will NOT need this. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#hostpath"
         ),
     )
     iscsi: Optional[ISCSIVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "ISCSI represents an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to"
             " the pod. More info: https://examples.k8s.io/volumes/iscsi/README.md"
         ),
     )
     name: str = Field(
         ...,
         description=(
             "Volume's name. Must be a DNS_LABEL and unique within the pod. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
     nfs: Optional[NFSVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "NFS represents an NFS mount on the host that shares a pod's lifetime More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#nfs"
         ),
     )
     persistent_volume_claim: Optional[PersistentVolumeClaimVolumeSource] = Field(
-        None,
+        default=None,
         alias="persistentVolumeClaim",
         description=(
             "PersistentVolumeClaimVolumeSource represents a reference to a PersistentVolumeClaim in the same"
             " namespace. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims"
         ),
     )
     photon_persistent_disk: Optional[PhotonPersistentDiskVolumeSource] = Field(
-        None,
+        default=None,
         alias="photonPersistentDisk",
         description=(
             "PhotonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host"
             " machine"
         ),
     )
     portworx_volume: Optional[PortworxVolumeSource] = Field(
-        None,
+        default=None,
         alias="portworxVolume",
         description="PortworxVolume represents a portworx volume attached and mounted on kubelets host machine",
     )
     projected: Optional[ProjectedVolumeSource] = Field(
-        None, description="Items for all in one resources secrets, configmaps, and downward API"
+        default=None, description="Items for all in one resources secrets, configmaps, and downward API"
     )
     quobyte: Optional[QuobyteVolumeSource] = Field(
-        None, description="Quobyte represents a Quobyte mount on the host that shares a pod's lifetime"
+        default=None, description="Quobyte represents a Quobyte mount on the host that shares a pod's lifetime"
     )
     rbd: Optional[RBDVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "RBD represents a Rados Block Device mount on the host that shares a pod's lifetime. More info:"
             " https://examples.k8s.io/volumes/rbd/README.md"
         ),
     )
     scale_io: Optional[ScaleIOVolumeSource] = Field(
-        None,
+        default=None,
         alias="scaleIO",
         description="ScaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.",
     )
     secret: Optional[SecretVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "Secret represents a secret that should populate this volume. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#secret"
         ),
     )
     storageos: Optional[StorageOSVolumeSource] = Field(
-        None, description="StorageOS represents a StorageOS volume attached and mounted on Kubernetes nodes."
+        default=None, description="StorageOS represents a StorageOS volume attached and mounted on Kubernetes nodes."
     )
     vsphere_volume: Optional[VsphereVirtualDiskVolumeSource] = Field(
-        None,
+        default=None,
         alias="vsphereVolume",
         description="VsphereVolume represents a vSphere volume attached and mounted on kubelets host machine",
     )
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/io/k8s/api/core/v1.pyi` & `hera_workflows-5.6.0/src/hera/events/models/io/k8s/api/core/v1.pyi`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,13 @@
-from enum import Enum
-from typing import Dict, List, Optional
-
-from hera.shared._base_model import BaseModel as BaseModel
-
 from ...apimachinery.pkg.api import resource as resource
 from ...apimachinery.pkg.apis.meta import v1 as v1
 from ...apimachinery.pkg.util import intstr as intstr
+from enum import Enum
+from hera.shared._base_model import BaseModel as BaseModel
+from typing import Dict, List, Optional
 
 class AWSElasticBlockStoreVolumeSource(BaseModel):
     fs_type: Optional[str]
     partition: Optional[int]
     read_only: Optional[bool]
     volume_id: str
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/io/k8s/api/policy/v1beta1.py` & `hera_workflows-5.6.0/src/hera/events/models/io/k8s/api/policy/v1beta1.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from ...apimachinery.pkg.apis.meta import v1
 from ...apimachinery.pkg.util import intstr
 
 
 class PodDisruptionBudgetSpec(BaseModel):
     max_unavailable: Optional[intstr.IntOrString] = Field(
-        None,
+        default=None,
         alias="maxUnavailable",
         description=(
             'An eviction is allowed if at most "maxUnavailable" pods selected by "selector" are unavailable after the'
             " eviction, i.e. even in absence of the evicted pod. For example, one can prevent all voluntary evictions"
             ' by specifying 0. This is a mutually exclusive setting with "minAvailable".'
         ),
     )
     min_available: Optional[intstr.IntOrString] = Field(
-        None,
+        default=None,
         alias="minAvailable",
         description=(
             'An eviction is allowed if at least "minAvailable" pods selected by "selector" will still be available'
             " after the eviction, i.e. even in the absence of the evicted pod.  So for example you can prevent all"
             ' voluntary evictions by specifying "100%".'
         ),
     )
     selector: Optional[v1.LabelSelector] = Field(
-        None,
+        default=None,
         description=(
             "Label query over pods whose evictions are managed by the disruption budget. A null selector selects no"
             " pods. An empty selector ({}) also selects no pods, which differs from standard behavior of selecting all"
             " pods. In policy/v1, an empty selector will select all pods in the namespace."
         ),
     )
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/api/resource.py` & `hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/api/resource.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 
 class Quantity(BaseModel):
     __root__: str = Field(
         ...,
         description=(
             "Quantity is a fixed-point representation of a number. It provides convenient marshaling/unmarshaling in"
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/apis/meta/v1.py` & `hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/apis/meta/v1.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,40 +2,39 @@
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from datetime import datetime
 from typing import Dict, List, Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 
 class CreateOptions(BaseModel):
     dry_run: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="dryRun",
         title=(
             "When present, indicates that modifications should not be\npersisted. An invalid or unrecognized dryRun"
             " directive will\nresult in an error response and no further processing of the\nrequest. Valid values"
             " are:\n- All: all dry run stages will be processed\n+optional"
         ),
     )
     field_manager: Optional[str] = Field(
-        None,
+        default=None,
         alias="fieldManager",
         title=(
             "fieldManager is a name associated with the actor or entity\nthat is making these changes. The value must"
             " be less than or\n128 characters long, and only contain printable characters,\nas defined by"
             " https://golang.org/pkg/unicode/#IsPrint.\n+optional"
         ),
     )
     field_validation: Optional[str] = Field(
-        None,
+        default=None,
         alias="fieldValidation",
         title=(
             "fieldValidation instructs the server on how to handle\nobjects in the request (POST/PUT/PATCH) containing"
             " unknown\nor duplicate fields, provided that the `ServerSideFieldValidation`\nfeature gate is also"
             " enabled. Valid values are:\n- Ignore: This will ignore any unknown fields that are silently\ndropped"
             " from the object, and will ignore all but the last duplicate\nfield that the decoder encounters. This is"
             " the default behavior\nprior to v1.23 and is the default behavior when the\n`ServerSideFieldValidation`"
@@ -66,61 +65,61 @@
         ...,
         description=(
             "operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and"
             " DoesNotExist."
         ),
     )
     values: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "values is an array of string values. If the operator is In or NotIn, the values array must be non-empty."
             " If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during"
             " a strategic merge patch."
         ),
     )
 
 
 class ListMeta(BaseModel):
     continue_: Optional[str] = Field(
-        None,
+        default=None,
         alias="continue",
         description=(
             "continue may be set if the user set a limit on the number of items returned, and indicates that the"
             " server has more data available. The value is opaque and may be used to issue another request to the"
             " endpoint that served this list to retrieve the next set of available objects. Continuing a consistent"
             " list may not be possible if the server configuration has changed or more than a few minutes have passed."
             " The resourceVersion field returned when using this continue value will be identical to the value in the"
             " first response, unless you have received this token from an error message."
         ),
     )
     remaining_item_count: Optional[int] = Field(
-        None,
+        default=None,
         alias="remainingItemCount",
         description=(
             "remainingItemCount is the number of subsequent items in the list which are not included in this list"
             " response. If the list request contained label or field selectors, then the number of remaining items is"
             " unknown and the field will be left unset and omitted during serialization. If the list is complete"
             " (either because it is not chunking or because this is the last chunk), then there are no more remaining"
             " items and this field will be left unset and omitted during serialization. Servers older than v1.15 do"
             " not set this field. The intended use of the remainingItemCount is *estimating* the size of a collection."
             " Clients should not rely on the remainingItemCount to be set or to be exact."
         ),
     )
     resource_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="resourceVersion",
         description=(
             "String that identifies the server's internal version of this object that can be used by clients to"
             " determine when objects have changed. Value must be treated as opaque by clients and passed unmodified"
             " back to the server. Populated by the system. Read-only. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"
         ),
     )
     self_link: Optional[str] = Field(
-        None,
+        default=None,
         alias="selfLink",
         description=(
             "selfLink is a URL representing this object. Populated by the system. Read-only.\n\nDEPRECATED Kubernetes"
             " will stop propagating this field in 1.20 release and the field is planned to be removed in 1.21 release."
         ),
     )
 
@@ -128,23 +127,25 @@
 class MicroTime(BaseModel):
     __root__: datetime = Field(..., description="MicroTime is version of Time with microsecond level precision.")
 
 
 class OwnerReference(BaseModel):
     api_version: str = Field(..., alias="apiVersion", description="API version of the referent.")
     block_owner_deletion: Optional[bool] = Field(
-        None,
+        default=None,
         alias="blockOwnerDeletion",
         description=(
             'If true, AND if the owner has the "foregroundDeletion" finalizer, then the owner cannot be deleted from'
             " the key-value store until this reference is removed. Defaults to false. To set this field, a user needs"
             ' "delete" permission of the owner, otherwise 422 (Unprocessable Entity) will be returned.'
         ),
     )
-    controller: Optional[bool] = Field(None, description="If true, this reference points to the managing controller.")
+    controller: Optional[bool] = Field(
+        default=None, description="If true, this reference points to the managing controller."
+    )
     kind: str = Field(
         ...,
         description=(
             "Kind of the referent. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
@@ -154,30 +155,30 @@
     uid: str = Field(
         ..., description="UID of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#uids"
     )
 
 
 class StatusCause(BaseModel):
     field: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "The field of the resource that has caused this error, as named by its JSON serialization. May include dot"
             " and postfix notation for nested attributes. Arrays are zero-indexed.  Fields may appear more than once"
             ' in an array of causes due to fields having multiple errors. Optional.\n\nExamples:\n  "name" - the field'
             ' "name" on the current resource\n  "items[0].name" - the field "name" on the first array entry in "items"'
         ),
     )
     message: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "A human-readable description of the cause of the error.  This field may be presented as-is to a reader."
         ),
     )
     reason: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "A machine-readable description of the cause of the error. If this value is empty there is no information"
             " available."
         ),
     )
 
 
@@ -189,116 +190,118 @@
             " provided for many of the factory methods that the time package offers."
         ),
     )
 
 
 class LabelSelector(BaseModel):
     match_expressions: Optional[List[LabelSelectorRequirement]] = Field(
-        None,
+        default=None,
         alias="matchExpressions",
         description="matchExpressions is a list of label selector requirements. The requirements are ANDed.",
     )
     match_labels: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         alias="matchLabels",
         description=(
             "matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to"
             ' an element of matchExpressions, whose key field is "key", the operator is "In", and the values array'
             ' contains only "value". The requirements are ANDed.'
         ),
     )
 
 
 class ManagedFieldsEntry(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the version of this resource that this field set applies to. The format is"
             ' "group/version" just like the top-level APIVersion field. It is necessary to track the version of a'
             " field set because it cannot be automatically converted."
         ),
     )
     fields_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fieldsType",
         description=(
             "FieldsType is the discriminator for the different fields format and version. There is currently only one"
             ' possible value: "FieldsV1"'
         ),
     )
     fields_v1: Optional[FieldsV1] = Field(
-        None,
+        default=None,
         alias="fieldsV1",
         description='FieldsV1 holds the first JSON version format as described in the "FieldsV1" type.',
     )
-    manager: Optional[str] = Field(None, description="Manager is an identifier of the workflow managing these fields.")
+    manager: Optional[str] = Field(
+        default=None, description="Manager is an identifier of the workflow managing these fields."
+    )
     operation: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Operation is the type of operation which lead to this ManagedFieldsEntry being created. The only valid"
             " values for this field are 'Apply' and 'Update'."
         ),
     )
     subresource: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Subresource is the name of the subresource used to update that object, or empty string if the object was"
             " updated through the main resource. The value of this field is used to distinguish between managers, even"
             " if they share the same name. For example, a status update will be distinct from a regular update using"
             " the same manager name. Note that the APIVersion field is not related to the Subresource field and it"
             " always corresponds to the version of the main resource."
         ),
     )
     time: Optional[Time] = Field(
-        None,
+        default=None,
         description=(
             "Time is timestamp of when these fields were set. It should always be empty if Operation is 'Apply'"
         ),
     )
 
 
 class ObjectMeta(BaseModel):
     annotations: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         description=(
             "Annotations is an unstructured key value map stored with a resource that may be set by external tools to"
             " store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying"
             " objects. More info: http://kubernetes.io/docs/user-guide/annotations"
         ),
     )
     cluster_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="clusterName",
         description=(
             "The name of the cluster which the object belongs to. This is used to distinguish resources with same name"
             " and namespace in different clusters. This field is not set anywhere right now and apiserver is going to"
             " ignore it if set in create or update request."
         ),
     )
     creation_timestamp: Optional[Time] = Field(
-        None,
+        default=None,
         alias="creationTimestamp",
         description=(
             "CreationTimestamp is a timestamp representing the server time when this object was created. It is not"
             " guaranteed to be set in happens-before order across separate operations. Clients may not set this value."
             " It is represented in RFC3339 form and is in UTC.\n\nPopulated by the system. Read-only. Null for lists."
             " More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata"
         ),
     )
     deletion_grace_period_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="deletionGracePeriodSeconds",
         description=(
             "Number of seconds allowed for this object to gracefully terminate before it will be removed from the"
             " system. Only set when deletionTimestamp is also set. May only be shortened. Read-only."
         ),
     )
     deletion_timestamp: Optional[Time] = Field(
-        None,
+        default=None,
         alias="deletionTimestamp",
         description=(
             "DeletionTimestamp is RFC 3339 date and time at which this resource will be deleted. This field is set by"
             " the server when a graceful deletion is requested by the user, and is not directly settable by a client."
             " The resource is expected to be deleted (no longer visible from resource lists, and not reachable by"
             " name) after the time in this field, once the finalizers list is empty. As long as the finalizers list"
             " contains items, deletion is blocked. Once the deletionTimestamp is set, this value may not be unset or"
@@ -310,29 +313,29 @@
             " until an administrator or automated process can determine the resource is fully terminated. If not set,"
             " graceful deletion of the object has not been requested.\n\nPopulated by the system when a graceful"
             " deletion is requested. Read-only. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata"
         ),
     )
     finalizers: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Must be empty before the object is deleted from the registry. Each entry is an identifier for the"
             " responsible component that will remove the entry from the list. If the deletionTimestamp of the object"
             " is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any"
             " order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is"
             " a shared field, any actor with permission can reorder it. If the finalizer list is processed in order,"
             " then this can lead to a situation in which the component responsible for the first finalizer in the list"
             " is waiting for a signal (field value, external system, or other) produced by a component responsible for"
             " a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to"
             " order amongst themselves and are not vulnerable to ordering changes in the list."
         ),
     )
     generate_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="generateName",
         description=(
             "GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field"
             " has not been provided. If this field is used, the name returned to the client will be different than the"
             " name passed. This value will also be combined with a unique suffix. The provided value has the same"
             " validation rules as the Name field, and may be truncated by the length of the suffix required to make"
             " the value unique on the server.\n\nIf this field is specified and the generated name exists, the server"
@@ -340,89 +343,89 @@
             " indicating a unique name could not be found in the time allotted, and the client should retry"
             " (optionally after the time indicated in the Retry-After header).\n\nApplied only if Name is not"
             " specified. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency"
         ),
     )
     generation: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "A sequence number representing a specific generation of the desired state. Populated by the system."
             " Read-only."
         ),
     )
     labels: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         description=(
             "Map of string keys and values that can be used to organize and categorize (scope and select) objects. May"
             " match selectors of replication controllers and services. More info:"
             " http://kubernetes.io/docs/user-guide/labels"
         ),
     )
     managed_fields: Optional[List[ManagedFieldsEntry]] = Field(
-        None,
+        default=None,
         alias="managedFields",
         description=(
             "ManagedFields maps workflow-id and version to the set of fields that are managed by that workflow. This"
             " is mostly for internal housekeeping, and users typically shouldn't need to set or understand this"
             " field. A workflow can be the user's name, a controller's name, or the name of a specific apply path"
             ' like "ci-cd". The set of fields is always in the version that the workflow used when modifying the'
             " object."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name must be unique within a namespace. Is required when creating resources, although some resources may"
             " allow a client to request the generation of an appropriate name automatically. Name is primarily"
             " intended for creation idempotence and configuration definition. Cannot be updated. More info:"
             " http://kubernetes.io/docs/user-guide/identifiers#names"
         ),
     )
     namespace: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Namespace defines the space within which each name must be unique. An empty namespace is equivalent to"
             ' the "default" namespace, but "default" is the canonical representation. Not all objects are required to'
             " be scoped to a namespace - the value of this field for those objects will be empty.\n\nMust be a"
             " DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces"
         ),
     )
     owner_references: Optional[List[OwnerReference]] = Field(
-        None,
+        default=None,
         alias="ownerReferences",
         description=(
             "List of objects depended by this object. If ALL objects in the list have been deleted, this object will"
             " be garbage collected. If this object is managed by a controller, then an entry in this list will point"
             " to this controller, with the controller field set to true. There cannot be more than one managing"
             " controller."
         ),
     )
     resource_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="resourceVersion",
         description=(
             "An opaque value that represents the internal version of this object that can be used by clients to"
             " determine when objects have changed. May be used for optimistic concurrency, change detection, and the"
             " watch operation on a resource or set of resources. Clients must treat these values as opaque and passed"
             " unmodified back to the server. They may only be valid for a particular resource or set of"
             " resources.\n\nPopulated by the system. Read-only. Value must be treated as opaque by clients and . More"
             " info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"
         ),
     )
     self_link: Optional[str] = Field(
-        None,
+        default=None,
         alias="selfLink",
         description=(
             "SelfLink is a URL representing this object. Populated by the system. Read-only.\n\nDEPRECATED Kubernetes"
             " will stop propagating this field in 1.20 release and the field is planned to be removed in 1.21 release."
         ),
     )
     uid: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "UID is the unique in time and space value for this object. It is typically generated by the server on"
             " successful creation of a resource and is not allowed to change on PUT operations.\n\nPopulated by the"
             " system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids"
         ),
     )
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/io/k8s/apimachinery/pkg/apis/meta/v1.pyi` & `hera_workflows-5.6.0/src/hera/events/models/io/k8s/apimachinery/pkg/apis/meta/v1.pyi`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 from datetime import datetime
-from typing import Dict, List, Optional
-
 from hera.shared._base_model import BaseModel as BaseModel
+from typing import Dict, List, Optional
 
 class CreateOptions(BaseModel):
     dry_run: Optional[List[str]]
     field_manager: Optional[str]
     field_validation: Optional[str]
 
 class FieldsV1(BaseModel): ...
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/sensor.py` & `hera_workflows-5.6.0/src/hera/events/models/sensor.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,39 +1,40 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from .io.argoproj.events import v1alpha1
 from .io.k8s.apimachinery.pkg.apis.meta import v1
 
 
 class DeleteSensorResponse(BaseModel):
     pass
 
 
 class LogEntry(BaseModel):
-    dependency_name: Optional[str] = Field(None, alias="dependencyName", title="optional - trigger dependency name")
-    event_context: Optional[str] = Field(None, alias="eventContext", title="optional - Cloud Event context")
+    dependency_name: Optional[str] = Field(
+        default=None, alias="dependencyName", title="optional - trigger dependency name"
+    )
+    event_context: Optional[str] = Field(default=None, alias="eventContext", title="optional - Cloud Event context")
     level: Optional[str] = None
     msg: Optional[str] = None
     namespace: Optional[str] = None
-    sensor_name: Optional[str] = Field(None, alias="sensorName")
+    sensor_name: Optional[str] = Field(default=None, alias="sensorName")
     time: Optional[v1.Time] = None
-    trigger_name: Optional[str] = Field(None, alias="triggerName", title="optional - any trigger name")
+    trigger_name: Optional[str] = Field(default=None, alias="triggerName", title="optional - any trigger name")
 
 
 class CreateSensorRequest(BaseModel):
-    create_options: Optional[v1.CreateOptions] = Field(None, alias="createOptions")
+    create_options: Optional[v1.CreateOptions] = Field(default=None, alias="createOptions")
     namespace: Optional[str] = None
     sensor: Optional[v1alpha1.Sensor] = None
 
 
 class SensorWatchEvent(BaseModel):
     object: Optional[v1alpha1.Sensor] = None
     type: Optional[str] = None
```

### Comparing `hera_workflows-5.5.2/src/hera/events/models/sensor.pyi` & `hera_workflows-5.6.0/src/hera/events/models/sensor.pyi`

 * *Files 9% similar despite different names*

```diff
@@ -1,13 +1,11 @@
-from typing import Optional
-
-from hera.shared._base_model import BaseModel as BaseModel
-
 from .io.argoproj.events import v1alpha1 as v1alpha1
 from .io.k8s.apimachinery.pkg.apis.meta import v1 as v1
+from hera.shared._base_model import BaseModel as BaseModel
+from typing import Optional
 
 class DeleteSensorResponse(BaseModel): ...
 
 class LogEntry(BaseModel):
     dependency_name: Optional[str]
     event_context: Optional[str]
     level: Optional[str]
```

### Comparing `hera_workflows-5.5.2/src/hera/events/service.py` & `hera_workflows-5.6.0/src/hera/events/service.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+"""[DO NOT MODIFY] Auto-generated by `hera.scripts.service.py`."""
 from typing import Optional, cast
 from urllib.parse import urljoin
 
 import requests
 
 from hera.events.models import (
     CreateEventSourceRequest,
@@ -26,25 +27,29 @@
     Version,
 )
 from hera.exceptions import exception_from_server_response
 from hera.shared import global_config
 
 
 def valid_host_scheme(host: str) -> bool:
+    """Validates the the given `host` starts with either `http` or `https`."""
     return host.startswith("http://") or host.startswith("https://")
 
 
 class EventsService:
+    """The core events service for interacting with the Argo server."""
+
     def __init__(
         self,
         host: Optional[str] = None,
         verify_ssl: Optional[bool] = None,
         token: Optional[str] = None,
         namespace: Optional[str] = None,
-    ):
+    ) -> None:
+        """Events service constructor."""
         self.host = cast(str, host or global_config.host)
         self.verify_ssl = verify_ssl if verify_ssl is not None else global_config.verify_ssl
         self.token = token or global_config.token
         self.namespace = namespace or global_config.namespace
 
     def list_event_sources(
         self,
@@ -55,14 +60,15 @@
         allow_watch_bookmarks: Optional[bool] = None,
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
     ) -> EventSourceList:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/event-sources/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "listOptions.labelSelector": label_selector,
@@ -82,33 +88,35 @@
 
         if resp.ok:
             return EventSourceList(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def create_event_source(self, req: CreateEventSourceRequest, namespace: Optional[str] = None) -> EventSource:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/event-sources/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return EventSource(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def get_event_source(self, name: str, namespace: Optional[str] = None) -> EventSource:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/event-sources/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
             headers={"Authorization": f"Bearer {self.token}"},
@@ -120,21 +128,22 @@
             return EventSource(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def update_event_source(
         self, name: str, req: UpdateEventSourceRequest, namespace: Optional[str] = None
     ) -> EventSource:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/event-sources/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -149,14 +158,15 @@
         grace_period_seconds: Optional[str] = None,
         uid: Optional[str] = None,
         resource_version: Optional[str] = None,
         orphan_dependents: Optional[bool] = None,
         propagation_policy: Optional[str] = None,
         dry_run: Optional[list] = None,
     ) -> EventSourceDeletedResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.delete(
             url=urljoin(self.host, "api/v1/event-sources/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "deleteOptions.gracePeriodSeconds": grace_period_seconds,
@@ -173,33 +183,35 @@
 
         if resp.ok:
             return EventSourceDeletedResponse()
 
         raise exception_from_server_response(resp)
 
     def receive_event(self, discriminator: str, req: Item, namespace: Optional[str] = None) -> EventResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/events/{namespace}/{discriminator}").format(
                 discriminator=discriminator, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return EventResponse()
 
         raise exception_from_server_response(resp)
 
     def get_info(self) -> InfoResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/info"),
             params=None,
             headers={"Authorization": f"Bearer {self.token}"},
             data=None,
             verify=self.verify_ssl,
@@ -219,14 +231,15 @@
         allow_watch_bookmarks: Optional[bool] = None,
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
     ) -> SensorList:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/sensors/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "listOptions.labelSelector": label_selector,
@@ -246,33 +259,35 @@
 
         if resp.ok:
             return SensorList(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def create_sensor(self, req: CreateSensorRequest, namespace: Optional[str] = None) -> Sensor:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/sensors/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return Sensor(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def get_sensor(self, name: str, namespace: Optional[str] = None, resource_version: Optional[str] = None) -> Sensor:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/sensors/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params={"getOptions.resourceVersion": resource_version},
             headers={"Authorization": f"Bearer {self.token}"},
@@ -282,21 +297,22 @@
 
         if resp.ok:
             return Sensor(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def update_sensor(self, name: str, req: UpdateSensorRequest, namespace: Optional[str] = None) -> Sensor:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/sensors/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -311,14 +327,15 @@
         grace_period_seconds: Optional[str] = None,
         uid: Optional[str] = None,
         resource_version: Optional[str] = None,
         orphan_dependents: Optional[bool] = None,
         propagation_policy: Optional[str] = None,
         dry_run: Optional[list] = None,
     ) -> DeleteSensorResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.delete(
             url=urljoin(self.host, "api/v1/sensors/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "deleteOptions.gracePeriodSeconds": grace_period_seconds,
@@ -347,14 +364,15 @@
         allow_watch_bookmarks: Optional[bool] = None,
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
     ) -> EventSourceWatchEvent:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/stream/event-sources/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "listOptions.labelSelector": label_selector,
@@ -391,14 +409,15 @@
         seconds: Optional[str] = None,
         nanos: Optional[int] = None,
         timestamps: Optional[bool] = None,
         tail_lines: Optional[str] = None,
         limit_bytes: Optional[str] = None,
         insecure_skip_tls_verify_backend: Optional[bool] = None,
     ) -> EventsourceLogEntry:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/stream/event-sources/{namespace}/logs").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "name": name,
@@ -435,14 +454,15 @@
         allow_watch_bookmarks: Optional[bool] = None,
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
     ) -> Event:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/stream/events/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "listOptions.labelSelector": label_selector,
@@ -474,14 +494,15 @@
         allow_watch_bookmarks: Optional[bool] = None,
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
     ) -> SensorWatchEvent:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/stream/sensors/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "listOptions.labelSelector": label_selector,
@@ -517,14 +538,15 @@
         seconds: Optional[str] = None,
         nanos: Optional[int] = None,
         timestamps: Optional[bool] = None,
         tail_lines: Optional[str] = None,
         limit_bytes: Optional[str] = None,
         insecure_skip_tls_verify_backend: Optional[bool] = None,
     ) -> SensorLogEntry:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/stream/sensors/{namespace}/logs").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "name": name,
@@ -548,14 +570,15 @@
 
         if resp.ok:
             return SensorLogEntry(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def get_user_info(self) -> GetUserInfoResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/userinfo"),
             params=None,
             headers={"Authorization": f"Bearer {self.token}"},
             data=None,
             verify=self.verify_ssl,
@@ -563,14 +586,15 @@
 
         if resp.ok:
             return GetUserInfoResponse()
 
         raise exception_from_server_response(resp)
 
     def get_version(self) -> Version:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/version"),
             params=None,
             headers={"Authorization": f"Bearer {self.token}"},
             data=None,
             verify=self.verify_ssl,
```

### Comparing `hera_workflows-5.5.2/src/hera/expr/_node.py` & `hera_workflows-5.6.0/src/hera/expr/_node.py`

 * *Files 0% similar despite different names*

```diff
@@ -59,15 +59,15 @@
         Transpiles `+var` to `+var`.
         """
         return UnaryOp(self, "+")
 
     def __format__(self, format_spec: str) -> str:
         """Supports easy output formatting to construct variable substitution expressions.
 
-        Examples
+        Examples:
         --------
         f"{g.input.parameters.value:$}" == "{{input.parameters.value}}"
         f"{g.workflow.parameters.config.jsonpath('$.a'):=}" == "{{=jsonpath(workflow.parameters.config, '$.a')}}"
         f"{g.input.parameters.value}" == "input.parameters.value"
         """
         # For more details around this see https://peps.python.org/pep-3101/
         if not format_spec:
@@ -228,15 +228,15 @@
         return f'{{{", ".join(key_value_pairs)}}}'
     return repr(obj)
 
 
 class Constant(Node):
     """Supports transpiling inline python constants to expr expressions.
 
-    Examples
+    Examples:
     --------
     str(C(1)) == 1
     str(C(True)) == true
     str(C(None)) == nil
     str(C([1, 2, 3])) == [1, 2, 3]
     """
 
@@ -261,15 +261,15 @@
     def __repr__(self) -> str:
         return self.value
 
 
 class Parentheses(Node):
     """Supports transpiling groups of Python expressions so that they are parathesized properly in expr.
 
-    Examples
+    Examples:
     --------
     str(P(C(1) + C(2)) + 3) == (1 + 2) + 3
     """
 
     def __init__(self, value: Node):
         self.value = value
```

### Comparing `hera_workflows-5.5.2/src/hera/expr/_sprig.py` & `hera_workflows-5.6.0/src/hera/expr/_sprig.py`

 * *Files identical despite different names*

### Comparing `hera_workflows-5.5.2/src/hera/shared/_global_config.py` & `hera_workflows-5.6.0/src/hera/shared/_global_config.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+"""Global config module supports configuring Hera's behavior based on client requirements."""
 from __future__ import annotations
 
 import inspect
 from collections import defaultdict
 from dataclasses import dataclass, field
 from typing import Any, Callable, Dict, List, Optional, Type, TypeVar, Union
 
@@ -11,68 +12,97 @@
 
 from ._base_model import BaseModel
 
 TBase = TypeVar("TBase", bound="BaseMixin")
 TypeTBase = Type[TBase]
 
 Hook = Callable[[TBase], TBase]
+"""`Hook` is a callable that takes a Hera objects and returns the same, optionally mutated, object. 
+
+This can be a Workflow, a Script, a Container, etc - any Hera object. 
+"""
+
 _HookMap = Dict[Type[TBase], List[Hook]]
+"""mapping of Hera object type to the list of mutating hooks to apply to the object"""
+
 _Defaults = Dict[TBase, Dict]
+"""mapping of Hera object type to a dictionary of default field/value combinations"""
 
 
 @dataclass
 class _GlobalConfig:
     """Hera global configuration holds any user configuration such as global tokens, hooks, etc.
 
     Notes:
-        This should not be instantiated directly by the user. There is an instance of the `_GlobalConfig` in this module,
-        which is what should be used. Access as `hera.shared.global_config` or `hera.shared.GlobalConfig`.
+    -----
+    This should not be instantiated directly by the user. There is an instance of the `_GlobalConfig` in this module,
+    which is what should be used. Access as `hera.shared.global_config` or `hera.shared.GlobalConfig`.
     """
 
     # protected attributes are ones that are computed/go through some light processing upon setting or
     # are processed upon accessing. The rest, which use primitive types, such as `str`, can remain public
     _token: Optional[Union[str, TokenGenerator, Callable[[], Optional[str]]]] = None
+    """an optional authentication token used by Hera in communicating with the Argo server"""
+
     _image: Union[str, Callable[[], str]] = "python:3.8"
+    """an optional Docker image specification"""
+
     _pre_build_hooks: Optional[_HookMap] = None
+    """any pre build hooks to invoke before Hera builds the objects necessary for communicating with Argo"""
+
     _defaults: _Defaults = field(default_factory=lambda: defaultdict(dict))
+    """any Hera class defaults to use. These can be overriden by users, otherwise assume the specified default"""
 
     host: Optional[str] = None
+    """the host address of the Argo server"""
+
     verify_ssl: bool = True
+    """whether to perform SSL verification on the path towards communicating with the Argo server"""
+
     api_version: str = "argoproj.io/v1alpha1"
+    """the Argo API verison to use on models"""
+
     namespace: Optional[str] = None
+    """the Kubernetes namespace to use on any submitted workflows. This is used by the Argo workflow controller"""
+
     service_account_name: Optional[str] = None
+    """the service account name to be used by pods created via a workflow"""
+
     script_command: Optional[List[str]] = field(default_factory=lambda: ["python"])
+    """the default script command to use in starting up `Script` containers"""
+
     experimental_features: Dict[str, bool] = field(default_factory=lambda: defaultdict(bool))
+    """an indicator holder for any Hera experimental features to use"""
 
     def reset(self) -> None:
-        """Resets the global config container to its initial state"""
+        """Resets the global config container to its initial state."""
         self.__dict__ = _GlobalConfig().__dict__
 
     @property
     def image(self) -> str:
-        """Return the default image to use for Tasks"""
+        """Return the default image to use for Tasks."""
         if isinstance(self._image, str):
             return self._image
         return self._image()
 
     @image.setter
     def image(self, image: Union[str, Callable[[], str]]) -> None:
-        """Set the default image to use for Tasks"""
+        """Set the default image to use for Tasks."""
         self._image = image
 
     @property
     def token(self) -> Optional[str]:
-        """Returns an Argo Workflows global token"""
+        """Returns an Argo Workflows global token."""
         if self._token is None or isinstance(self._token, str):
             return self._token
         return self._token()
 
     @token.setter
     def token(self, t: Union[Optional[str], TokenGenerator, Callable[[], Optional[str]]]) -> None:
-        """Sets the Argo Workflows token at a global level so services can use it"""
+        """Sets the Argo Workflows token at a global level so services can use it."""
         self._token = t
 
     def register_pre_build_hook(self, hook: Hook) -> Hook:
         """Registers a hook to be called before building a model."""
         return_type = inspect.signature(hook).return_annotation
         if return_type is inspect.Signature.empty:
             raise TypeError("Hook must have a return type annotation")
@@ -103,30 +133,33 @@
         Args:
             cls: The class to get defaults for.
         """
         return self._defaults[cls]
 
 
 class BaseMixin(BaseModel):
-    # Note this is pydantic private method that
-    # is called after __init__
-    # In order to inject __hera_init__ after __init__
-    # without destroying the autocomplete, we have opted
-    # for this method. We also tried other ways
-    # including creating a metaclass that invokes hera_init
-    # after init, but that always broke auto-complete for vscode
     def _init_private_attributes(self):
+        """A pydantic private method called after `__init__`.
+
+        Notes:
+        -----
+        In order to inject `__hera_init__` after `__init__` without destroying the autocomplete, we opted for
+        this method. We also tried other ways including creating a metaclass that invokes hera_init after init,
+        but that always broke auto-complete for IDEs like VSCode.
+        """
         super()._init_private_attributes()
         self.__hera_init__()
 
     def __hera_init__(self):
+        """A method that is optionally implemented and invoked by `BaseMixin` subclasses to perform some post init."""
         ...
 
     @root_validator(pre=True)
     def _set_defaults(cls, values):
+        """Sets the user-provided defaults of Hera objects."""
         defaults = global_config._get_class_defaults(cls)
         for key, value in defaults.items():
             if values.get(key) is None:
                 values[key] = value
         return values
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/__init__.py` & `hera_workflows-5.6.0/src/hera/workflows/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,8 @@
-"""Hera classes
-"""
+"""Hera classes."""
 # [DO NOT EDIT MANUALLY]
 # Auto-generated by Hera via `make init-files`
 # In order to add objects to the hera.workflows namespace
 # add them to the __all__ list in the relevant module.
 # Hera submodules should not use `from hera.workflows import X`
 # themselves, as it introduces a circular dependency.
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/_context.py` & `hera_workflows-5.6.0/src/hera/workflows/_context.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,62 +1,80 @@
+"""Module that manages Hera's use of contexts.
+
+This module provides the functionality necessary to support the implementation backing elements such as `with`
+clauses for workflows and DAGs.
+"""
 from contextvars import ContextVar
 from typing import List, Optional, TypeVar, Union
 
 from hera.shared import BaseMixin
 from hera.workflows.exceptions import InvalidType
 from hera.workflows.protocol import Subbable, TTemplate
 
 TNode = TypeVar("TNode", bound="SubNodeMixin")
 
 _pieces = ContextVar("_pieces", default=None)
 
 
 class SubNodeMixin(BaseMixin):
-    """SubNodeMixin ensures that the class gets added to the Hera context on initialization."""
+    """SubNodeMixin ensures that the class gets added to the Hera context on initialization.
+
+    The mixin implements the core Hera `__hera_init__`, which is invoked post Hera object initialization. Anything
+    that inherits from this mixin has the capacity to manage a context via either being added to a context (like being
+    added to a Workflow/DAG) or managing a context itself (like holding Container, Script, etc).
+    """
 
     def __hera_init__(self: TNode) -> TNode:
+        """The Hera init that is invoked post object initialization."""
         _context.add_sub_node(self)
         return self
 
 
 class _HeraContext:
     """_HeraContext uses a ContextVar under the hood to store the context.
 
-    Note: To avoid the ContextVar being shared, it must be lazily initialized to an empty list at runtime,
-    and not at import time (Context is called at import time if we use the @script decorator for instance)
+    Notes:
+    -----
+    To avoid the ContextVar being shared, it must be lazily initialized to an empty list at runtime,
+    and not at import time (Context is called at import time if we use the @script decorator for instance).
     """
 
     def enter(self, p: Subbable) -> None:
+        """Adds the given 'subbable' piece to the context of the current parent object."""
         if not isinstance(p, Subbable):
             raise InvalidType(type(p))
         if self.pieces is None:
             self.pieces = []
         self.pieces.append(p)
 
     def exit(self) -> None:
+        """Pops the latest 'subbable' piece from the context."""
         if self.pieces:
             self.pieces.pop()
 
     @property
     def pieces(self) -> Optional[List[Subbable]]:
         """Get the context local variable for the pieces.
 
-        The variable is None at import time to prevent shared state between contexts
+        The variable is None at import time to prevent shared state between contexts.
         """
         return _pieces.get()
 
     @pieces.setter
-    def pieces(self, value):
+    def pieces(self, value) -> None:
+        """Sets the given values as the pieces of the context."""
         _pieces.set(value)
 
     @property
     def active(self) -> bool:
+        """Tells whether there's an active context."""
         return bool(self.pieces)
 
     def add_sub_node(self, node: Union[SubNodeMixin, TTemplate]) -> None:
+        """Adds the given node to the active context."""
         pieces = self.pieces
         if not pieces:
             return
 
         try:
             # here, we are trying to add a node to the last piece of context in the hopes that it is a subbable
             pieces[-1]._add_sub(node)
@@ -64,15 +82,16 @@
             # if the above fails, it means the user invoked a decorated function e.g. `@script`. Hence,
             # the object needs to be added as a template to the piece of context at [-1]. This will be the case for
             # DAGs and Steps
             pieces[-1]._add_sub(node.template)  # type: ignore
 
         # when the above does not raise an exception, it means the user invoked a decorated function e.g. `@script`
         # inside a proper context. Here, we add the object to the overall workflow context, directly as a template,
-        # in case it is not found (based on the name)
+        # in case it is not found (based on the name). This helps users save on the number of templates that are
+        # added when using an object that is a `Script`
         if hasattr(node, "template") and node.template is not None and not isinstance(node.template, str):
             found = False
             for t in pieces[0].templates:  # type: ignore
                 if t.name == node.template.name:
                     found = True
                     break
             if not found:
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/_inspect.py` & `hera_workflows-5.6.0/src/hera/workflows/_inspect.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 # mypy: ignore-errors
-""" Copy of get_annotations and required imports from python 3.10 inspect module"""
+"""Copy of get_annotations and required imports from python 3.10 inspect module."""
 
 import dis
 import functools
 import sys
 import types
 
 # Create constants for the compiler flags in Include/code.h
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/_mixins.py` & `hera_workflows-5.6.0/src/hera/workflows/_mixins.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+"""Core collection of Hera mixins that isolate shareable functionality between Hera objects."""
 from __future__ import annotations
 
 import inspect
 from pathlib import Path
 
 try:
     from inspect import get_annotations  # type: ignore
@@ -21,15 +22,15 @@
 from hera.shared import BaseMixin, global_config
 from hera.shared._base_model import BaseModel
 from hera.shared.serialization import serialize
 from hera.workflows._context import SubNodeMixin, _context
 from hera.workflows.artifact import Artifact
 from hera.workflows.env import Env, _BaseEnv
 from hera.workflows.env_from import _BaseEnvFrom
-from hera.workflows.exceptions import InvalidTemplateCall, InvalidType
+from hera.workflows.exceptions import InvalidTemplateCall
 from hera.workflows.metrics import Metrics, _BaseMetric
 from hera.workflows.models import (
     HTTP,
     Affinity,
     Arguments as ModelArguments,
     Artifact as ModelArtifact,
     ArtifactLocation,
@@ -88,98 +89,157 @@
 InputsT = Optional[
     Union[
         ModelInputs,
         Union[Parameter, ModelParameter, Artifact, ModelArtifact, Dict[str, Any]],
         List[Union[Parameter, ModelParameter, Artifact, ModelArtifact, Dict[str, Any]]],
     ]
 ]
+"""`InputsT` is the main type associated with inputs that can be specified in Hera workflows, dags, steps, etc.
+
+This type enables uses of Hera auto-generated models such as (`hera.workflows.models.Inputs`, 
+`hera.workflows.models.Parameter`), Hera managed models such as (`hera.workflows.Parameter`, 
+`hera.workflows.Artifact`), dictionary mappings of parameter names to values (auto-converted by Hera to 
+`hera.workflows.Parameter`), or lists of any of the aforementioned objects.
+"""
+
 OutputsT = Optional[
     Union[
         ModelOutputs,
         Union[Parameter, ModelParameter, Artifact, ModelArtifact],
         List[Union[Parameter, ModelParameter, Artifact, ModelArtifact]],
     ]
 ]
+"""`OutputsT` is the main type associated with outputs the can be specified in Hera workflows, dags, steps, etc.
+
+This type enables uses of Hera auto-generated models such as (`hera.workflows.models.Outputs`, 
+`hera.workflows.models.Parameter`), Hera managed models such as (`hera.workflows.Parameter`, 
+`hera.workflows.Artifact`),  or lists of the aforementioned objects.
+"""
+
 ArgumentsT = Optional[
     Union[
         ModelArguments,
         Union[Parameter, ModelParameter, Artifact, ModelArtifact, Dict[str, Any]],
         List[Union[Parameter, ModelParameter, Artifact, ModelArtifact, Dict[str, Any]]],
     ]
 ]
+"""`ArgumentsT` is the main type associated with arguments that can be used on DAG tasks, steps, etc.
+
+This type enables uses of the Hera auto-generated `hera.workflows.models.Arguments` model, Hera managed models such as 
+`hera.workflows.Parameter`, `hera.workflows.Artifact`, a dictionary mapping of parameter names to values, or a list of
+any of the aforementioned objects.
+"""
+
 MetricsT = Optional[
     Union[
         _BaseMetric,
         List[_BaseMetric],
         Metrics,
         ModelPrometheus,
         List[ModelPrometheus],
         ModelMetrics,
     ]
 ]
+"""`MetricsT` is the core Hera type for Prometheus metrics. 
+
+This metrics type enables users to use either auto-generated Hera metrics, lists of auto-generated single metrics, or
+the variations of metrics provided by `hera.workflows.metrics.*`  
+"""
+
 EnvT = Optional[
     Union[
         _BaseEnv,
         EnvVar,
         List[Union[_BaseEnv, EnvVar, Dict[str, Any]]],
         Dict[str, Any],
     ]
 ]
+"""`EnvT` is the core Hera type for environment variables.
+
+The env type enables setting single valued environment variables, lists of environment variables, or dictionary 
+mappings of env variables names to values, which are automatically parsed by Hera.
+"""
+
 EnvFromT = Optional[Union[_BaseEnvFrom, EnvFromSource, List[Union[_BaseEnvFrom, EnvFromSource]]]]
+"""`EnvFromT` is the core Hera type for environment variables derived from Argo/Kubernetes sources.
+
+This env type enables specifying environment variables in base form, as `hera.workflows.env` form, or lists of the 
+aforementioned objects.
+"""
+
 VolumesT = Optional[Union[Union[ModelVolume, _BaseVolume], List[Union[ModelVolume, _BaseVolume]]]]
+"""`VolumesT` is the core Hera type for volumes. 
+
+This volume type is used to specify the configuration of volumes to be automatically created by Argo/K8s and mounted
+by Hera at specific mount paths in containers.
+"""
+
 TContext = TypeVar("TContext", bound="ContextMixin")
+"""`TContext` is the bounded context controlled by the context mixin that enable context management in workflow/dag"""
+
 THookable = TypeVar("THookable", bound="HookMixin")
+"""`THookable` is the type associated with mixins that provide the ability to apply hooks from the global config"""
 
 
 class HookMixin(BaseMixin):
+    """`HookMixin` provides the ability to dispatch hooks set on the global config to any inheritors."""
+
     def _dispatch_hooks(self: THookable) -> THookable:
+        """Dispatches the global hooks on the current object."""
         output = self
         for hook in global_config._get_pre_build_hooks(output):
             output = hook(output)
             if output is None:
                 raise RuntimeError(
                     f"Pre-build hook {hook.__name__} returned None."
                     "Please ensure you are returning the output value from the hook."
                 )
         return output
 
 
 class ContextMixin(BaseMixin):
-    def __enter__(self: TContext) -> TContext:
-        """Enter the context of the workflow"""
+    """`ContextMixin` provides the ability to implement context management.
 
+    The mixin implements the `__enter__` and `__exit__` functionality that enables the core `with` clause. The mixin
+    expects that inheritors implement the `_add_sub` functionality, which adds a node defined within the context to the
+    main object context such as `Workflow`, `DAG`, or `ContainerSet`.
+    """
+
+    def __enter__(self: TContext) -> TContext:
+        """Enter the context of the inheritor."""
         _context.enter(self)
         return self
 
     def __exit__(self, *_) -> None:
-        """Leave the context of the workflow.
-
-        This supports using `with Workflow(...)`.
-        """
+        """Leave the context of the inheritor."""
         _context.exit()
 
     def _add_sub(self, node: Any) -> Any:
+        """Adds the supplied node to the context of the inheritor."""
         raise NotImplementedError()
 
 
 class ContainerMixin(BaseMixin):
+    """`ContainerMixin` provides a subset of the fields of a container such as image, probes, etc."""
+
     image: Optional[str] = None
     image_pull_policy: Optional[Union[str, ImagePullPolicy]] = None
 
     liveness_probe: Optional[Probe] = None
     ports: Optional[List[ContainerPort]] = None
     readiness_probe: Optional[Probe] = None
     startup_probe: Optional[Probe] = None
     stdin: Optional[bool] = None
     stdin_once: Optional[bool] = None
     termination_message_path: Optional[str] = None
     termination_message_policy: Optional[TerminationMessagePolicy] = None
     tty: Optional[bool] = None
 
     def _build_image_pull_policy(self) -> Optional[ImagePullPolicy]:
+        """Processes the image pull policy field and returns a generated `ImagePullPolicy` enum."""
         if self.image_pull_policy is None:
             return None
         elif isinstance(self.image_pull_policy, ImagePullPolicy):
             return self.image_pull_policy
 
         # this helps map image pull policy values as a convenience
         policy_mapper = {
@@ -191,29 +251,34 @@
             # some users might submit the policy in lowercase
             **{ipp.name.lower(): ipp for ipp in ImagePullPolicy},
         }
         try:
             return ImagePullPolicy[policy_mapper[self.image_pull_policy].name]
         except KeyError as e:
             raise KeyError(
-                f"Supplied image policy {self.image_pull_policy} is not valid. Use one of {ImagePullPolicy.__members__}"
+                f"Supplied image policy {self.image_pull_policy} is not valid. "
+                "Use one of {ImagePullPolicy.__members__}"
             ) from e
 
     @validator("image", pre=True, always=True)
     def _set_image(cls, v):
+        """Validator that sets the image field to the global image unless the image is specified on the container."""
         if v is None:
             return global_config.image
         return v
 
 
 class IOMixin(BaseMixin):
+    """`IOMixin` provides the capabilities of performing I/O between steps via fields such as `inputs`/`outputs`."""
+
     inputs: InputsT = None
     outputs: OutputsT = None
 
     def _build_inputs(self) -> Optional[ModelInputs]:
+        """Processes the `inputs` field and returns a generated `ModelInputs`."""
         if self.inputs is None:
             return None
         elif isinstance(self.inputs, ModelInputs):
             return self.inputs
 
         result = ModelInputs()
         inputs = self.inputs if isinstance(self.inputs, list) else [self.inputs]
@@ -233,19 +298,22 @@
                     [value._build_artifact()]
                     if result.artifacts is None
                     else result.artifacts + [value._build_artifact()]
                 )
             else:
                 result.artifacts = [value] if result.artifacts is None else result.artifacts + [value]
 
+        # returning `None` for `ModelInputs` means the submission to the server will not even have the `inputs` field
+        # set, which saves some space
         if result.parameters is None and result.artifacts is None:
             return None
         return result
 
     def _build_outputs(self) -> Optional[ModelOutputs]:
+        """Processes the `outputs` field and returns a generated `ModelOutputs`."""
         if not self.outputs:
             return None
         elif isinstance(self.outputs, ModelOutputs):
             return self.outputs
 
         result = ModelOutputs()
         outputs = self.outputs if isinstance(self.outputs, list) else [self.outputs]
@@ -261,77 +329,72 @@
                     [value._build_artifact()]
                     if result.artifacts is None
                     else result.artifacts + [value._build_artifact()]
                 )
             else:
                 result.artifacts = [value] if result.artifacts is None else result.artifacts + [value]
 
+        # returning `None` for `ModelInputs` means the submission to the server will not even have the `outputs` field
+        # set, which saves some space
         if result.parameters is None and result.artifacts is None:
             return None
         return result
 
 
 class EnvMixin(BaseMixin):
+    """`EnvMixin` provides the ability to set simple env variables along with env variables that are derived."""
+
     env: EnvT = None
     env_from: EnvFromT = None
 
     def _build_env(self) -> Optional[List[EnvVar]]:
+        """Processes the `env` field and returns a list of generated `EnvVar` or `None`."""
         if self.env is None:
             return None
 
         result: List[EnvVar] = []
         env = self.env if isinstance(self.env, list) else [self.env]
         for e in env:
             if isinstance(e, EnvVar):
                 result.append(e)
             elif issubclass(e.__class__, _BaseEnv):
                 result.append(e.build())
             elif isinstance(e, dict):
                 for k, v in e.items():
                     result.append(EnvVar(name=k, value=v))
-        return result
+
+        # returning `None` for `envs` means the submission to the server will not even have the `envs` field
+        # set, which saves some space
+        return result if result else None
 
     def _build_env_from(self) -> Optional[List[EnvFromSource]]:
+        """Processes the `env_from` field and returns a list of generated `EnvFrom` or `None`."""
         if self.env_from is None:
             return None
 
         result: List[EnvFromSource] = []
         env_from = self.env_from if isinstance(self.env_from, list) else [self.env_from]
         for e in env_from:
             if isinstance(e, EnvFromSource):
                 result.append(e)
             elif issubclass(e.__class__, _BaseEnvFrom):
                 result.append(e.build())
-        return result
-
-    def _build_params_from_env(self) -> Optional[List[Parameter]]:
-        if self.env is None:
-            return None
-
-        params: Optional[List[Parameter]] = None
-        for spec in self.env:
-            if isinstance(spec, Env) and spec.value_from_input is not None:
-                value = (
-                    spec.value_from_input.value
-                    if isinstance(spec.value_from_input, Parameter)
-                    else spec.value_from_input
-                )
-                params = (
-                    [Parameter(name=spec.param_name, value=value)]
-                    if params is None
-                    else params + [Parameter(name=spec.param_name, value=value)]
-                )
 
-        return params
+        # returning `None` for `envs` means the submission to the server will not even have the `env_from` field
+        # set, which saves some space
+        return result if result else None
 
 
 class MetricsMixin(BaseMixin):
+    """`MetricsMixin` provides the ability to set metrics on a n object."""
+
     metrics: MetricsT = None
 
     def _build_metrics(self) -> Optional[ModelMetrics]:
+        """Processes the `metrics` field and returns the generated `ModelMetrics` or `None`."""
         if self.metrics is None or isinstance(self.metrics, ModelMetrics):
             return self.metrics
         elif isinstance(self.metrics, ModelPrometheus):
             return ModelMetrics(prometheus=[self.metrics])
         elif isinstance(self.metrics, Metrics):
             return ModelMetrics(prometheus=self.metrics._build_metrics())
         elif isinstance(self.metrics, _BaseMetric):
@@ -339,18 +402,23 @@
 
         metrics = []
         for m in self.metrics:
             if isinstance(m, _BaseMetric):
                 metrics.append(m._build_metric())
             else:
                 metrics.append(m)
-        return ModelMetrics(prometheus=metrics)
+        return ModelMetrics(prometheus=metrics) if metrics else None
 
 
 class TemplateMixin(SubNodeMixin, HookMixin, MetricsMixin):
+    """`TemplateMixin` provides the Argo template fields that are shared between different sub-template fields.
+
+    The supported sub-template fields are `Script`, `Data`, `DAG`, `Resource`, `Container`, `ContainerSet`, etc.
+    """
+
     active_deadline_seconds: Optional[Union[int, str, IntOrString]] = None
     affinity: Optional[Affinity] = None
     archive_location: Optional[ArtifactLocation] = None
     automount_service_account_token: Optional[bool] = None
     daemon: Optional[bool] = None
     executor: Optional[ExecutorConfig] = None
     fail_fast: Optional[bool] = None
@@ -373,81 +441,100 @@
     service_account_name: Optional[str] = None
     sidecars: Optional[Union[UserContainer, List[UserContainer]]] = None
     synchronization: Optional[Synchronization] = None
     timeout: Optional[str] = None
     tolerations: Optional[List[Toleration]] = None
 
     def _build_sidecars(self) -> Optional[List[UserContainer]]:
+        """Builds the `sidecars` field and optionally returns a list of `UserContainer`."""
         if self.sidecars is None:
             return None
 
         if isinstance(self.sidecars, UserContainer):
             return [self.sidecars]
 
         return self.sidecars
 
     def _build_active_deadline_seconds(self) -> Optional[IntOrString]:
+        """Builds the `active_deadline_seconds` field and optionally returns a generated `IntOrString`."""
         if self.active_deadline_seconds is None:
             return None
 
         return IntOrString(__root__=str(self.active_deadline_seconds))
 
     def _build_metadata(self) -> Optional[Metadata]:
+        """Builds the `metadata` field of the template since the `annotations` and `labels` fields are separated."""
         if self.annotations is None and self.labels is None:
             return None
 
         return Metadata(
             annotations=self.annotations,
             labels=self.labels,
         )
 
 
 class ResourceMixin(BaseMixin):
+    """`ResourceMixin` provides the capability to set resources such as compute requirements like CPU, GPU, etc."""
+
     resources: Optional[Union[ResourceRequirements, Resources]] = None
 
     def _build_resources(self) -> Optional[ResourceRequirements]:
+        """Parses the resources and returns a generated `ResourceRequirements` object."""
         if self.resources is None or isinstance(self.resources, ResourceRequirements):
             return self.resources
-
         return self.resources.build()
 
 
 class VolumeMixin(BaseMixin):
+    """`VolumeMixin` provides the ability to set volumes on an inheriting resource.
+
+    Note that *any* volumes set on the `volumes` field automatically get an associated persistent volume claim
+    constructed and set on the workflow. This way users do not have to set the PVC themselves. However, clients of the
+    mixin should be careful to *not* generate multiple PVCs for the same volume.
+    """
+
     volumes: VolumesT = None
 
     def _build_volumes(self) -> Optional[List[ModelVolume]]:
+        """Processes the `volumes` and creates an optional list of generates `Volume`s."""
         if self.volumes is None:
             return None
 
         volumes = self.volumes if isinstance(self.volumes, list) else [self.volumes]
         # filter volumes for otherwise we're building extra Argo volumes
         filtered_volumes = [v for v in volumes if not isinstance(v, Volume)]
         # only build volumes if there are any of type `_BaseVolume`, otherwise it must be an autogenerated model
         # already, so kept it as it is
         result = [v._build_volume() if issubclass(v.__class__, _BaseVolume) else v for v in filtered_volumes]
         return result or None
 
     def _build_persistent_volume_claims(self) -> Optional[List[PersistentVolumeClaim]]:
+        """Generates the list of persistent volume claims to associate with the set `volumes`."""
         if self.volumes is None:
             return None
 
         volumes = self.volumes if isinstance(self.volumes, list) else [self.volumes]
         volumes_with_pv_claims = [v for v in volumes if isinstance(v, Volume)]
         if not volumes_with_pv_claims:
             return None
-
-        claims = [v._build_persistent_volume_claim() for v in volumes_with_pv_claims]
-        return claims or None
+        return [v._build_persistent_volume_claim() for v in volumes_with_pv_claims] or None
 
 
 class VolumeMountMixin(VolumeMixin):
+    """`VolumeMountMixin` supports setting `volume_devices` and `volume_mounts` on the inheritor.
+
+    Devices and mounts are approaches for mounting existing volume resources from a cluster on the job that is
+    created via inheriting from this mixin.
+    """
+
     volume_devices: Optional[List[VolumeDevice]] = None
     volume_mounts: Optional[List[VolumeMount]] = None
 
     def _build_volume_mounts(self) -> Optional[List[VolumeMount]]:
+        """Processes the `volume_mounts` field and generates an optional list of `VolumeMount`s."""
         # while it's possible for `volume_mounts` to be `None`, this has to check that `volumes` is also `None` since
         # it's possible that Hera can find volume mounts to generate for the user if there are any volumes set
         if self.volume_mounts is None and self.volumes is None:
             return None
 
         if isinstance(self.volumes, list):
             volumes = self.volumes
@@ -467,17 +554,20 @@
             return self.volume_mounts
 
         mounts = cast(List[VolumeMount], self.volume_mounts) or [] + cast(List[VolumeMount], result) or []
         return mounts or None
 
 
 class ArgumentsMixin(BaseMixin):
+    """`ArgumentsMixin` provides the ability to set the `arguments` field on the inheriting object."""
+
     arguments: ArgumentsT = None
 
     def _build_arguments(self) -> Optional[ModelArguments]:
+        """Processes the `arguments` field and builds the optional generated `Arguments` to set as arguments."""
         if self.arguments is None:
             return None
         elif isinstance(self.arguments, ModelArguments):
             return self.arguments
 
         result = ModelArguments()
         arguments = self.arguments if isinstance(self.arguments, list) else [self.arguments]
@@ -495,21 +585,36 @@
             elif isinstance(arg, ModelParameter):
                 result.parameters = [arg] if result.parameters is None else result.parameters + [arg]
             elif isinstance(arg, Parameter):
                 result.parameters = (
                     [arg.as_argument()] if result.parameters is None else result.parameters + [arg.as_argument()]
                 )
 
+        # returning `None` for `Arguments` means the submission to the server will not even have the
+        # `arguments` field set, which saves some payload
         if result.parameters is None and result.artifacts is None:
             return None
         return result
 
 
 class CallableTemplateMixin(ArgumentsMixin):
-    def __call__(self, *args, **kwargs) -> Union[Step, Task]:
+    """`CallableTemplateMixin` provides the ability to 'call' the template like a regular Python function.
+
+    The callable template implements the `__call__` method for the inheritor. The `__call__` method supports invoking
+    the template as a regular Python function. The call must be executed within an active context, which is a
+    `Workflow`, `DAG` or `Steps` context since the call optionally returns a `Step` or a `Task` depending on the active
+    context (`None` for `Workflow`, `Step` for `Steps` and `Task` for `DAG`). Note that `Steps` also supports calling
+    templates in a parallel steps context via using `Steps(...).parallel()`. When the call is executed and the template
+    does not exist on the active context, i.e. the workflow, it is automatically added for the user. Note that invoking
+    the same template multiple times does *not* result in the creation/addition of the same template to the active
+    context/workflow. Rather, a union is performed, so space is saved for users on the templates field and templates are
+    not duplicated.
+    """
+
+    def __call__(self, *args, **kwargs) -> Union[None, Step, Task]:
         if "name" not in kwargs:
             kwargs["name"] = self.name  # type: ignore
 
         arguments = self._get_arguments(**kwargs)
         parameter_names = self._get_parameter_names(arguments)
         artifact_names = self._get_artifact_names(arguments)
 
@@ -524,49 +629,68 @@
             arguments += self._get_deduped_params_from_items(parameter_names, kwargs["with_items"])
 
         # it is possible for the user to pass `arguments` via `kwargs` along with `with_param`. The `with_param`
         # additional parameters are inferred and have to be added to the `kwargs['arguments']`, otherwise
         # the step/task will miss adding them when building the final arguments
         kwargs["arguments"] = arguments
 
-        try:
-            from hera.workflows.steps import Step
-
-            return Step(*args, template=self, **kwargs)
-        except InvalidType:
-            pass
+        from hera.workflows.dag import DAG
+        from hera.workflows.script import Script
+        from hera.workflows.steps import Parallel, Step, Steps
+        from hera.workflows.task import Task
+        from hera.workflows.workflow import Workflow
+
+        if _context.pieces:
+            if isinstance(_context.pieces[-1], Workflow):
+                # Notes on callable templates under a Workflow:
+                # * If the user calls a script directly under a Workflow (outside of a Steps/DAG) then we add the script
+                #   template to the workflow and return None.
+                # * Containers, ContainerSets and Data objects (i.e. subclasses of CallableTemplateMixin) are already
+                #   added when initialized under the Workflow context so a callable doesn't make sense in that context,
+                #   so we raise an InvalidTemplateCall exception.
+                # * We do not currently validate the added templates to stop a user adding the same template multiple times,
+                #   which can happen if "calling" the same script multiple times to add it to the workflow, or initializing
+                #   a second `Container` exactly like the first.
+                if isinstance(self, Script):
+                    _context.add_sub_node(self)
+                    return None
 
-        try:
-            from hera.workflows.task import Task
+                raise InvalidTemplateCall(
+                    f"Callable Template '{self.name}' is not callable under a Workflow"  # type: ignore
+                )
+            if isinstance(_context.pieces[-1], (Steps, Parallel)):
+                return Step(*args, template=self, **kwargs)
 
-            return Task(*args, template=self, **kwargs)
-        except InvalidType:
-            pass
+            if isinstance(_context.pieces[-1], DAG):
+                return Task(*args, template=self, **kwargs)
 
-        raise InvalidTemplateCall("Container is not under a Steps, Parallel, or DAG context")
+        raise InvalidTemplateCall(
+            f"Callable Template '{self.name}' is not under a Workflow, Steps, Parallel, or DAG context"  # type: ignore
+        )
 
     def _get_arguments(self, **kwargs) -> List:
-        """Returns a list of arguments from the kwargs given to the template call"""
-
+        """Returns a list of arguments from the kwargs given to the template call."""
         # these are the already set parameters. If a user has already set a parameter argument, then Hera
         # uses the user-provided value rather than the inferred value
         kwargs_arguments = kwargs.get("arguments", [])
         kwargs_arguments = (
             kwargs_arguments if isinstance(kwargs_arguments, List) else [kwargs_arguments]
         )  # type: ignore
         arguments = (
             self.arguments if isinstance(self.arguments, List) else [self.arguments] + kwargs_arguments
         )  # type: ignore
         return list(filter(lambda x: x is not None, arguments))
 
     def _get_parameter_names(self, arguments: List) -> Set[str]:
+        """Returns the set of parameter names that are currently set on the mixin inheritor."""
         parameters = [arg for arg in arguments if isinstance(arg, ModelParameter) or isinstance(arg, Parameter)]
         return {p.name for p in parameters}
 
     def _get_artifact_names(self, arguments: List) -> Set[str]:
+        """Returns the set of artifact names that are currently set on the mixin inheritor."""
         artifacts = [arg for arg in arguments if isinstance(arg, ModelArtifact) or isinstance(arg, Artifact)]
         return {a.name for a in artifacts}
 
     def _get_deduped_params_from_source(
         self, parameter_names: Set[str], artifact_names: Set[str], source: Callable
     ) -> List[Parameter]:
         """Infer arguments from the given source and deduplicates based on the given params and artifacts.
@@ -582,15 +706,15 @@
         parameter_names: Set[str]
             Set of already constructed parameter names.
         artifact_names: Set[str]
             Set of already constructed artifact names.
         source: Callable
             The source function to infer the arguments from.
 
-        Returns
+        Returns:
         -------
         List[Parameter]
             The list of inferred arguments to set.
         """
         new_arguments = []
         new_parameters = _get_params_from_source(source)
         if new_parameters is not None:
@@ -610,46 +734,63 @@
         Parameters
         ----------
         parameter_names: Set[str]
             Set of already constructed parameter names.
         items: List[Any]
             The items to infer the arguments from.
 
-        Returns
+        Returns:
         -------
         List[Parameter]
             The list of inferred arguments to set.
         """
         item_params = _get_params_from_items(items)
         new_params = []
         if item_params is not None:
             for p in item_params:
                 if p.name not in parameter_names:
                     new_params.append(p)
         return new_params
 
 
 class ParameterMixin(BaseMixin):
+    """`ParameterMixin` supports the usage of `with_param` on inheritors."""
+
     with_param: Optional[Any] = None  # this must be a serializable object, or `hera.workflows.parameter.Parameter`
 
     def _build_with_param(self) -> Optional[str]:
+        """Build the `with_param` field and returns the corresponding `str`.
+
+        The string encodes what to parallelize a process over.
+        """
         if self.with_param is None:
             return None
 
         if isinstance(self.with_param, Parameter):
             return self.with_param.value
         elif isinstance(self.with_param, str):
             return self.with_param
         return serialize(self.with_param)
 
 
 class ItemMixin(BaseMixin):
-    with_items: Optional[List[Any]] = None  # this must composed of serializable objects
+    """Add `with_items` capability for inheritors, which supports parallelism over supplied items.
+
+    Notes:
+        The items passed in `with_items` must be serializable objects
+    """
+
+    with_items: Optional[List[Any]] = None
 
     def _build_with_items(self) -> Optional[List[Item]]:
+        """Process the `with_items` field and returns an optional list of corresponding `Item`s.
+
+        Notes:
+            Tthese `Item`s contain the serialized version of the supplied items/values.
+        """
         if self.with_items is None:
             return None
 
         if isinstance(self.with_items, list):
             items = []
             for item in self.with_items:
                 if isinstance(item, Parameter):
@@ -667,15 +808,39 @@
             return [Item(__root__=self.with_items.value)]
         elif isinstance(self.with_items, str):
             return [Item(__root__=self.with_items)]
         return [Item(__root__=serialize(self.with_items))]
 
 
 class EnvIOMixin(EnvMixin, IOMixin):
+    """`EnvIOMixin` provides the capacity to use environment variables."""
+
+    def _build_params_from_env(self) -> Optional[List[Parameter]]:
+        """Assemble a list of any environment variables that are set to obtain values from `Parameter`s."""
+        if self.env is None:
+            return None
+
+        params: Optional[List[Parameter]] = None
+        for spec in self.env:
+            if isinstance(spec, Env) and spec.value_from_input is not None:
+                value = (
+                    spec.value_from_input.value
+                    if isinstance(spec.value_from_input, Parameter)
+                    else spec.value_from_input
+                )
+                params = (
+                    [Parameter(name=spec.param_name, value=value)]
+                    if params is None
+                    else params + [Parameter(name=spec.param_name, value=value)]
+                )
+
+        return params if params else None
+
     def _build_inputs(self) -> Optional[ModelInputs]:
+        """Builds the inputs from the combination of env variables that require specific input parameters to be set."""
         inputs = super()._build_inputs()
         env_params = self._build_params_from_env()
         if inputs is None and env_params is None:
             return None
         elif inputs is None:
             return ModelInputs(parameters=env_params)
         elif env_params is None:
@@ -690,38 +855,41 @@
         for param in env_params:
             if param.name not in already_set_params:
                 inputs.parameters = [param] if inputs.parameters is None else inputs.parameters + [param]
         return inputs
 
 
 class TemplateInvocatorSubNodeMixin(BaseMixin):
-    """Used for classes that form sub nodes of Template Invocators - "Steps" and "DAG".
+    """Used for classes that form sub nodes of Template invocators - `Steps` and `DAG`.
 
-    See https://argoproj.github.io/argo-workflows/workflow-concepts/#template-invocators for
-    more on template invocators
+    See Also:
+    --------
+    https://argoproj.github.io/argo-workflows/workflow-concepts/#template-invocators for more on template invocators.
     """
 
     name: str
     continue_on: Optional[ContinueOn] = None
     hooks: Optional[Dict[str, LifecycleHook]] = None
     on_exit: Optional[Union[str, Templatable]] = None
     template: Optional[Union[str, Template, TemplateMixin]] = None
     template_ref: Optional[TemplateRef] = None
     inline: Optional[Union[Template, TemplateMixin]] = None
     when: Optional[str] = None
     with_sequence: Optional[Sequence] = None
 
     def _build_on_exit(self) -> Optional[str]:
+        """Builds the `on_exit` field `str` representation from the set `Templatable` or the specified `str`."""
         if isinstance(self.on_exit, Templatable):
             return self.on_exit._build_template().name  # type: ignore
         return self.on_exit
 
     @property
     def _subtype(self) -> str:
-        raise NotImplementedError
+        """Provides the subtype specification of the inheritor."""
+        raise NotImplementedError("Implement me")
 
     @property
     def id(self) -> str:
         """ID of this node."""
         return f"{{{{{self._subtype}.{self.name}.id}}}}"
 
     @property
@@ -751,63 +919,66 @@
 
     @property
     def result(self) -> str:
         """Result holds the result (stdout) of a script template."""
         return f"{{{{{self._subtype}.{self.name}.outputs.result}}}}"
 
     def get_result_as(self, name: str) -> Parameter:
+        """Returns a `Parameter` specification with the given name containing the `results` of `self`."""
         return Parameter(name=name, value=self.result)
 
     @root_validator(pre=False)
     def _check_values(cls, values):
+        """Validates that a single field is set between `template`, `template_ref`, and `inline`."""
+
         def one(xs: List):
             xs = list(map(bool, xs))
             return xs.count(True) == 1
 
         if not one([values.get("template"), values.get("template_ref"), values.get("inline")]):
-            raise ValueError("exactly one of ['template', 'template_ref', 'inline'] must be present")
+            raise ValueError("Exactly one of ['template', 'template_ref', 'inline'] must be present")
         return values
 
     def _get_parameters_as(self, name: str, subtype: str) -> Parameter:
         """Returns a `Parameter` that represents all the outputs of the specified subtype.
 
         Parameters
         ----------
         name: str
             The name of the parameter to search for.
         subtype: str
             The inheritor subtype field, used to construct the output artifact `from_` reference.
 
-        Returns
+        Returns:
         -------
         Parameter
             The parameter, named based on the given `name`, along with a value that references all outputs.
         """
         return Parameter(name=name, value=f"{{{{{subtype}.{self.name}.outputs.parameters}}}}")
 
     def _get_parameter(self, name: str, subtype: str) -> Parameter:
         """Attempts to find the specified parameter in the outputs for the specified subtype.
 
-        Notes
+        Notes:
         -----
         This is specifically designed to be invoked by inheritors.
 
         Parameters
         ----------
         name: str
             The name of the parameter to search for.
         subtype: str
             The inheritor subtype field, used to construct the output artifact `from_` reference.
 
-        Returns
+        Returns:
         -------
         Parameter
             The parameter if found.
 
-        Raises
+        Raises:
         ------
         ValueError
             When no outputs can be constructed/no outputs are set.
         KeyError
             When the artifact is not found.
         NotImplementedError
             When something else other than an `Parameter` is found for the specified name.
@@ -835,31 +1006,31 @@
                 value=f"{{{{{subtype}.{self.name}.outputs.parameters.{name}}}}}",
             )
         raise KeyError(f"No output parameter named `{name}` found")
 
     def _get_artifact(self, name: str, subtype: str) -> Artifact:
         """Attempts to find the specified artifact in the outputs for the specified subtype.
 
-        Notes
+        Notes:
         -----
         This is specifically designed to be invoked by inheritors.
 
         Parameters
         ----------
         name: str
             The name of the artifact to search for.
         subtype: str
             The inheritor subtype field, used to construct the output artifact `from_` reference.
 
-        Returns
+        Returns:
         -------
         Artifact
             The artifact if found.
 
-        Raises
+        Raises:
         ------
         ValueError
             When no outputs can be constructed/no outputs are set.
         KeyError
             When the artifact is not found.
         NotImplementedError
             When something else other than an `Artifact` is found for the specified name.
@@ -889,31 +1060,39 @@
         """Returns a `Parameter` that represents all the outputs of this subnode.
 
         Parameters
         ----------
         name: str
             The name of the parameter to search for.
 
-        Returns
+        Returns:
         -------
         Parameter
             The parameter, named based on the given `name`, along with a value that references all outputs.
         """
         return self._get_parameters_as(name=name, subtype=self._subtype)
 
     def get_artifact(self, name: str) -> Artifact:
-        """Gets an artifact from the outputs of this subnode"""
+        """Gets an artifact from the outputs of this subnode."""
         return self._get_artifact(name=name, subtype=self._subtype)
 
     def get_parameter(self, name: str) -> Parameter:
-        """Gets a parameter from the outputs of this subnode"""
+        """Gets a parameter from the outputs of this subnode."""
         return self._get_parameter(name=name, subtype=self._subtype)
 
 
 def _get_params_from_source(source: Callable) -> Optional[List[Parameter]]:
+    """Assembles an optional list of `Parameter` from the given `Callable` arguments.
+
+    Notes:
+    -----
+    If the value of an identified `Callable` keyword argument is found to be empty the value of
+    `hera.shared.serialization.MISSING` is used as a placeholder. This is later serialized as `None` -> `null` when
+    submitted to the Argo server.
+    """
     source_signature: Dict[str, Optional[object]] = {}
     for p in inspect.signature(source).parameters.values():
         if p.default != inspect.Parameter.empty and p.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:
             source_signature[p.name] = p.default
         else:
             source_signature[p.name] = MISSING
 
@@ -921,14 +1100,22 @@
         return None
     elif len(source_signature) == 1:
         return [Parameter(name=n, value="{{item}}") for n in source_signature.keys()]
     return [Parameter(name=n, value=f"{{{{item.{n}}}}}") for n in source_signature.keys()]
 
 
 def _get_params_from_items(with_items: List[Any]) -> Optional[List[Parameter]]:
+    """Returns an optional list of `Parameter` from the specified list of `with_items`.
+
+    The assembled list of `Parameter` contains all the unique parameters identified from the `with_items` list. For
+    example, if the `with_items` list contains 3 serializable elements such as
+    `[{'a': 1, 'b': 2}, {'a': 3, 'b': 4}, {'a': 5, 'b': 6}]`, then only 2 `Parameter`s are returned. Namely, only
+     `Parameter(name='a')` and `Parameter(name='b')` is returned, with values `{{item.a}}` and `{{item.b}}`,
+     respectively. This helps with the parallel/serial processing of the supplied items.
+    """
     if len(with_items) == 0:
         return None
     elif len(with_items) == 1:
         el = with_items[0]
         if len(el.keys()) == 1:
             return [Parameter(name=n, value="{{item}}") for n in el.keys()]
         else:
@@ -1059,15 +1246,16 @@
         """Parse from given yaml_file."""
         raise NotImplementedError
 
 
 class ExperimentalMixin(BaseMixin):
     _experimental_warning_message: str = (
         "Unable to instantiate {} since it is an experimental feature."
-        ' Please turn on experimental features by setting `hera.shared.global_config.experimental_features["{}"] = True`.'
+        " Please turn on experimental features by setting "
+        '`hera.shared.global_config.experimental_features["{}"] = True`.'
         " Note that experimental features are unstable and subject to breaking changes."
     )
 
     _flag: str
 
     @root_validator
     def _check_enabled(cls, values):
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/_unparse.py` & `hera_workflows-5.6.0/src/hera/workflows/_unparse.py`

 * *Files 0% similar despite different names*

```diff
@@ -49,27 +49,30 @@
 _MULTI_QUOTES = ('"""', "'''")
 _ALL_QUOTES = (*_SINGLE_QUOTES, *_MULTI_QUOTES)
 
 
 class _Unparser(ast.NodeVisitor):
     """Methods in this class recursively traverse an AST and
     output source code for the abstract syntax; original formatting
-    is disregarded."""
+    is disregarded.
+    """
 
     def __init__(self, *, _avoid_backslashes=False):
         self._source = []
         self._buffer = []
         self._precedences = {}
         self._type_ignores = {}
         self._indent = 0
         self._avoid_backslashes = _avoid_backslashes
 
     def __call__(self, node):
         """Outputs a source code string that, if converted back to an ast
-        (using ast.parse) will generate an AST equivalent to *node*"""
+        (using ast.parse) will generate an AST equivalent to *node*
+        .
+        """
         self._source = []
         self.traverse(node)
         return "".join(self._source)
 
     def interleave(self, inter, f, seq):
         """Call f on each item in seq, calling inter() in between."""
         seq = iter(seq)
@@ -81,34 +84,37 @@
             for x in seq:
                 inter()
                 f(x)
 
     def items_view(self, traverser, items):
         """Traverse and separate the given *items* with a comma and append it to
         the buffer. If *items* is a single item sequence, a trailing comma
-        will be added."""
+        will be added.
+        """
         if len(items) == 1:
             traverser(items[0])
             self.write(",")
         else:
             self.interleave(lambda: self.write(", "), traverser, items)
 
     def maybe_newline(self):
-        """Adds a newline if it isn't the start of generated source"""
+        """Adds a newline if it isn't the start of generated source."""
         if self._source:
             self.write("\n")
 
     def fill(self, text=""):
         """Indent a piece of text and append it, according to the current
-        indentation level"""
+        indentation level
+        .
+        """
         self.maybe_newline()
         self.write("    " * self._indent + text)
 
     def write(self, text):
-        """Append a piece of text"""
+        """Append a piece of text."""
         self._source.append(text)
 
     def buffer_writer(self, text):
         self._buffer.append(text)
 
     @property
     def buffer(self):
@@ -129,42 +135,43 @@
         self._indent += 1
         yield
         self._indent -= 1
 
     @contextmanager
     def delimit(self, start, end):
         """A context manager for preparing the source for expressions. It adds
-        *start* to the buffer and enters, after exit it adds *end*."""
-
+        *start* to the buffer and enters, after exit it adds *end*.
+        """
         self.write(start)
         yield
         self.write(end)
 
     def delimit_if(self, start, end, condition):
         if condition:
             return self.delimit(start, end)
         else:
             return nullcontext()
 
     def require_parens(self, precedence, node):
-        """Shortcut to adding precedence related parens"""
+        """Shortcut to adding precedence related parens."""
         return self.delimit_if("(", ")", self.get_precedence(node) > precedence)
 
     def get_precedence(self, node):
         return self._precedences.get(node, _Precedence.TEST)
 
     def set_precedence(self, precedence, *nodes):
         for node in nodes:
             self._precedences[node] = precedence
 
     def get_raw_docstring(self, node):
         """If a docstring node is found in the body of the *node* parameter,
         return that docstring node, None otherwise.
 
-        Logic mirrored from ``_PyAST_GetDocString``."""
+        Logic mirrored from ``_PyAST_GetDocString``.
+        """
         if not isinstance(node, (AsyncFunctionDef, FunctionDef, ClassDef, Module)) or len(node.body) < 1:
             return None
         node = node.body[0]
         if not isinstance(node, Expr):
             return None
         node = node.value
         if isinstance(node, Constant) and isinstance(node.value, str):
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/artifact.py` & `hera_workflows-5.6.0/src/hera/workflows/artifact.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,12 +1,10 @@
-"""The artifact module provides the base Artifact class, along with the various
-types of artifacts as subclasses.
+"""The artifact module provides the base Artifact class, along with the various types of artifacts as subclasses.
 
-See https://argoproj.github.io/argo-workflows/walk-through/artifacts/
-for a tutorial on Artifacts.
+See https://argoproj.github.io/argo-workflows/walk-through/artifacts/ for a tutorial on Artifacts.
 """
 from typing import List, Optional, Union, cast
 
 from hera.shared._base_model import BaseModel
 from hera.workflows.archive import ArchiveStrategy
 from hera.workflows.models import (
     ArchiveStrategy as _ModelArchiveStrategy,
@@ -23,26 +21,51 @@
     RawArtifact as _ModelRawArtifact,
     S3Artifact as _ModelS3Artifact,
     SecretKeySelector,
 )
 
 
 class Artifact(BaseModel):
+    """Base artifact representation."""
+
     name: str
+    """name of the artifact"""
+
     archive: Optional[Union[_ModelArchiveStrategy, ArchiveStrategy]] = None
+    """artifact archiving configuration"""
+
     archive_logs: Optional[bool] = None
+    """whether to log the archive object"""
+
     artifact_gc: Optional[ArtifactGC] = None
+    """artifact garbage collection configuration"""
+
     deleted: Optional[bool] = None
+    """whether the artifact is deleted"""
+
     from_: Optional[str] = None
+    """configures the artifact task/step origin"""
+
     from_expression: Optional[str] = None
+    """an expression that dictates where to obtain the artifact from"""
+
     global_name: Optional[str] = None
+    """global workflow artifact name"""
+
     mode: Optional[int] = None
+    """mode bits to use on the artifact, must be a value between 0 and 0777 set when loading input artifacts."""
+
     path: Optional[str] = None
+    """path where the artifact should be placed/loaded from"""
+
     recurse_mode: Optional[str] = None
+    """recursion mode when applying the permissions of the artifact if it is an artifact folder"""
+
     sub_path: Optional[str] = None
+    """allows the specification of an artifact from a subpath within the main source."""
 
     def _build_archive(self) -> Optional[_ModelArchiveStrategy]:
         if self.archive is None:
             return None
 
         if isinstance(self.archive, _ModelArchiveStrategy):
             return self.archive
@@ -65,53 +88,62 @@
         )
 
     def _build_artifact_paths(self) -> _ModelArtifactPaths:
         artifact = self._build_artifact()
         return _ModelArtifactPaths(**artifact.dict())
 
     def as_name(self, name: str) -> _ModelArtifact:
+        """Returns a copy of the current artifact but named based on the specified `name`."""
         artifact = self._build_artifact()
         artifact.name = name
         return artifact
 
 
 class ArtifactoryArtifact(_ModelArtifactoryArtifact, Artifact):
+    """An artifact sourced from Artifactory."""
+
     def _build_artifact(self) -> _ModelArtifact:
         artifact = super()._build_artifact()
         artifact.artifactory = _ModelArtifactoryArtifact(
             url=self.url, password_secret=self.password_secret, username_secret=self.username_secret
         )
         return artifact
 
 
 class AzureArtifact(_ModelAzureArtifact, Artifact):
+    """An artifact sourced from Microsoft Azure."""
+
     def _build_artifact(self) -> _ModelArtifact:
         artifact = super()._build_artifact()
         artifact.azure = _ModelAzureArtifact(
             account_key_secret=self.account_key_secret,
             blob=self.blob,
             container=self.container,
             endpoint=self.endpoint,
             use_sdk_creds=self.use_sdk_creds,
         )
         return artifact
 
 
 class GCSArtifact(_ModelGCSArtifact, Artifact):
+    """An artifact sourced from Google Cloud Storage."""
+
     def _build_artifact(self) -> _ModelArtifact:
         artifact = super()._build_artifact()
         artifact.gcs = _ModelGCSArtifact(
             bucket=self.bucket,
             key=self.key,
             service_account_key_secret=self.service_account_key_secret,
         )
         return artifact
 
 
 class GitArtifact(_ModelGitArtifact, Artifact):
+    """An artifact sourced from GitHub."""
+
     def _build_artifact(self) -> _ModelArtifact:
         artifact = super()._build_artifact()
         artifact.git = _ModelGitArtifact(
             branch=self.branch,
             depth=self.depth,
             disable_submodules=self.disable_submodules,
             fetch=self.fetch,
@@ -123,17 +155,21 @@
             ssh_private_key_secret=self.ssh_private_key_secret,
             username_secret=self.username_secret,
         )
         return artifact
 
 
 class HDFSArtifact(Artifact):
-    # note that `HDFSArtifact` does not inherit from the auto-generated `HDFSArtifact` because there's a
-    # conflict in `path` with the base class `Artifact`. Here, we redefine the HDFS `path` to `hdfs_path` to
-    # differentiate between the parent class and the child class `path`
+    """A Hadoop File System artifact.
+
+    Note that `HDFSArtifact` does not inherit from the auto-generated `HDFSArtifact` because there's a
+    conflict in `path` with the base class `Artifact`. Here, we redefine the HDFS `path` to `hdfs_path` to
+    differentiate between the parent class and the child class `path`.
+    """
+
     hdfs_path: str
     addresses: Optional[List[str]] = None
     force: Optional[bool] = None
     hdfs_user: Optional[str]
     krb_c_cache_secret: Optional[SecretKeySelector] = None
     krb_config_config_map: Optional[SecretKeySelector] = None
     krb_keytab_secret: Optional[SecretKeySelector] = None
@@ -155,25 +191,29 @@
             krb_username=self.krb_username,
             path=self.hdfs_path,
         )
         return artifact
 
 
 class HTTPArtifact(_ModelHTTPArtifact, Artifact):
+    """An artifact sourced from an HTTP URL."""
+
     def _build_artifact(self) -> _ModelArtifact:
         artifact = super()._build_artifact()
         artifact.http = _ModelHTTPArtifact(
             auth=self.auth,
             headers=self.headers,
             url=self.url,
         )
         return artifact
 
 
 class OSSArtifact(_ModelOSSArtifact, Artifact):
+    """An artifact sourced from OSS."""
+
     def _build_artifact(self) -> _ModelArtifact:
         artifact = super()._build_artifact()
         artifact.oss = _ModelOSSArtifact(
             access_key_secret=self.access_key_secret,
             bucket=self.bucket,
             create_bucket_if_not_present=self.create_bucket_if_not_present,
             endpoint=self.endpoint,
@@ -182,21 +222,25 @@
             secret_key_secret=self.secret_key_secret,
             security_token=self.security_token,
         )
         return artifact
 
 
 class RawArtifact(_ModelRawArtifact, Artifact):
+    """A raw bytes artifact representation."""
+
     def _build_artifact(self) -> _ModelArtifact:
         artifact = super()._build_artifact()
         artifact.raw = _ModelRawArtifact(data=self.data)
         return artifact
 
 
 class S3Artifact(_ModelS3Artifact, Artifact):
+    """An artifact sourced from AWS S3."""
+
     def _build_artifact(self) -> _ModelArtifact:
         artifact = super()._build_artifact()
         artifact.s3 = _ModelS3Artifact(
             access_key_secret=self.access_key_secret,
             bucket=self.bucket,
             create_bucket_if_not_present=self.create_bucket_if_not_present,
             encryption_options=self.encryption_options,
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/cluster_workflow_template.py` & `hera_workflows-5.6.0/src/hera/workflows/cluster_workflow_template.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,23 +1,25 @@
+"""Module that provides Hera objects for cluster workflow templates."""
 from pydantic import validator
 
 from hera.exceptions import NotFound
 from hera.workflows.models import (
     ClusterWorkflowTemplate as _ModelClusterWorkflowTemplate,
     ClusterWorkflowTemplateCreateRequest,
     ClusterWorkflowTemplateLintRequest,
     ClusterWorkflowTemplateUpdateRequest,
 )
 from hera.workflows.protocol import TWorkflow
 from hera.workflows.workflow_template import WorkflowTemplate
 
 
 class ClusterWorkflowTemplate(WorkflowTemplate):
-    """ClusterWorkflowTemplates are WorkflowTemplates that are cluster scoped, meaning they are accessible from all
-    namespaces.
+    """ClusterWorkflowTemplates are cluster scoped templates.
+
+    Since cluster workflow templates are scoped at the cluster level, they are available globally in the cluster.
     """
 
     @validator("namespace", pre=True, always=True)
     def _set_namespace(cls, v):
         if v is not None:
             raise ValueError("namespace is not a valid field on a ClusterWorkflowTemplate")
 
@@ -25,23 +27,23 @@
         """Creates the ClusterWorkflowTemplate on the Argo cluster."""
         assert self.workflows_service, "workflow service not initialized"
         return self.workflows_service.create_cluster_workflow_template(
             ClusterWorkflowTemplateCreateRequest(template=self.build())
         )
 
     def get(self) -> TWorkflow:
-        """Attempts to get a workflow template based on the parameters of this template e.g. name + namespace"""
+        """Attempts to get a workflow template based on the parameters of this template e.g. name + namespace."""
         assert self.workflows_service, "workflow service not initialized"
         assert self.name, "workflow name not defined"
         return self.workflows_service.get_cluster_workflow_template(name=self.name)
 
     def update(self) -> TWorkflow:
-        """
-        Attempts to perform a workflow template update based on the parameters of this template
-        e.g. name, namespace. Note that this creates the template if it does not exist. In addition, this performs
+        """Attempts to perform a workflow template update based on the parameters of this template.
+
+        Note that this creates the template if it does not exist. In addition, this performs
         a get prior to updating to get the resource version to update in the first place. If you know the template
         does not exist ahead of time, it is more efficient to use `create()` directly to avoid one round trip.
         """
         assert self.workflows_service, "workflow service not initialized"
         assert self.name, "workflow name not defined"
         # we always need to do a get prior to updating to get the resource version to update in the first place
         # https://github.com/argoproj/argo-workflows/pull/5465#discussion_r597797052
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/container.py` & `hera_workflows-5.6.0/src/hera/workflows/container.py`

 * *Files 16% similar despite different names*

```diff
@@ -27,23 +27,29 @@
     EnvIOMixin,
     ContainerMixin,
     TemplateMixin,
     ResourceMixin,
     VolumeMountMixin,
     CallableTemplateMixin,
 ):
-    """The Container template type defines a container to run on Argo."""
+    """The Container template type defines a container to run on Argo.
+
+    The container generally consists of running a Docker container remotely, which is configured via fields such as
+    `command` (the command to run to start the container), `args` (arguments for the container), `working_dir` (for
+    setting the active working directory relative to container execution), etc.
+    """
 
     args: Optional[List[str]] = None
     command: Optional[List[str]] = None
     lifecycle: Optional[Lifecycle] = None
     security_context: Optional[SecurityContext] = None
     working_dir: Optional[str] = None
 
     def _build_container(self) -> _ModelContainer:
+        """Builds the generated `Container` representation."""
         return _ModelContainer(
             args=self.args,
             command=self.command,
             env=self._build_env(),
             env_from=self._build_env_from(),
             image=self.image,
             image_pull_policy=self._build_image_pull_policy(),
@@ -61,14 +67,15 @@
             tty=self.tty,
             volume_devices=self.volume_devices,
             volume_mounts=self._build_volume_mounts(),
             working_dir=self.working_dir,
         )
 
     def _build_template(self) -> _ModelTemplate:
+        """Builds the generated `Template` representation of the container."""
         return _ModelTemplate(
             active_deadline_seconds=self.active_deadline_seconds,
             affinity=self.affinity,
             archive_location=self.archive_location,
             automount_service_account_token=self.automount_service_account_token,
             container=self._build_container(),
             daemon=self.daemon,
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/container_set.py` & `hera_workflows-5.6.0/src/hera/workflows/container_set.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+"""Provides Hera implementations of Argo's container node for usage in Workflow tasks/steps."""
 from __future__ import annotations
 
 from typing import Any, List, Optional, Union
 
 from hera.workflows._mixins import (
     CallableTemplateMixin,
     ContainerMixin,
@@ -23,15 +24,15 @@
     Template as _ModelTemplate,
 )
 
 
 class ContainerNode(ContainerMixin, VolumeMountMixin, ResourceMixin, EnvMixin, SubNodeMixin):
     """A regular container that can be used as part of a `hera.workflows.ContainerSet`.
 
-    See Also
+    See Also:
     --------
     https://argoproj.github.io/argo-workflows/container-set-template/
     """
 
     name: str
     args: Optional[List[str]] = None
     command: Optional[List[str]] = None
@@ -39,76 +40,71 @@
     lifecycle: Optional[Lifecycle] = None
     security_context: Optional[SecurityContext] = None
     working_dir: Optional[str] = None
 
     def next(self, other: ContainerNode) -> ContainerNode:
         """Sets the given container as a dependency of this container and returns the given container.
 
-        Examples
-        --------
-        >>> from hera.workflows import ContainerNode
-        >>> # normally, you use the following within a `hera.workflows.ContainerSet` context.
-        >>> a, b = ContainerNode(name="a"), ContainerNode(name="b")
-        >>> a.next(b)
-        >>> b.dependencies
-        ['a']
+        Examples:
+            from hera.workflows import ContainerNode
+            # normally, you use the following within a `hera.workflows.ContainerSet` context.
+            a, b = ContainerNode(name="a"), ContainerNode(name="b")
+            a.next(b)
+            b.dependencies  # prints ['a']
         """
         assert issubclass(other.__class__, ContainerNode)
         if other.dependencies is None:
             other.dependencies = [self.name]
         else:
             other.dependencies.append(self.name)
         other.dependencies = sorted(list(set(other.dependencies)))
         return other
 
     def __rrshift__(self, other: List[ContainerNode]) -> ContainerNode:
         """Sets `self` as a dependent of the given list of other `hera.workflows.ContainerNode`.
 
         Practically, the `__rrshift__` allows us to express statements such as `[a, b, c] >> d`, where `d` is `self.`
 
-        Examples
-        --------
-        >>> from hera.workflows import ContainerNode
-        >>> # normally, you use the following within a `hera.workflows.ContainerSet` context.
-        >>> a, b, c = ContainerNode(name="a"), ContainerNode(name="b"), ContainerNode(name="c")
-        >>> [a, b]
-        >>> c.dependencies
-        ['a', 'b']
+        Examples:
+            from hera.workflows import ContainerNode
+            # normally, you use the following within a `hera.workflows.ContainerSet` context.
+            a, b, c = ContainerNode(name="a"), ContainerNode(name="b"), ContainerNode(name="c")
+            [a, b] >> c
+            c.dependencies  # prints ['a', 'b']
         """
         assert isinstance(other, list), f"Unknown type {type(other)} specified using reverse right bitshift operator"
         for o in other:
             o.next(self)
         return self
 
     def __rshift__(
         self, other: Union[ContainerNode, List[ContainerNode]]
     ) -> Union[ContainerNode, List[ContainerNode]]:
         """Sets the given container as a dependency of this container and returns the given container.
 
-        Examples
-        --------
-        >>> from hera.workflows import ContainerNode
-        >>> # normally, you use the following within a `hera.workflows.ContainerSet` context.
-        >>> a, b = ContainerNode(name="a"), ContainerNode(name="b")
-        >>> a >> b
-        >>> b.dependencies
-        ['a']
+        Examples:
+            from hera.workflows import ContainerNode
+            # normally, you use the following within a `hera.workflows.ContainerSet` context.
+            a, b = ContainerNode(name="a"), ContainerNode(name="b")
+            a >> b
+            b.dependencies  # prints ['a']
         """
         if isinstance(other, ContainerNode):
             return self.next(other)
         elif isinstance(other, list):
             for o in other:
                 assert isinstance(
                     o, ContainerNode
                 ), f"Unknown list item type {type(o)} specified using right bitshift operator `>>`"
                 self.next(o)
             return other
         raise ValueError(f"Unknown type {type(other)} provided to `__rshift__`")
 
     def _build_container_node(self) -> _ModelContainerNode:
+        """Builds the generated `ContainerNode`."""
         return _ModelContainerNode(
             args=self.args,
             command=self.command,
             dependencies=self.dependencies,
             env=self._build_env(),
             env_from=self._build_env_from(),
             image=self.image,
@@ -137,32 +133,45 @@
     ContainerMixin,
     TemplateMixin,
     CallableTemplateMixin,
     ResourceMixin,
     VolumeMountMixin,
     ContextMixin,
 ):
+    """`ContainerSet` is the implementation of a set of containers that can be run in parallel on Kubernetes.
+
+    The containers are run within the same pod.
+
+    Examples:
+    --------
+    >>> with ContainerSet(...) as cs:
+    >>>     ContainerNode(...)
+    >>>     ContainerNode(...)
+    """
+
     containers: List[Union[ContainerNode, _ModelContainerNode]] = []
     container_set_retry_strategy: Optional[ContainerSetRetryStrategy] = None
 
     def _add_sub(self, node: Any):
         if not isinstance(node, ContainerNode):
             raise InvalidType(type(node))
 
         self.containers.append(node)
 
     def _build_container_set(self) -> _ModelContainerSetTemplate:
+        """Builds the generated `ContainerSetTemplate`."""
         containers = [c._build_container_node() if isinstance(c, ContainerNode) else c for c in self.containers]
         return _ModelContainerSetTemplate(
             containers=containers,
             retry_strategy=self.container_set_retry_strategy,
             volume_mounts=self.volume_mounts,
         )
 
     def _build_template(self) -> _ModelTemplate:
+        """Builds the generated `Template` representation of the container set."""
         return _ModelTemplate(
             active_deadline_seconds=self.active_deadline_seconds,
             affinity=self.affinity,
             archive_location=self.archive_location,
             automount_service_account_token=self.automount_service_account_token,
             container_set=self._build_container_set(),
             daemon=self.daemon,
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/cron_workflow.py` & `hera_workflows-5.6.0/src/hera/workflows/cron_workflow.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-"""The cron_workflow module provides the CronWorkflow class
+"""The cron_workflow module provides the CronWorkflow class.
 
 See https://argoproj.github.io/argo-workflows/cron-workflows
 for more on CronWorkflows.
 """
 from pathlib import Path
 from typing import Dict, Optional, Type, Union
 
@@ -90,24 +90,24 @@
         assert self.workflows_service, "workflow service not initialized"
         assert self.namespace, "workflow namespace not defined"
         return self.workflows_service.create_cron_workflow(
             CreateCronWorkflowRequest(cron_workflow=self.build()), namespace=self.namespace
         )
 
     def get(self) -> TWorkflow:
-        """Attempts to get a cron workflow based on the parameters of this template e.g. name + namespace"""
+        """Attempts to get a cron workflow based on the parameters of this template e.g. name + namespace."""
         assert self.workflows_service, "workflow service not initialized"
         assert self.namespace, "workflow namespace not defined"
         assert self.name, "workflow name not defined"
         return self.workflows_service.get_cron_workflow(name=self.name, namespace=self.namespace)
 
     def update(self) -> TWorkflow:
-        """
-        Attempts to perform a workflow template update based on the parameters of this template
-        e.g. name, namespace. Note that this creates the template if it does not exist. In addition, this performs
+        """Attempts to perform a workflow template update based on the parameters of this template.
+
+        Note that this creates the template if it does not exist. In addition, this performs
         a get prior to updating to get the resource version to update in the first place. If you know the template
         does not exist ahead of time, it is more efficient to use `create()` directly to avoid one round trip.
         """
         assert self.workflows_service, "workflow service not initialized"
         assert self.namespace, "workflow namespace not defined"
         assert self.name, "workflow name not defined"
         # we always need to do a get prior to updating to get the resource version to update in the first place
@@ -147,15 +147,14 @@
         )
 
         return _CronWorkflowModelMapper.build_model(CronWorkflow, self, model_cron_workflow)
 
     @classmethod
     def _from_model(cls, model: BaseModel) -> ModelMapperMixin:
         """Parse from given model to cls's type."""
-
         assert isinstance(model, _ModelCronWorkflow)
         hera_cron_workflow = CronWorkflow(schedule="")
 
         for attr, annotation in cls._get_all_annotations().items():
             if get_origin(annotation) is Annotated and isinstance(
                 get_args(annotation)[1], ModelMapperMixin.ModelMapper
             ):
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/dag.py` & `hera_workflows-5.6.0/src/hera/workflows/dag.py`

 * *Files 23% similar despite different names*

```diff
@@ -13,31 +13,46 @@
     DAGTask,
     DAGTemplate as _ModelDAGTemplate,
     Template as _ModelTemplate,
 )
 from hera.workflows.task import Task
 
 
-class DAG(IOMixin, TemplateMixin, CallableTemplateMixin, ContextMixin):
+class DAG(
+    IOMixin,
+    TemplateMixin,
+    CallableTemplateMixin,
+    ContextMixin,
+):
     """A DAG template invocator is used to define Task dependencies as an acyclic graph.
 
     DAG implements the contextmanager interface so allows usage of `with`, under which any
     `hera.workflows.task.Task` objects instantiated will be added to the DAG's list of Tasks.
+
+    Examples:
+    --------
+    >>> @script()
+    >>> def foo() -> None:
+    >>>     print(42)
+    >>>
+    >>> with DAG(...) as dag:
+    >>>     foo()
     """
 
     fail_fast: Optional[bool] = None
     target: Optional[str] = None
     tasks: List[Union[Task, DAGTask]] = []
 
     def _add_sub(self, node: Any):
         if not isinstance(node, Task):
             raise InvalidType(type(node))
         self.tasks.append(node)
 
     def _build_template(self) -> _ModelTemplate:
+        """Builds the auto-generated `Template` representation of the `DAG`."""
         tasks = []
         for task in self.tasks:
             if isinstance(task, Task):
                 tasks.append(task._build_dag_task())
             else:
                 tasks.append(task)
         return _ModelTemplate(
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/data.py` & `hera_workflows-5.6.0/src/hera/workflows/data.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,33 +1,44 @@
+"""Data module provides Hera objects for Argo data functionality, such as sourcing data + applying transformations."""
 from typing import List, Union
 
 from hera.expr._node import Node
 from hera.workflows import models as m
 from hera.workflows._mixins import CallableTemplateMixin, IOMixin, TemplateMixin
 from hera.workflows.artifact import Artifact
 
 
 class Data(TemplateMixin, IOMixin, CallableTemplateMixin):
+    """`Data` implements the Argo data template representation.
+
+    Data can be used to indicate that some data, identified by a `source`, should be processed via the specified
+    `transformations`. The `transformations` field can be either expressed via a pure `str` or via a `hera.expr`,
+    which transpiles the expression into a statement that can be processed by Argo.
+    """
+
     source: Union[m.DataSource, m.ArtifactPaths, Artifact]
     transformations: List[Union[str, Node]] = []
 
     def _build_source(self) -> m.DataSource:
+        """Builds the generated `DataSource`."""
         if isinstance(self.source, m.DataSource):
             return self.source
         elif isinstance(self.source, m.ArtifactPaths):
             return m.DataSource(artifact_paths=self.source)
         return m.DataSource(artifact_paths=self.source._build_artifact_paths())
 
     def _build_data(self) -> m.Data:
+        """Builds the generated `Data` template."""
         return m.Data(
             source=self._build_source(),
             transformation=list(map(lambda expr: m.TransformationStep(expression=str(expr)), self.transformations)),
         )
 
     def _build_template(self) -> m.Template:
+        """Builds the generated `Template` from the fields of `Data`."""
         return m.Template(
             active_deadline_seconds=self.active_deadline_seconds,
             affinity=self.affinity,
             archive_location=self.archive_location,
             automount_service_account_token=self.automount_service_account_token,
             data=self._build_data(),
             executor=self.executor,
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/http_template.py` & `hera_workflows-5.6.0/src/hera/workflows/http_template.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,41 +1,46 @@
+"""The HTTP template module provides functionality and objects required for executing HTTP calls in workflows."""
 from typing import List, Optional
 
 from hera.workflows._mixins import CallableTemplateMixin, IOMixin, TemplateMixin
 from hera.workflows.models import (
     HTTP as _ModelHTTP,
     HTTPBodySource,
     Template as _ModelTemplate,
     V1HTTPHeader as HTTPHeader,
 )
 
 
 class HTTP(TemplateMixin, IOMixin, CallableTemplateMixin):
+    """`HTTP` is an implementation of the HTTP template that supports executing HTTP actions in a step/task."""
+
     url: str
     body: Optional[str] = None
     body_from: Optional[HTTPBodySource] = None
     headers: Optional[List[HTTPHeader]] = None
     insecure_skip_verify: Optional[bool] = None
     method: Optional[str] = None
     success_condition: Optional[str] = None
     timeout_seconds: Optional[int] = None
 
     def _build_http_template(self) -> _ModelHTTP:
+        """Builds the generated HTTP sub-template."""
         return _ModelHTTP(
             url=self.url,
             body=self.body,
             body_from=self.body_from,
             headers=self.headers,
             insecure_skip_verify=self.insecure_skip_verify,
             method=self.method,
             success_condition=self.success_condition,
             timeout_seconds=self.timeout_seconds,
         )
 
     def _build_template(self) -> _ModelTemplate:
+        """Builds the HTTP generated `Template`."""
         return _ModelTemplate(
             active_deadline_seconds=self.active_deadline_seconds,
             affinity=self.affinity,
             archive_location=self.archive_location,
             automount_service_account_token=self.automount_service_account_token,
             executor=self.executor,
             fail_fast=self.fail_fast,
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/metrics.py` & `hera_workflows-5.6.0/src/hera/workflows/metrics.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+"""The metrics module implements independent and collections of Prometheus metrics that can be used in Argo."""
 from typing import List, Optional, Union
 
 from hera.shared import BaseMixin
 from hera.workflows.models import (
     Amount as _ModelAmount,
     Counter as _ModelCounter,
     Gauge as _ModelGauge,
@@ -10,15 +11,15 @@
     Prometheus as _ModelPrometheus,
 )
 
 Label = _ModelMetricLabel
 
 
 class _BaseMetric(BaseMixin):
-    """Base metric wrapper around `hera.workflows.models.Prometheus`"""
+    """Base metric wrapper around `hera.workflows.models.Prometheus`."""
 
     name: str
     help: str
     labels: Optional[Union[Label, List[Label]]] = None
     when: Optional[str] = None
 
     def _build_labels(self) -> Optional[List[_ModelMetricLabel]]:
@@ -31,15 +32,15 @@
     def _build_metric(self) -> _ModelPrometheus:
         raise NotImplementedError
 
 
 class Counter(_BaseMetric):
     """Counter metric component used to count specific events based on the given value.
 
-    Notes
+    Notes:
     -----
     See: https://argoproj.github.io/argo-workflows/metrics/#grafana-dashboard-for-argo-controller-metrics
     """
 
     value: str
 
     def _build_metric(self) -> _ModelPrometheus:
@@ -53,15 +54,15 @@
             when=self.when,
         )
 
 
 class Gauge(_BaseMetric, _ModelGauge):
     """Gauge metric component used to record intervals based on the given value.
 
-    Notes
+    Notes:
     -----
     See: https://argoproj.github.io/argo-workflows/metrics/#grafana-dashboard-for-argo-controller-metrics
     """
 
     realtime: bool
     value: str
 
@@ -76,15 +77,15 @@
             when=self.when,
         )
 
 
 class Histogram(_BaseMetric):
     """Histogram metric that records the value at the specified bucket intervals.
 
-    Notes
+    Notes:
     -----
     See: https://argoproj.github.io/argo-workflows/metrics/#grafana-dashboard-for-argo-controller-metrics
     """
 
     buckets: List[Union[float, _ModelAmount]]  # type: ignore
     value: str
 
@@ -105,15 +106,15 @@
             when=self.when,
         )
 
 
 class Metric(_BaseMetric):
     """Prometheus metric that can be used at the workflow or task/template level.
 
-    Notes
+    Notes:
     -----
     See: https://argoproj.github.io/argo-workflows/metrics/#grafana-dashboard-for-argo-controller-metrics
     """
 
     counter: Optional[Counter] = None
     gauge: Optional[Gauge] = None
     histogram: Optional[Histogram] = None
@@ -134,15 +135,15 @@
             when=self.when,
         )
 
 
 class Metrics(BaseMixin):
     """A collection of Prometheus metrics.
 
-    Notes
+    Notes:
     -----
     See: https://argoproj.github.io/argo-workflows/metrics/#grafana-dashboard-for-argo-controller-metrics
     """
 
     metrics: List[Metric]
 
     def _build_metrics(self) -> List[_ModelPrometheus]:
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/eventsource.py` & `hera_workflows-5.6.0/src/hera/workflows/models/eventsource.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,45 +1,46 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from .io.argoproj.events import v1alpha1
 from .io.k8s.apimachinery.pkg.apis.meta import v1
 
 
 class EventSourceDeletedResponse(BaseModel):
     pass
 
 
 class LogEntry(BaseModel):
-    event_name: Optional[str] = Field(None, alias="eventName", title="optional - the event name (e.g. `example`)")
-    event_source_name: Optional[str] = Field(None, alias="eventSourceName")
+    event_name: Optional[str] = Field(
+        default=None, alias="eventName", title="optional - the event name (e.g. `example`)"
+    )
+    event_source_name: Optional[str] = Field(default=None, alias="eventSourceName")
     event_source_type: Optional[str] = Field(
-        None, alias="eventSourceType", title="optional - the event source type (e.g. `webhook`)"
+        default=None, alias="eventSourceType", title="optional - the event source type (e.g. `webhook`)"
     )
     level: Optional[str] = None
     msg: Optional[str] = None
     namespace: Optional[str] = None
     time: Optional[v1.Time] = None
 
 
 class CreateEventSourceRequest(BaseModel):
-    event_source: Optional[v1alpha1.EventSource] = Field(None, alias="eventSource")
+    event_source: Optional[v1alpha1.EventSource] = Field(default=None, alias="eventSource")
     namespace: Optional[str] = None
 
 
 class EventSourceWatchEvent(BaseModel):
     object: Optional[v1alpha1.EventSource] = None
     type: Optional[str] = None
 
 
 class UpdateEventSourceRequest(BaseModel):
-    event_source: Optional[v1alpha1.EventSource] = Field(None, alias="eventSource")
+    event_source: Optional[v1alpha1.EventSource] = Field(default=None, alias="eventSource")
     name: Optional[str] = None
     namespace: Optional[str] = None
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/eventsource.pyi` & `hera_workflows-5.6.0/src/hera/workflows/models/eventsource.pyi`

 * *Files 1% similar despite different names*

```diff
@@ -1,13 +1,11 @@
-from typing import Optional
-
-from hera.shared._base_model import BaseModel as BaseModel
-
 from .io.argoproj.events import v1alpha1 as v1alpha1
 from .io.k8s.apimachinery.pkg.apis.meta import v1 as v1
+from hera.shared._base_model import BaseModel as BaseModel
+from typing import Optional
 
 class EventSourceDeletedResponse(BaseModel): ...
 
 class LogEntry(BaseModel):
     event_name: Optional[str]
     event_source_name: Optional[str]
     event_source_type: Optional[str]
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/grpc/gateway/runtime.py` & `hera_workflows-5.6.0/src/hera/workflows/models/grpc/gateway/runtime.py`

 * *Files identical despite different names*

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/events/v1alpha1.py` & `hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/events/v1alpha1.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,279 +1,295 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from typing import Dict, List, Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from ...k8s.api.core import v1
 from ...k8s.apimachinery.pkg.apis.meta import v1 as v1_1
 
 
 class AMQPConsumeConfig(BaseModel):
     auto_ack: Optional[bool] = Field(
-        None,
+        default=None,
         alias="autoAck",
         title=(
             "AutoAck when true, the server will acknowledge deliveries to this consumer prior to writing\nthe delivery"
             " to the network\n+optional"
         ),
     )
     consumer_tag: Optional[str] = Field(
-        None,
+        default=None,
         alias="consumerTag",
         title="ConsumerTag is the identity of the consumer included in every delivery\n+optional",
     )
     exclusive: Optional[bool] = Field(
-        None,
+        default=None,
         title="Exclusive when true, the server will ensure that this is the sole consumer from this queue\n+optional",
     )
     no_local: Optional[bool] = Field(
-        None, alias="noLocal", title="NoLocal flag is not supported by RabbitMQ\n+optional"
+        default=None, alias="noLocal", title="NoLocal flag is not supported by RabbitMQ\n+optional"
     )
     no_wait: Optional[bool] = Field(
-        None,
+        default=None,
         alias="noWait",
         title=(
             "NowWait when true, do not wait for the server to confirm the request and immediately begin"
             " deliveries\n+optional"
         ),
     )
 
 
 class AMQPExchangeDeclareConfig(BaseModel):
     auto_delete: Optional[bool] = Field(
-        None, alias="autoDelete", title="AutoDelete removes the exchange when no bindings are active\n+optional"
+        default=None,
+        alias="autoDelete",
+        title="AutoDelete removes the exchange when no bindings are active\n+optional",
     )
-    durable: Optional[bool] = Field(None, title="Durable keeps the exchange also after the server restarts\n+optional")
-    internal: Optional[bool] = Field(None, title="Internal when true does not accept publishings\n+optional")
+    durable: Optional[bool] = Field(
+        default=None, title="Durable keeps the exchange also after the server restarts\n+optional"
+    )
+    internal: Optional[bool] = Field(default=None, title="Internal when true does not accept publishings\n+optional")
     no_wait: Optional[bool] = Field(
-        None, alias="noWait", title="NowWait when true does not wait for a confirmation from the server\n+optional"
+        default=None,
+        alias="noWait",
+        title="NowWait when true does not wait for a confirmation from the server\n+optional",
     )
 
 
 class AMQPQueueBindConfig(BaseModel):
     no_wait: Optional[bool] = Field(
-        None,
+        default=None,
         alias="noWait",
         title="NowWait false and the queue could not be bound, the channel will be closed with an error\n+optional",
     )
 
 
 class AMQPQueueDeclareConfig(BaseModel):
     arguments: Optional[str] = Field(
-        None,
+        default=None,
         title='Arguments of a queue (also known as "x-arguments") used for optional features and plugins\n+optional',
     )
     auto_delete: Optional[bool] = Field(
-        None, alias="autoDelete", title="AutoDelete removes the queue when no consumers are active\n+optional"
+        default=None, alias="autoDelete", title="AutoDelete removes the queue when no consumers are active\n+optional"
+    )
+    durable: Optional[bool] = Field(
+        default=None, title="Durable keeps the queue also after the server restarts\n+optional"
     )
-    durable: Optional[bool] = Field(None, title="Durable keeps the queue also after the server restarts\n+optional")
     exclusive: Optional[bool] = Field(
-        None,
+        default=None,
         title=(
             "Exclusive sets the queues to be accessible only by the connection that declares them and will be\ndeleted"
             " wgen the connection closes\n+optional"
         ),
     )
     name: Optional[str] = Field(
-        None, title="Name of the queue. If empty the server auto-generates a unique name for this queue\n+optional"
+        default=None,
+        title="Name of the queue. If empty the server auto-generates a unique name for this queue\n+optional",
     )
     no_wait: Optional[bool] = Field(
-        None, alias="noWait", title="NowWait when true, the queue assumes to be declared on the server\n+optional"
+        default=None,
+        alias="noWait",
+        title="NowWait when true, the queue assumes to be declared on the server\n+optional",
     )
 
 
 class Amount(BaseModel):
     value: Optional[str] = None
 
 
 class BitbucketRepository(BaseModel):
-    owner: Optional[str] = Field(None, title="Owner is the owner of the repository")
+    owner: Optional[str] = Field(default=None, title="Owner is the owner of the repository")
     repository_slug: Optional[str] = Field(
-        None,
+        default=None,
         alias="repositorySlug",
         title=(
             "RepositorySlug is a URL-friendly version of a repository name, automatically generated by Bitbucket for"
             " use in the URL"
         ),
     )
 
 
 class BitbucketServerRepository(BaseModel):
     project_key: Optional[str] = Field(
-        None, alias="projectKey", title="ProjectKey is the key of project for which integration needs to set up"
+        default=None,
+        alias="projectKey",
+        title="ProjectKey is the key of project for which integration needs to set up",
     )
     repository_slug: Optional[str] = Field(
-        None,
+        default=None,
         alias="repositorySlug",
         title="RepositorySlug is the slug of the repository for which integration needs to set up",
     )
 
 
 class CatchupConfiguration(BaseModel):
     enabled: Optional[bool] = Field(
-        None, title="Enabled enables to triggered the missed schedule when eventsource restarts"
+        default=None, title="Enabled enables to triggered the missed schedule when eventsource restarts"
+    )
+    max_duration: Optional[str] = Field(
+        default=None, alias="maxDuration", title="MaxDuration holds max catchup duration"
     )
-    max_duration: Optional[str] = Field(None, alias="maxDuration", title="MaxDuration holds max catchup duration")
 
 
 class ConditionsResetByTime(BaseModel):
     cron: Optional[str] = Field(
-        None, title="Cron is a cron-like expression. For reference, see: https://en.wikipedia.org/wiki/Cron"
+        default=None, title="Cron is a cron-like expression. For reference, see: https://en.wikipedia.org/wiki/Cron"
     )
-    timezone: Optional[str] = Field(None, title="+optional")
+    timezone: Optional[str] = Field(default=None, title="+optional")
 
 
 class ConditionsResetCriteria(BaseModel):
     by_time: Optional[ConditionsResetByTime] = Field(
-        None,
+        default=None,
         alias="byTime",
         title="Schedule is a cron-like expression. For reference, see: https://en.wikipedia.org/wiki/Cron",
     )
 
 
 class ConfigMapPersistence(BaseModel):
     create_if_not_exist: Optional[bool] = Field(
-        None, alias="createIfNotExist", title="CreateIfNotExist will create configmap if it doesn't exists"
+        default=None, alias="createIfNotExist", title="CreateIfNotExist will create configmap if it doesn't exists"
     )
-    name: Optional[str] = Field(None, title="Name of the configmap")
+    name: Optional[str] = Field(default=None, title="Name of the configmap")
 
 
 class DataFilter(BaseModel):
     comparator: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Comparator compares the event data with a user given value.\nCan be ">=", ">", "=", "!=", "<", or'
             ' "<=".\nIs optional, and if left blank treated as equality "=".'
         ),
     )
     path: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Path is the JSONPath of the event's (JSON decoded) data key\nPath is a series of keys separated by a dot."
             " A key may contain wildcard characters '*' and '?'.\nTo access an array value use the index as the key."
             " The dot and wildcard characters can be escaped with '\\\\'.\nSee"
             " https://github.com/tidwall/gjson#path-syntax for more information on how to use this."
         ),
     )
     template: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Template is a go-template for extracting a string from the event's data.\nA Template is evaluated with"
             " provided path, type and value.\nThe templating follows the standard go-template syntax as well as"
             " sprig's extra functions.\nSee https://pkg.go.dev/text/template and https://masterminds.github.io/sprig/"
         ),
     )
-    type: Optional[str] = Field(None, title="Type contains the JSON type of the data")
+    type: Optional[str] = Field(default=None, title="Type contains the JSON type of the data")
     value: Optional[List[str]] = Field(
-        None,
+        default=None,
         title=(
             "Value is the allowed string values for this key\nBooleans are passed using strconv.ParseBool()\nNumbers"
             " are parsed using as float64 using strconv.ParseFloat()\nStrings are taken as is\nNils this value is"
             " ignored"
         ),
     )
 
 
 class EventDependencyTransformer(BaseModel):
-    jq: Optional[str] = Field(None, title="JQ holds the jq command applied for transformation\n+optional")
-    script: Optional[str] = Field(None, title="Script refers to a Lua script used to transform the event\n+optional")
+    jq: Optional[str] = Field(default=None, title="JQ holds the jq command applied for transformation\n+optional")
+    script: Optional[str] = Field(
+        default=None, title="Script refers to a Lua script used to transform the event\n+optional"
+    )
 
 
 class EventPersistence(BaseModel):
     catchup: Optional[CatchupConfiguration] = Field(
-        None, title="Catchup enables to triggered the missed schedule when eventsource restarts"
+        default=None, title="Catchup enables to triggered the missed schedule when eventsource restarts"
     )
     config_map: Optional[ConfigMapPersistence] = Field(
-        None, alias="configMap", title="ConfigMap holds configmap details for persistence"
+        default=None, alias="configMap", title="ConfigMap holds configmap details for persistence"
     )
 
 
 class EventSourceFilter(BaseModel):
     expression: Optional[str] = None
 
 
 class FileArtifact(BaseModel):
     path: Optional[str] = None
 
 
 class GitRemoteConfig(BaseModel):
-    name: Optional[str] = Field(None, description="Name of the remote to fetch from.")
+    name: Optional[str] = Field(default=None, description="Name of the remote to fetch from.")
     urls: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "URLs the URLs of a remote repository. It must be non-empty. Fetch will\nalways use the first URL, while"
             " push will use all of them."
         ),
     )
 
 
 class Int64OrString(BaseModel):
-    int64_val: Optional[str] = Field(None, alias="int64Val")
-    str_val: Optional[str] = Field(None, alias="strVal")
+    int64_val: Optional[str] = Field(default=None, alias="int64Val")
+    str_val: Optional[str] = Field(default=None, alias="strVal")
     type: Optional[str] = None
 
 
 class KafkaConsumerGroup(BaseModel):
-    group_name: Optional[str] = Field(None, alias="groupName", title="The name for the consumer group to use")
+    group_name: Optional[str] = Field(default=None, alias="groupName", title="The name for the consumer group to use")
     oldest: Optional[bool] = Field(
-        None,
+        default=None,
         title=(
             "When starting up a new group do we want to start from the oldest event (true) or the newest event"
             " (false), defaults to false\n+optional"
         ),
     )
     rebalance_strategy: Optional[str] = Field(
-        None,
+        default=None,
         alias="rebalanceStrategy",
         title="Rebalance strategy can be one of: sticky, roundrobin, range. Range is the default.\n+optional",
     )
 
 
 class LogTrigger(BaseModel):
     interval_seconds: Optional[str] = Field(
-        None,
+        default=None,
         alias="intervalSeconds",
         title=(
             "Only print messages every interval. Useful to prevent logging too much data for busy events.\n+optional"
         ),
     )
 
 
 class Metadata(BaseModel):
     annotations: Optional[Dict[str, str]] = None
     labels: Optional[Dict[str, str]] = None
 
 
 class OwnedRepositories(BaseModel):
-    names: Optional[List[str]] = Field(None, title="Repository names")
-    owner: Optional[str] = Field(None, title="Organization or user name")
+    names: Optional[List[str]] = Field(default=None, title="Repository names")
+    owner: Optional[str] = Field(default=None, title="Organization or user name")
 
 
 class PayloadField(BaseModel):
-    name: Optional[str] = Field(None, description="Name acts as key that holds the value at the path.")
+    name: Optional[str] = Field(default=None, description="Name acts as key that holds the value at the path.")
     path: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Path is the JSONPath of the event's (JSON decoded) data key\nPath is a series of keys separated by a dot."
             " A key may contain wildcard characters '*' and '?'.\nTo access an array value use the index as the key."
             " The dot and wildcard characters can be escaped with '\\\\'.\nSee"
             " https://github.com/tidwall/gjson#path-syntax for more information on how to use this."
         ),
     )
 
 
 class RateLimit(BaseModel):
-    requests_per_unit: Optional[int] = Field(None, alias="requestsPerUnit")
-    unit: Optional[str] = Field(None, title="Defaults to Second")
+    requests_per_unit: Optional[int] = Field(default=None, alias="requestsPerUnit")
+    unit: Optional[str] = Field(default=None, title="Defaults to Second")
 
 
 class Resource(BaseModel):
     value: Optional[str] = None
 
 
 class S3Bucket(BaseModel):
@@ -283,2050 +299,2151 @@
 
 class S3Filter(BaseModel):
     prefix: Optional[str] = None
     suffix: Optional[str] = None
 
 
 class Selector(BaseModel):
-    key: Optional[str] = Field(None, title="Key name")
+    key: Optional[str] = Field(default=None, title="Key name")
     operation: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Supported operations like ==, !=, <=, >= etc.\nDefaults to ==.\nRefer"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors for more"
             " io.argoproj.workflow.v1alpha1.\n+optional"
         ),
     )
-    value: Optional[str] = Field(None, title="Value")
+    value: Optional[str] = Field(default=None, title="Value")
 
 
 class StatusPolicy(BaseModel):
     allow: Optional[List[int]] = None
 
 
 class StorageGridFilter(BaseModel):
     prefix: Optional[str] = None
     suffix: Optional[str] = None
 
 
 class TimeFilter(BaseModel):
     start: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Start is the beginning of a time window in UTC.\nBefore this time, events for this dependency are"
             " ignored.\nFormat is hh:mm:ss."
         ),
     )
     stop: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Stop is the end of a time window in UTC.\nAfter or equal to this time, events for this dependency are"
             " ignored and\nFormat is hh:mm:ss.\nIf it is smaller than Start, it is treated as next day of"
             " Start\n(e.g.: 22:00:00-01:00:00 means 22:00:00-25:00:00)."
         ),
     )
 
 
 class TriggerParameterSource(BaseModel):
     context_key: Optional[str] = Field(
-        None,
+        default=None,
         alias="contextKey",
         description=(
             "ContextKey is the JSONPath of the event's (JSON decoded) context key\nContextKey is a series of keys"
             " separated by a dot. A key may contain wildcard characters '*' and '?'.\nTo access an array value use the"
             " index as the key. The dot and wildcard characters can be escaped with '\\\\'.\nSee"
             " https://github.com/tidwall/gjson#path-syntax for more information on how to use this."
         ),
     )
     context_template: Optional[str] = Field(
-        None,
+        default=None,
         alias="contextTemplate",
         title=(
             "ContextTemplate is a go-template for extracting a string from the event's context.\nIf a ContextTemplate"
             " is provided with a ContextKey, the template will be evaluated first and fallback to the ContextKey.\nThe"
             " templating follows the standard go-template syntax as well as sprig's extra functions.\nSee"
             " https://pkg.go.dev/text/template and https://masterminds.github.io/sprig/"
         ),
     )
     data_key: Optional[str] = Field(
-        None,
+        default=None,
         alias="dataKey",
         description=(
             "DataKey is the JSONPath of the event's (JSON decoded) data key\nDataKey is a series of keys separated by"
             " a dot. A key may contain wildcard characters '*' and '?'.\nTo access an array value use the index as the"
             " key. The dot and wildcard characters can be escaped with '\\\\'.\nSee"
             " https://github.com/tidwall/gjson#path-syntax for more information on how to use this."
         ),
     )
     data_template: Optional[str] = Field(
-        None,
+        default=None,
         alias="dataTemplate",
         title=(
             "DataTemplate is a go-template for extracting a string from the event's data.\nIf a DataTemplate is"
             " provided with a DataKey, the template will be evaluated first and fallback to the DataKey.\nThe"
             " templating follows the standard go-template syntax as well as sprig's extra functions.\nSee"
             " https://pkg.go.dev/text/template and https://masterminds.github.io/sprig/"
         ),
     )
     dependency_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="dependencyName",
         description=(
             "DependencyName refers to the name of the dependency. The event which is stored for this dependency is"
             " used as payload\nfor the parameterization. Make sure to refer to one of the dependencies you have"
             " defined under Dependencies list."
         ),
     )
     value: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Value is the default literal value to use for this parameter source\nThis is only used if the DataKey is"
             " invalid.\nIf the DataKey is invalid and this is not defined, this param source will produce an error."
         ),
     )
 
 
 class URLArtifact(BaseModel):
-    path: Optional[str] = Field(None, title="Path is the complete URL")
+    path: Optional[str] = Field(default=None, title="Path is the complete URL")
     verify_cert: Optional[bool] = Field(
-        None, alias="verifyCert", title="VerifyCert decides whether the connection is secure or not"
+        default=None, alias="verifyCert", title="VerifyCert decides whether the connection is secure or not"
     )
 
 
 class WatchPathConfig(BaseModel):
-    directory: Optional[str] = Field(None, title="Directory to watch for events")
-    path: Optional[str] = Field(None, title="Path is relative path of object to watch with respect to the directory")
+    directory: Optional[str] = Field(default=None, title="Directory to watch for events")
+    path: Optional[str] = Field(
+        default=None, title="Path is relative path of object to watch with respect to the directory"
+    )
     path_regexp: Optional[str] = Field(
-        None,
+        default=None,
         alias="pathRegexp",
         title="PathRegexp is regexp of relative path of object to watch with respect to the directory",
     )
 
 
 class AzureEventsHubEventSource(BaseModel):
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     fqdn: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "FQDN of the EventHubs namespace you created\nMore info at"
             " https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-get-connection-string"
         ),
     )
-    hub_name: Optional[str] = Field(None, alias="hubName", title="Event Hub path/name")
+    hub_name: Optional[str] = Field(default=None, alias="hubName", title="Event Hub path/name")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     shared_access_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="sharedAccessKey", title="SharedAccessKey is the generated value of the key"
+        default=None, alias="sharedAccessKey", title="SharedAccessKey is the generated value of the key"
     )
     shared_access_key_name: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="sharedAccessKeyName",
         title="SharedAccessKeyName is the name you chose for your application's SAS keys",
     )
 
 
 class Backoff(BaseModel):
     duration: Optional[Int64OrString] = Field(
-        None, title='The initial duration in nanoseconds or strings like "1s", "3m"\n+optional'
+        default=None, title='The initial duration in nanoseconds or strings like "1s", "3m"\n+optional'
     )
-    factor: Optional[Amount] = Field(None, title="Duration is multiplied by factor each iteration\n+optional")
-    jitter: Optional[Amount] = Field(None, title="The amount of jitter applied each iteration\n+optional")
-    steps: Optional[int] = Field(None, title="Exit with error after this many steps\n+optional")
+    factor: Optional[Amount] = Field(default=None, title="Duration is multiplied by factor each iteration\n+optional")
+    jitter: Optional[Amount] = Field(default=None, title="The amount of jitter applied each iteration\n+optional")
+    steps: Optional[int] = Field(default=None, title="Exit with error after this many steps\n+optional")
 
 
 class BasicAuth(BaseModel):
     password: Optional[v1.SecretKeySelector] = Field(
-        None, description="Password refers to the Kubernetes secret that holds the password required for basic auth."
+        default=None,
+        description="Password refers to the Kubernetes secret that holds the password required for basic auth.",
     )
     username: Optional[v1.SecretKeySelector] = Field(
-        None, description="Username refers to the Kubernetes secret that holds the username required for basic auth."
+        default=None,
+        description="Username refers to the Kubernetes secret that holds the username required for basic auth.",
     )
 
 
 class BitbucketBasicAuth(BaseModel):
     password: Optional[v1.SecretKeySelector] = Field(
-        None, description="Password refers to the K8s secret that holds the password."
+        default=None, description="Password refers to the K8s secret that holds the password."
     )
     username: Optional[v1.SecretKeySelector] = Field(
-        None, description="Username refers to the K8s secret that holds the username."
+        default=None, description="Username refers to the K8s secret that holds the username."
     )
 
 
 class CalendarEventSource(BaseModel):
     exclusion_dates: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="exclusionDates",
         description="ExclusionDates defines the list of DATE-TIME exceptions for recurring events.",
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     interval: Optional[str] = Field(
-        None, title="Interval is a string that describes an interval duration, e.g. 1s, 30m, 2h...\n+optional"
+        default=None, title="Interval is a string that describes an interval duration, e.g. 1s, 30m, 2h...\n+optional"
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     persistence: Optional[EventPersistence] = Field(
-        None, title="Persistence hold the configuration for event persistence"
+        default=None, title="Persistence hold the configuration for event persistence"
     )
     schedule: Optional[str] = Field(
-        None,
+        default=None,
         title="Schedule is a cron-like expression. For reference, see: https://en.wikipedia.org/wiki/Cron\n+optional",
     )
-    timezone: Optional[str] = Field(None, title="Timezone in which to run the schedule\n+optional")
+    timezone: Optional[str] = Field(default=None, title="Timezone in which to run the schedule\n+optional")
 
 
 class Condition(BaseModel):
     last_transition_time: Optional[v1_1.Time] = Field(
-        None,
+        default=None,
         alias="lastTransitionTime",
         title="Last time the condition transitioned from one status to another.\n+optional",
     )
     message: Optional[str] = Field(
-        None, title="Human-readable message indicating details about last transition.\n+optional"
+        default=None, title="Human-readable message indicating details about last transition.\n+optional"
     )
     reason: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Unique, this should be a short, machine understandable string that gives the reason\nfor condition's"
             ' last transition. For example, "ImageNotFound"\n+optional'
         ),
     )
-    status: Optional[str] = Field(None, title="Condition status, True, False or Unknown.\n+required")
-    type: Optional[str] = Field(None, title="Condition type.\n+required")
+    status: Optional[str] = Field(default=None, title="Condition status, True, False or Unknown.\n+required")
+    type: Optional[str] = Field(default=None, title="Condition type.\n+required")
 
 
 class EventContext(BaseModel):
     datacontenttype: Optional[str] = Field(
-        None, description="DataContentType - A MIME (RFC2046) string describing the media type of `data`."
+        default=None, description="DataContentType - A MIME (RFC2046) string describing the media type of `data`."
     )
     id: Optional[str] = Field(
-        None, description="ID of the event; must be non-empty and unique within the scope of the producer."
+        default=None, description="ID of the event; must be non-empty and unique within the scope of the producer."
     )
-    source: Optional[str] = Field(None, description="Source - A URI describing the event producer.")
+    source: Optional[str] = Field(default=None, description="Source - A URI describing the event producer.")
     specversion: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "SpecVersion - The version of the CloudEvents specification used by the io.argoproj.workflow.v1alpha1."
         ),
     )
     subject: Optional[str] = Field(
-        None, title="Subject - The subject of the event in the context of the event producer"
+        default=None, title="Subject - The subject of the event in the context of the event producer"
     )
-    time: Optional[v1_1.Time] = Field(None, description="Time - A Timestamp when the event happened.")
-    type: Optional[str] = Field(None, description="Type - The type of the occurrence which has happened.")
+    time: Optional[v1_1.Time] = Field(default=None, description="Time - A Timestamp when the event happened.")
+    type: Optional[str] = Field(default=None, description="Type - The type of the occurrence which has happened.")
 
 
 class ExprFilter(BaseModel):
     expr: Optional[str] = Field(
-        None, description="Expr refers to the expression that determines the outcome of the filter."
+        default=None, description="Expr refers to the expression that determines the outcome of the filter."
     )
     fields: Optional[List[PayloadField]] = Field(
-        None, description="Fields refers to set of keys that refer to the paths within event payload."
+        default=None, description="Fields refers to set of keys that refer to the paths within event payload."
     )
 
 
 class FileEventSource(BaseModel):
     event_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="eventType",
         title=(
             "Type of file operations to watch\nRefer https://github.com/fsnotify/fsnotify/blob/master/fsnotify.go for"
             " more information"
         ),
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    polling: Optional[bool] = Field(None, title="Use polling instead of inotify")
+    polling: Optional[bool] = Field(default=None, title="Use polling instead of inotify")
     watch_path_config: Optional[WatchPathConfig] = Field(
-        None, alias="watchPathConfig", title="WatchPathConfig contains configuration about the file path to watch"
+        default=None,
+        alias="watchPathConfig",
+        title="WatchPathConfig contains configuration about the file path to watch",
     )
 
 
 class GenericEventSource(BaseModel):
     auth_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="authSecret",
         title="AuthSecret holds a secret selector that contains a bearer token for authentication\n+optional",
     )
-    config: Optional[str] = Field(None, title="Config is the event source configuration")
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
-    insecure: Optional[bool] = Field(None, description="Insecure determines the type of connection.")
+    config: Optional[str] = Field(default=None, title="Config is the event source configuration")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
+    insecure: Optional[bool] = Field(default=None, description="Insecure determines the type of connection.")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    url: Optional[str] = Field(None, description="URL of the gRPC server that implements the event source.")
+    url: Optional[str] = Field(default=None, description="URL of the gRPC server that implements the event source.")
 
 
 class GitCreds(BaseModel):
     password: Optional[v1.SecretKeySelector] = None
     username: Optional[v1.SecretKeySelector] = None
 
 
 class GithubAppCreds(BaseModel):
     app_id: Optional[str] = Field(
-        None, alias="appID", title="AppID refers to the GitHub App ID for the application you created"
+        default=None, alias="appID", title="AppID refers to the GitHub App ID for the application you created"
     )
     installation_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="installationID",
         title="InstallationID refers to the Installation ID of the GitHub app you created and installed",
     )
     private_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="privateKey", title="PrivateKey refers to a K8s secret containing the GitHub app private key"
+        default=None,
+        alias="privateKey",
+        title="PrivateKey refers to a K8s secret containing the GitHub app private key",
     )
 
 
 class HDFSEventSource(BaseModel):
     addresses: Optional[List[str]] = None
     check_interval: Optional[str] = Field(
-        None,
+        default=None,
         alias="checkInterval",
         title=(
             "CheckInterval is a string that describes an interval duration to check the directory state, e.g. 1s, 30m,"
             " 2h... (defaults to 1m)"
         ),
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     hdfs_user: Optional[str] = Field(
-        None,
+        default=None,
         alias="hdfsUser",
         description=(
             "HDFSUser is the user to access HDFS file system.\nIt is ignored if either ccache or keytab is used."
         ),
     )
     krb_c_cache_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbCCacheSecret",
         description=(
             "KrbCCacheSecret is the secret selector for Kerberos ccache\nEither ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_config_config_map: Optional[v1.ConfigMapKeySelector] = Field(
-        None,
+        default=None,
         alias="krbConfigConfigMap",
         description=(
             "KrbConfig is the configmap selector for Kerberos config as string\nIt must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_keytab_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbKeytabSecret",
         description=(
             "KrbKeytabSecret is the secret selector for Kerberos keytab\nEither ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_realm: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbRealm",
         description="KrbRealm is the Kerberos realm used with Kerberos keytab\nIt must be set if keytab is used.",
     )
     krb_service_principal_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbServicePrincipalName",
         description=(
             "KrbServicePrincipalName is the principal name of Kerberos service\nIt must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_username: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbUsername",
         description=(
             "KrbUsername is the Kerberos username used with Kerberos keytab\nIt must be set if keytab is used."
         ),
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    type: Optional[str] = Field(None, title="Type of file operations to watch")
-    watch_path_config: Optional[WatchPathConfig] = Field(None, alias="watchPathConfig")
+    type: Optional[str] = Field(default=None, title="Type of file operations to watch")
+    watch_path_config: Optional[WatchPathConfig] = Field(default=None, alias="watchPathConfig")
 
 
 class K8SResourcePolicy(BaseModel):
-    backoff: Optional[Backoff] = Field(None, title="Backoff before checking resource state")
+    backoff: Optional[Backoff] = Field(default=None, title="Backoff before checking resource state")
     error_on_backoff_timeout: Optional[bool] = Field(
-        None,
+        default=None,
         alias="errorOnBackoffTimeout",
         title=(
             "ErrorOnBackoffTimeout determines whether sensor should transition to error state if the trigger policy is"
             " unable to determine\nthe state of the resource"
         ),
     )
     labels: Optional[Dict[str, str]] = Field(
-        None, title="Labels required to identify whether a resource is in success state"
+        default=None, title="Labels required to identify whether a resource is in success state"
     )
 
 
 class NATSAuth(BaseModel):
-    basic: Optional[BasicAuth] = Field(None, title="Baisc auth with username and password\n+optional")
-    credential: Optional[v1.SecretKeySelector] = Field(None, title="credential used to connect\n+optional")
-    nkey: Optional[v1.SecretKeySelector] = Field(None, title="NKey used to connect\n+optional")
-    token: Optional[v1.SecretKeySelector] = Field(None, title="Token used to connect\n+optional")
+    basic: Optional[BasicAuth] = Field(default=None, title="Baisc auth with username and password\n+optional")
+    credential: Optional[v1.SecretKeySelector] = Field(default=None, title="credential used to connect\n+optional")
+    nkey: Optional[v1.SecretKeySelector] = Field(default=None, title="NKey used to connect\n+optional")
+    token: Optional[v1.SecretKeySelector] = Field(default=None, title="Token used to connect\n+optional")
 
 
 class PubSubEventSource(BaseModel):
     credential_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="credentialSecret",
         title=(
             "CredentialSecret references to the secret that contains JSON credentials to access GCP.\nIf it is"
             " missing, it implicitly uses Workload Identity to"
             " access.\nhttps://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity\n+optional"
         ),
     )
     delete_subscription_on_finish: Optional[bool] = Field(
-        None,
+        default=None,
         alias="deleteSubscriptionOnFinish",
         title=(
             "DeleteSubscriptionOnFinish determines whether to delete the GCP PubSub subscription once the event source"
             " is stopped.\n+optional"
         ),
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     project_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="projectID",
         title=(
             "ProjectID is GCP project ID for the subscription.\nRequired if you run Argo Events outside of"
             " GKE/GCE.\n(otherwise, the default value is its project)\n+optional"
         ),
     )
     subscription_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="subscriptionID",
         title=(
             "SubscriptionID is ID of subscription.\nRequired if you use existing subscription.\nThe default value will"
             " be auto generated hash based on this eventsource setting, so the subscription\nmight be recreated every"
             " time you update the setting, which has a possibility of event loss.\n+optional"
         ),
     )
     topic: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Topic to which the subscription should belongs.\nRequired if you want the eventsource to create a new"
             " subscription.\nIf you specify this field along with an existing subscription,\nit will be verified"
             " whether it actually belongs to the specified topic.\n+optional"
         ),
     )
     topic_project_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="topicProjectID",
         title="TopicProjectID is GCP project ID for the topic.\nBy default, it is same as ProjectID.\n+optional",
     )
 
 
 class ResourceFilter(BaseModel):
     after_start: Optional[bool] = Field(
-        None,
+        default=None,
         alias="afterStart",
         title="If the resource is created after the start time then the event is treated as valid.\n+optional",
     )
     created_by: Optional[v1_1.Time] = Field(
-        None,
+        default=None,
         alias="createdBy",
         title="If resource is created before the specified time then the event is treated as valid.\n+optional",
     )
     fields: Optional[List[Selector]] = Field(
-        None,
+        default=None,
         title=(
             "Fields provide field filters similar to K8s field selector\n(see"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/).\nUnlike K8s field"
             ' selector, it supports arbitrary fileds like "spec.serviceAccountName",\nand the value could be a string'
             ' or a regex.\nSame as K8s field selector, operator "=", "==" and "!=" are supported.\n+optional'
         ),
     )
     labels: Optional[List[Selector]] = Field(
-        None,
+        default=None,
         title=(
             "Labels provide listing options to K8s API to watch resource/s.\nRefer"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/label-selectors/ for more"
             " io.argoproj.workflow.v1alpha1.\n+optional"
         ),
     )
-    prefix: Optional[str] = Field(None, title="Prefix filter is applied on the resource name.\n+optional")
+    prefix: Optional[str] = Field(default=None, title="Prefix filter is applied on the resource name.\n+optional")
 
 
 class S3Artifact(BaseModel):
-    access_key: Optional[v1.SecretKeySelector] = Field(None, alias="accessKey")
+    access_key: Optional[v1.SecretKeySelector] = Field(default=None, alias="accessKey")
     bucket: Optional[S3Bucket] = None
     endpoint: Optional[str] = None
     events: Optional[List[str]] = None
     filter: Optional[S3Filter] = None
     insecure: Optional[bool] = None
     metadata: Optional[Dict[str, str]] = None
     region: Optional[str] = None
-    secret_key: Optional[v1.SecretKeySelector] = Field(None, alias="secretKey")
+    secret_key: Optional[v1.SecretKeySelector] = Field(default=None, alias="secretKey")
 
 
 class SASLConfig(BaseModel):
     mechanism: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "SASLMechanism is the name of the enabled SASL mechanism.\nPossible values: OAUTHBEARER, PLAIN (defaults"
             " to PLAIN).\n+optional"
         ),
     )
-    password: Optional[v1.SecretKeySelector] = Field(None, title="Password for SASL/PLAIN authentication")
+    password: Optional[v1.SecretKeySelector] = Field(default=None, title="Password for SASL/PLAIN authentication")
     user: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         title="User is the authentication identity (authcid) to present for\nSASL/PLAIN or SASL/SCRAM authentication",
     )
 
 
 class SQSEventSource(BaseModel):
     access_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key"
+        default=None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key"
     )
     dlq: Optional[bool] = Field(
-        None,
+        default=None,
         title=(
             "DLQ specifies if a dead-letter queue is configured for messages that can't be processed successfully.\nIf"
             " set to true, messages with invalid payload won't be acknowledged to allow to forward them farther to the"
             " dead-letter queue.\nThe default value is false.\n+optional"
         ),
     )
     endpoint: Optional[str] = Field(
-        None, title="Endpoint configures connection to a specific SQS endpoint instead of Amazons servers\n+optional"
+        default=None,
+        title="Endpoint configures connection to a specific SQS endpoint instead of Amazons servers\n+optional",
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    queue: Optional[str] = Field(None, title="Queue is AWS SQS queue to listen to for messages")
+    queue: Optional[str] = Field(default=None, title="Queue is AWS SQS queue to listen to for messages")
     queue_account_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="queueAccountId",
         title="QueueAccountID is the ID of the account that created the queue to monitor\n+optional",
     )
-    region: Optional[str] = Field(None, title="Region is AWS region")
+    region: Optional[str] = Field(default=None, title="Region is AWS region")
     role_arn: Optional[str] = Field(
-        None, alias="roleARN", title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional"
+        default=None,
+        alias="roleARN",
+        title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional",
     )
     secret_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key"
+        default=None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key"
     )
     session_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="sessionToken",
         title="SessionToken refers to K8s secret containing AWS temporary credentials(STS) session token\n+optional",
     )
     wait_time_seconds: Optional[str] = Field(
-        None,
+        default=None,
         alias="waitTimeSeconds",
         description=(
             "WaitTimeSeconds is The duration (in seconds) for which the call waits for a message to arrive\nin the"
             " queue before returning."
         ),
     )
 
 
 class Status(BaseModel):
     conditions: Optional[List[Condition]] = Field(
-        None,
+        default=None,
         title=(
             "Conditions are the latest available observations of a resource's current"
             " state.\n+optional\n+patchMergeKey=type\n+patchStrategy=merge"
         ),
     )
 
 
 class TLSConfig(BaseModel):
     ca_cert_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="caCertSecret", title="CACertSecret refers to the secret that contains the CA cert"
+        default=None, alias="caCertSecret", title="CACertSecret refers to the secret that contains the CA cert"
     )
     client_cert_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="clientCertSecret", title="ClientCertSecret refers to the secret that contains the client cert"
+        default=None,
+        alias="clientCertSecret",
+        title="ClientCertSecret refers to the secret that contains the client cert",
     )
     client_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="clientKeySecret", title="ClientKeySecret refers to the secret that contains the client key"
+        default=None,
+        alias="clientKeySecret",
+        title="ClientKeySecret refers to the secret that contains the client key",
     )
     insecure_skip_verify: Optional[bool] = Field(
-        None,
+        default=None,
         alias="insecureSkipVerify",
         title=(
             "If true, skips creation of TLSConfig with certs and creates an empty TLSConfig. (Defaults to"
             " false)\n+optional"
         ),
     )
 
 
 class TriggerParameter(BaseModel):
     dest: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Dest is the JSONPath of a resource key.\nA path is a series of keys separated by a dot. The colon"
             " character can be escaped with '.'\nThe -1 key can be used to append a value to an existing array.\nSee"
             " https://github.com/tidwall/sjson#path-syntax for more information about how this is used."
         ),
     )
     operation: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Operation is what to do with the existing value at Dest, whether to\n'prepend', 'overwrite', or"
             " 'append' it."
         ),
     )
     src: Optional[TriggerParameterSource] = Field(
-        None, title="Src contains a source reference to the value of the parameter from a dependency"
+        default=None, title="Src contains a source reference to the value of the parameter from a dependency"
     )
 
 
 class TriggerPolicy(BaseModel):
     k8s: Optional[K8SResourcePolicy] = Field(
-        None,
+        default=None,
         title=(
             "K8SResourcePolicy refers to the policy used to check the state of K8s based triggers using using labels"
         ),
     )
     status: Optional[StatusPolicy] = Field(
-        None, title="Status refers to the policy used to check the state of the trigger using response status"
+        default=None, title="Status refers to the policy used to check the state of the trigger using response status"
     )
 
 
 class ValueFromSource(BaseModel):
-    config_map_key_ref: Optional[v1.ConfigMapKeySelector] = Field(None, alias="configMapKeyRef")
-    secret_key_ref: Optional[v1.SecretKeySelector] = Field(None, alias="secretKeyRef")
+    config_map_key_ref: Optional[v1.ConfigMapKeySelector] = Field(default=None, alias="configMapKeyRef")
+    secret_key_ref: Optional[v1.SecretKeySelector] = Field(default=None, alias="secretKeyRef")
 
 
 class WebhookContext(BaseModel):
     auth_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="authSecret",
         title="AuthSecret holds a secret selector that contains a bearer token for authentication\n+optional",
     )
-    endpoint: Optional[str] = Field(None, title="REST API endpoint")
+    endpoint: Optional[str] = Field(default=None, title="REST API endpoint")
     max_payload_size: Optional[str] = Field(
-        None,
+        default=None,
         alias="maxPayloadSize",
         title=(
             "MaxPayloadSize is the maximum webhook payload size that the server will accept.\nRequests exceeding that"
             ' limit will be rejected with "request too large" response.\nDefault value: 1048576 (1MB).\n+optional'
         ),
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     method: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Method is HTTP request method that indicates the desired action to be performed for a given"
             " resource.\nSee RFC7231 Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content"
         ),
     )
-    port: Optional[str] = Field(None, description="Port on which HTTP server is listening for incoming events.")
+    port: Optional[str] = Field(
+        default=None, description="Port on which HTTP server is listening for incoming events."
+    )
     server_cert_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="serverCertSecret", description="ServerCertPath refers the file that contains the cert."
+        default=None, alias="serverCertSecret", description="ServerCertPath refers the file that contains the cert."
     )
     server_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="serverKeySecret", title="ServerKeyPath refers the file that contains private key"
+        default=None, alias="serverKeySecret", title="ServerKeyPath refers the file that contains private key"
     )
-    url: Optional[str] = Field(None, description="URL is the url of the server.")
+    url: Optional[str] = Field(default=None, description="URL is the url of the server.")
 
 
 class WebhookEventSource(BaseModel):
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
-    webhook_context: Optional[WebhookContext] = Field(None, alias="webhookContext")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
+    webhook_context: Optional[WebhookContext] = Field(default=None, alias="webhookContext")
 
 
 class AMQPEventSource(BaseModel):
-    auth: Optional[BasicAuth] = Field(None, title="Auth hosts secret selectors for username and password\n+optional")
+    auth: Optional[BasicAuth] = Field(
+        default=None, title="Auth hosts secret selectors for username and password\n+optional"
+    )
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
+        default=None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
     )
     consume: Optional[AMQPConsumeConfig] = Field(
-        None,
+        default=None,
         title=(
             "Consume holds the configuration to immediately starts delivering queued messages\nFor more information,"
             " visit https://pkg.go.dev/github.com/rabbitmq/amqp091-go#Channel.Consume\n+optional"
         ),
     )
     exchange_declare: Optional[AMQPExchangeDeclareConfig] = Field(
-        None,
+        default=None,
         alias="exchangeDeclare",
         title=(
             "ExchangeDeclare holds the configuration for the exchange on the server\nFor more information, visit"
             " https://pkg.go.dev/github.com/rabbitmq/amqp091-go#Channel.ExchangeDeclare\n+optional"
         ),
     )
     exchange_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="exchangeName",
         title=(
             "ExchangeName is the exchange name\nFor more information, visit"
             " https://www.rabbitmq.com/tutorials/amqp-concepts.html"
         ),
     )
-    exchange_type: Optional[str] = Field(None, alias="exchangeType", title="ExchangeType is rabbitmq exchange type")
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    exchange_type: Optional[str] = Field(
+        default=None, alias="exchangeType", title="ExchangeType is rabbitmq exchange type"
+    )
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     queue_bind: Optional[AMQPQueueBindConfig] = Field(
-        None,
+        default=None,
         alias="queueBind",
         title=(
             "QueueBind holds the configuration that binds an exchange to a queue so that publishings to the\nexchange"
             " will be routed to the queue when the publishing routing key matches the binding routing key\nFor more"
             " information, visit https://pkg.go.dev/github.com/rabbitmq/amqp091-go#Channel.QueueBind\n+optional"
         ),
     )
     queue_declare: Optional[AMQPQueueDeclareConfig] = Field(
-        None,
+        default=None,
         alias="queueDeclare",
         title=(
             "QueueDeclare holds the configuration of a queue to hold messages and deliver to consumers.\nDeclaring"
             " creates a queue if it doesn't already exist, or ensures that an existing queue matches\nthe same"
             " parameters\nFor more information, visit"
             " https://pkg.go.dev/github.com/rabbitmq/amqp091-go#Channel.QueueDeclare\n+optional"
         ),
     )
-    routing_key: Optional[str] = Field(None, alias="routingKey", title="Routing key for bindings")
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the amqp client.\n+optional")
-    url: Optional[str] = Field(None, title="URL for rabbitmq service")
+    routing_key: Optional[str] = Field(default=None, alias="routingKey", title="Routing key for bindings")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the amqp client.\n+optional")
+    url: Optional[str] = Field(default=None, title="URL for rabbitmq service")
     url_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="urlSecret", title="URLSecret is secret reference for rabbitmq service URL"
+        default=None, alias="urlSecret", title="URLSecret is secret reference for rabbitmq service URL"
     )
 
 
 class AWSLambdaTrigger(BaseModel):
     access_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key\n+optional"
+        default=None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key\n+optional"
     )
     function_name: Optional[str] = Field(
-        None, alias="functionName", description="FunctionName refers to the name of the function to invoke."
+        default=None, alias="functionName", description="FunctionName refers to the name of the function to invoke."
     )
     invocation_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="invocationType",
         description=(
             "Choose from the following options.\n\n   * RequestResponse (default) - Invoke the function synchronously."
             " Keep\n   the connection open until the function returns a response or times out.\n   The API response"
             " includes the function response and additional data.\n\n   * Event - Invoke the function asynchronously."
             " Send events that fail multiple\n   times to the function's dead-letter queue (if it's configured). The"
             " API\n   response only includes a status code.\n\n   * DryRun - Validate parameter values and verify that"
             " the user or role\n   has permission to invoke the function.\n+optional"
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         title=(
             "Parameters is the list of key-value extracted from event's payload that are applied to\nthe trigger"
             " resource.\n+optional"
         ),
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
-    region: Optional[str] = Field(None, title="Region is AWS region")
+    region: Optional[str] = Field(default=None, title="Region is AWS region")
     role_arn: Optional[str] = Field(
-        None, alias="roleARN", title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional"
+        default=None,
+        alias="roleARN",
+        title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional",
     )
     secret_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key\n+optional"
+        default=None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key\n+optional"
     )
 
 
 class AzureEventHubsTrigger(BaseModel):
     fqdn: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "FQDN refers to the namespace dns of Azure Event Hubs to be used i.e. <namespace>.servicebus.windows.net"
         ),
     )
     hub_name: Optional[str] = Field(
-        None, alias="hubName", title="HubName refers to the Azure Event Hub to send events to"
+        default=None, alias="hubName", title="HubName refers to the Azure Event Hub to send events to"
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         title=(
             "Parameters is the list of key-value extracted from event's payload that are applied to\nthe trigger"
             " resource.\n+optional"
         ),
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
     shared_access_key: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="sharedAccessKey",
         title="SharedAccessKey refers to a K8s secret containing the primary key for the",
     )
     shared_access_key_name: Optional[v1.SecretKeySelector] = Field(
-        None, alias="sharedAccessKeyName", title="SharedAccessKeyName refers to the name of the Shared Access Key"
+        default=None,
+        alias="sharedAccessKeyName",
+        title="SharedAccessKeyName refers to the name of the Shared Access Key",
     )
 
 
 class BitbucketAuth(BaseModel):
-    basic: Optional[BitbucketBasicAuth] = Field(None, title="Basic is BasicAuth auth strategy.\n+optional")
+    basic: Optional[BitbucketBasicAuth] = Field(default=None, title="Basic is BasicAuth auth strategy.\n+optional")
     oauth_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="oauthToken",
         title="OAuthToken refers to the K8s secret that holds the OAuth Bearer token.\n+optional",
     )
 
 
 class BitbucketEventSource(BaseModel):
-    auth: Optional[BitbucketAuth] = Field(None, description="Auth information required to connect to Bitbucket.")
+    auth: Optional[BitbucketAuth] = Field(
+        default=None, description="Auth information required to connect to Bitbucket."
+    )
     delete_hook_on_finish: Optional[bool] = Field(
-        None,
+        default=None,
         alias="deleteHookOnFinish",
         title=(
             "DeleteHookOnFinish determines whether to delete the defined Bitbucket hook once the event source is"
             " stopped.\n+optional"
         ),
     )
-    events: Optional[List[str]] = Field(None, description="Events this webhook is subscribed to.")
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    events: Optional[List[str]] = Field(default=None, description="Events this webhook is subscribed to.")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will be passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will be passed along the event payload.\n+optional",
     )
     owner: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "DeprecatedOwner is the owner of the repository.\nDeprecated: use Repositories instead. Will be"
             " unsupported in v1.9\n+optional"
         ),
     )
     project_key: Optional[str] = Field(
-        None,
+        default=None,
         alias="projectKey",
         title=(
             "DeprecatedProjectKey is the key of the project to which the repository relates\nDeprecated: use"
             " Repositories instead. Will be unsupported in v1.9\n+optional"
         ),
     )
     repositories: Optional[List[BitbucketRepository]] = Field(
-        None, title="Repositories holds a list of repositories for which integration needs to set up\n+optional"
+        default=None,
+        title="Repositories holds a list of repositories for which integration needs to set up\n+optional",
     )
     repository_slug: Optional[str] = Field(
-        None,
+        default=None,
         alias="repositorySlug",
         title=(
             "DeprecatedRepositorySlug is a URL-friendly version of a repository name, automatically generated by"
             " Bitbucket for use in the URL\nDeprecated: use Repositories instead. Will be unsupported in"
             " v1.9\n+optional"
         ),
     )
     webhook: Optional[WebhookContext] = Field(
-        None, title="Webhook refers to the configuration required to run an http server"
+        default=None, title="Webhook refers to the configuration required to run an http server"
     )
 
 
 class BitbucketServerEventSource(BaseModel):
     access_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="accessToken",
         title="AccessToken is reference to K8s secret which holds the bitbucket api access information",
     )
     bitbucketserver_base_url: Optional[str] = Field(
-        None,
+        default=None,
         alias="bitbucketserverBaseURL",
         title="BitbucketServerBaseURL is the base URL for API requests to a custom endpoint",
     )
     delete_hook_on_finish: Optional[bool] = Field(
-        None,
+        default=None,
         alias="deleteHookOnFinish",
         title=(
             "DeleteHookOnFinish determines whether to delete the Bitbucket Server hook for the project once the event"
             " source is stopped.\n+optional"
         ),
     )
     events: Optional[List[str]] = Field(
-        None,
+        default=None,
         title=(
             "Events are bitbucket event to listen to.\nRefer"
             " https://confluence.atlassian.com/bitbucketserver/event-payload-938025882.html"
         ),
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     project_key: Optional[str] = Field(
-        None,
+        default=None,
         alias="projectKey",
         title=(
             "DeprecatedProjectKey is the key of project for which integration needs to set up\nDeprecated: use"
             " Repositories instead. Will be unsupported in v1.8\n+optional"
         ),
     )
     repositories: Optional[List[BitbucketServerRepository]] = Field(
-        None, title="Repositories holds a list of repositories for which integration needs to set up\n+optional"
+        default=None,
+        title="Repositories holds a list of repositories for which integration needs to set up\n+optional",
     )
     repository_slug: Optional[str] = Field(
-        None,
+        default=None,
         alias="repositorySlug",
         title=(
             "DeprecatedRepositorySlug is the slug of the repository for which integration needs to set up\nDeprecated:"
             " use Repositories instead. Will be unsupported in v1.8\n+optional"
         ),
     )
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook holds configuration to run a http server")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook holds configuration to run a http server")
     webhook_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="webhookSecret",
         title=(
             "WebhookSecret is reference to K8s secret which holds the bitbucket webhook secret (for HMAC validation)"
         ),
     )
 
 
 class CustomTrigger(BaseModel):
     cert_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="certSecret",
         description=(
             "CertSecret refers to the secret that contains cert for secure connection between sensor and custom"
             " trigger gRPC server."
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description="Parameters is the list of parameters that is applied to resolved custom trigger trigger object.",
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
     secure: Optional[bool] = Field(
-        None, title="Secure refers to type of the connection between sensor to custom trigger gRPC"
+        default=None, title="Secure refers to type of the connection between sensor to custom trigger gRPC"
     )
     server_name_override: Optional[str] = Field(
-        None,
+        default=None,
         alias="serverNameOverride",
         description="ServerNameOverride for the secure connection between sensor and custom trigger gRPC server.",
     )
     server_url: Optional[str] = Field(
-        None, alias="serverURL", title="ServerURL is the url of the gRPC server that executes custom trigger"
+        default=None, alias="serverURL", title="ServerURL is the url of the gRPC server that executes custom trigger"
     )
     spec: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         description=(
             "Spec is the custom trigger resource specification that custom trigger gRPC server knows how to interpret."
         ),
     )
 
 
 class EmitterEventSource(BaseModel):
-    broker: Optional[str] = Field(None, description="Broker URI to connect to.")
-    channel_key: Optional[str] = Field(None, alias="channelKey", title="ChannelKey refers to the channel key")
-    channel_name: Optional[str] = Field(None, alias="channelName", title="ChannelName refers to the channel name")
+    broker: Optional[str] = Field(default=None, description="Broker URI to connect to.")
+    channel_key: Optional[str] = Field(default=None, alias="channelKey", title="ChannelKey refers to the channel key")
+    channel_name: Optional[str] = Field(
+        default=None, alias="channelName", title="ChannelName refers to the channel name"
+    )
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
+        default=None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
+    )
+    password: Optional[v1.SecretKeySelector] = Field(
+        default=None, title="Password to use to connect to broker\n+optional"
+    )
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the emitter client.\n+optional")
+    username: Optional[v1.SecretKeySelector] = Field(
+        default=None, title="Username to use to connect to broker\n+optional"
     )
-    password: Optional[v1.SecretKeySelector] = Field(None, title="Password to use to connect to broker\n+optional")
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the emitter client.\n+optional")
-    username: Optional[v1.SecretKeySelector] = Field(None, title="Username to use to connect to broker\n+optional")
 
 
 class EventDependencyFilter(BaseModel):
-    context: Optional[EventContext] = Field(None, title="Context filter constraints")
-    data: Optional[List[DataFilter]] = Field(None, title="Data filter constraints with escalation")
+    context: Optional[EventContext] = Field(default=None, title="Context filter constraints")
+    data: Optional[List[DataFilter]] = Field(default=None, title="Data filter constraints with escalation")
     data_logical_operator: Optional[str] = Field(
-        None,
+        default=None,
         alias="dataLogicalOperator",
         description=(
             "DataLogicalOperator defines how multiple Data filters (if defined) are evaluated together.\nAvailable"
             " values: and (&&), or (||)\nIs optional and if left blank treated as and (&&)."
         ),
     )
     expr_logical_operator: Optional[str] = Field(
-        None,
+        default=None,
         alias="exprLogicalOperator",
         description=(
             "ExprLogicalOperator defines how multiple Exprs filters (if defined) are evaluated together.\nAvailable"
             " values: and (&&), or (||)\nIs optional and if left blank treated as and (&&)."
         ),
     )
     exprs: Optional[List[ExprFilter]] = Field(
-        None, description="Exprs contains the list of expressions evaluated against the event payload."
+        default=None, description="Exprs contains the list of expressions evaluated against the event payload."
     )
     script: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Script refers to a Lua script evaluated to determine the validity of an io.argoproj.workflow.v1alpha1."
         ),
     )
-    time: Optional[TimeFilter] = Field(None, title="Time filter on the event with escalation")
+    time: Optional[TimeFilter] = Field(default=None, title="Time filter on the event with escalation")
 
 
 class EventSourceStatus(BaseModel):
     status: Optional[Status] = None
 
 
 class GitArtifact(BaseModel):
-    branch: Optional[str] = Field(None, title="Branch to use to pull trigger resource\n+optional")
+    branch: Optional[str] = Field(default=None, title="Branch to use to pull trigger resource\n+optional")
     clone_directory: Optional[str] = Field(
-        None,
+        default=None,
         alias="cloneDirectory",
         description=(
             "Directory to clone the repository. We clone complete directory because GitArtifact is not limited to any"
             " specific Git service providers.\nHence we don't use any specific git provider client."
         ),
     )
-    creds: Optional[GitCreds] = Field(None, title="Creds contain reference to git username and password\n+optional")
+    creds: Optional[GitCreds] = Field(
+        default=None, title="Creds contain reference to git username and password\n+optional"
+    )
     file_path: Optional[str] = Field(
-        None, alias="filePath", title="Path to file that contains trigger resource definition"
+        default=None, alias="filePath", title="Path to file that contains trigger resource definition"
     )
     insecure_ignore_host_key: Optional[bool] = Field(
-        None, alias="insecureIgnoreHostKey", title="Whether to ignore host key\n+optional"
+        default=None, alias="insecureIgnoreHostKey", title="Whether to ignore host key\n+optional"
     )
     ref: Optional[str] = Field(
-        None, title="Ref to use to pull trigger resource. Will result in a shallow clone and\nfetch.\n+optional"
+        default=None,
+        title="Ref to use to pull trigger resource. Will result in a shallow clone and\nfetch.\n+optional",
     )
     remote: Optional[GitRemoteConfig] = Field(
-        None,
+        default=None,
         title=(
             'Remote to manage set of tracked repositories. Defaults to "origin".\nRefer'
             " https://git-scm.com/docs/git-remote\n+optional"
         ),
     )
     ssh_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="sshKeySecret", title="SSHKeySecret refers to the secret that contains SSH key"
+        default=None, alias="sshKeySecret", title="SSHKeySecret refers to the secret that contains SSH key"
     )
-    tag: Optional[str] = Field(None, title="Tag to use to pull trigger resource\n+optional")
-    url: Optional[str] = Field(None, title="Git URL")
+    tag: Optional[str] = Field(default=None, title="Tag to use to pull trigger resource\n+optional")
+    url: Optional[str] = Field(default=None, title="Git URL")
 
 
 class GithubEventSource(BaseModel):
     active: Optional[bool] = Field(
-        None,
+        default=None,
         title=(
             "Active refers to status of the webhook for event"
             " deliveries.\nhttps://developer.github.com/webhooks/creating/#active\n+optional"
         ),
     )
     api_token: Optional[v1.SecretKeySelector] = Field(
-        None, alias="apiToken", title="APIToken refers to a K8s secret containing github api token\n+optional"
+        default=None, alias="apiToken", title="APIToken refers to a K8s secret containing github api token\n+optional"
     )
-    content_type: Optional[str] = Field(None, alias="contentType", title="ContentType of the event delivery")
+    content_type: Optional[str] = Field(default=None, alias="contentType", title="ContentType of the event delivery")
     delete_hook_on_finish: Optional[bool] = Field(
-        None,
+        default=None,
         alias="deleteHookOnFinish",
         title=(
             "DeleteHookOnFinish determines whether to delete the GitHub hook for the repository once the event source"
             " is stopped.\n+optional"
         ),
     )
     events: Optional[List[str]] = Field(
-        None, title="Events refer to Github events to which the event source will subscribe"
+        default=None, title="Events refer to Github events to which the event source will subscribe"
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     github_app: Optional[GithubAppCreds] = Field(
-        None, alias="githubApp", title="GitHubApp holds the GitHub app credentials\n+optional"
+        default=None, alias="githubApp", title="GitHubApp holds the GitHub app credentials\n+optional"
     )
     github_base_url: Optional[str] = Field(
-        None, alias="githubBaseURL", title="GitHub base URL (for GitHub Enterprise)\n+optional"
+        default=None, alias="githubBaseURL", title="GitHub base URL (for GitHub Enterprise)\n+optional"
     )
     github_upload_url: Optional[str] = Field(
-        None, alias="githubUploadURL", title="GitHub upload URL (for GitHub Enterprise)\n+optional"
+        default=None, alias="githubUploadURL", title="GitHub upload URL (for GitHub Enterprise)\n+optional"
     )
     id: Optional[str] = Field(
-        None, title="Id is the webhook's id\nDeprecated: This is not used at all, will be removed in v1.6\n+optional"
+        default=None,
+        title="Id is the webhook's id\nDeprecated: This is not used at all, will be removed in v1.6\n+optional",
     )
-    insecure: Optional[bool] = Field(None, title="Insecure tls verification")
+    insecure: Optional[bool] = Field(default=None, title="Insecure tls verification")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     organizations: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Organizations holds the names of organizations (used for organization level webhooks). Not required if"
             " Repositories is set."
         ),
     )
     owner: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "DeprecatedOwner refers to GitHub owner name i.e. argoproj\nDeprecated: use Repositories instead. Will be"
             " unsupported in v 1.6\n+optional"
         ),
     )
     repositories: Optional[List[OwnedRepositories]] = Field(
-        None,
+        default=None,
         description=(
             "Repositories holds the information of repositories, which uses repo owner as the key,\nand list of repo"
             " names as the value. Not required if Organizations is set."
         ),
     )
     repository: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "DeprecatedRepository refers to GitHub repo name i.e. argo-events\nDeprecated: use Repositories instead."
             " Will be unsupported in v 1.6\n+optional"
         ),
     )
     webhook: Optional[WebhookContext] = Field(
-        None, title="Webhook refers to the configuration required to run a http server"
+        default=None, title="Webhook refers to the configuration required to run a http server"
     )
     webhook_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="webhookSecret",
         title=(
             "WebhookSecret refers to K8s secret containing GitHub webhook"
             " secret\nhttps://developer.github.com/webhooks/securing/\n+optional"
         ),
     )
 
 
 class GitlabEventSource(BaseModel):
     access_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="accessToken",
         title="AccessToken references to k8 secret which holds the gitlab api access information",
     )
     delete_hook_on_finish: Optional[bool] = Field(
-        None,
+        default=None,
         alias="deleteHookOnFinish",
         title=(
             "DeleteHookOnFinish determines whether to delete the GitLab hook for the project once the event source is"
             " stopped.\n+optional"
         ),
     )
     enable_ssl_verification: Optional[bool] = Field(
-        None, alias="enableSSLVerification", title="EnableSSLVerification to enable ssl verification\n+optional"
+        default=None,
+        alias="enableSSLVerification",
+        title="EnableSSLVerification to enable ssl verification\n+optional",
     )
     events: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Events are gitlab event to listen to.\nRefer"
             " https://github.com/xanzy/go-gitlab/blob/bf34eca5d13a9f4c3f501d8a97b8ac226d55e4d9/projects.go#L794."
         ),
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     gitlab_base_url: Optional[str] = Field(
-        None, alias="gitlabBaseURL", title="GitlabBaseURL is the base URL for API requests to a custom endpoint"
+        default=None,
+        alias="gitlabBaseURL",
+        title="GitlabBaseURL is the base URL for API requests to a custom endpoint",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     project_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="projectID",
         title=(
             "DeprecatedProjectID is the id of project for which integration needs to setup\nDeprecated: use Projects"
             " instead. Will be unsupported in v 1.7\n+optional"
         ),
     )
     projects: Optional[List[str]] = Field(
-        None, title='List of project IDs or project namespace paths like "whynowy/test"'
+        default=None, title='List of project IDs or project namespace paths like "whynowy/test"'
     )
     secret_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="secretToken",
         title="SecretToken references to k8 secret which holds the Secret Token used by webhook config",
     )
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook holds configuration to run a http server")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook holds configuration to run a http server")
 
 
 class KafkaEventSource(BaseModel):
     config: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Yaml format Sarama config for Kafka connection.\nIt follows the struct of sarama.Config. See"
             " https://github.com/Shopify/sarama/blob/main/config.go\ne.g.\n\nconsumer:\n  fetch:\n    min: 1\nnet:\n "
             " MaxOpenRequests: 5\n\n+optional"
         ),
     )
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", description="Backoff holds parameters applied to connection."
+        default=None, alias="connectionBackoff", description="Backoff holds parameters applied to connection."
     )
     consumer_group: Optional[KafkaConsumerGroup] = Field(
-        None, alias="consumerGroup", title="Consumer group for kafka client\n+optional"
+        default=None, alias="consumerGroup", title="Consumer group for kafka client\n+optional"
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     limit_events_per_second: Optional[str] = Field(
-        None,
+        default=None,
         alias="limitEventsPerSecond",
         title="Sets a limit on how many events get read from kafka per second.\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    partition: Optional[str] = Field(None, title="Partition name")
-    sasl: Optional[SASLConfig] = Field(None, title="SASL configuration for the kafka client\n+optional")
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the kafka client.\n+optional")
-    topic: Optional[str] = Field(None, title="Topic name")
-    url: Optional[str] = Field(None, title="URL to kafka cluster, multiple URLs separated by comma")
+    partition: Optional[str] = Field(default=None, title="Partition name")
+    sasl: Optional[SASLConfig] = Field(default=None, title="SASL configuration for the kafka client\n+optional")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the kafka client.\n+optional")
+    topic: Optional[str] = Field(default=None, title="Topic name")
+    url: Optional[str] = Field(default=None, title="URL to kafka cluster, multiple URLs separated by comma")
     version: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Specify what kafka version is being connected to enables certain features in sarama, defaults to"
             " 1.0.0\n+optional"
         ),
     )
 
 
 class KafkaTrigger(BaseModel):
     compress: Optional[bool] = Field(
-        None,
+        default=None,
         title=(
             "Compress determines whether to compress message or not.\nDefaults to false.\nIf set to true, compresses"
             " message using snappy compression.\n+optional"
         ),
     )
     flush_frequency: Optional[int] = Field(
-        None,
+        default=None,
         alias="flushFrequency",
         title=(
             "FlushFrequency refers to the frequency in milliseconds to flush batches.\nDefaults to 500"
             " milliseconds.\n+optional"
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None, description="Parameters is the list of parameters that is applied to resolved Kafka trigger object."
+        default=None,
+        description="Parameters is the list of parameters that is applied to resolved Kafka trigger object.",
     )
-    partition: Optional[int] = Field(None, description="Partition to write data to.")
+    partition: Optional[int] = Field(default=None, description="Partition to write data to.")
     partitioning_key: Optional[str] = Field(
-        None,
+        default=None,
         alias="partitioningKey",
         description=(
             "The partitioning key for the messages put on the Kafka topic.\nDefaults to broker url.\n+optional."
         ),
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
     required_acks: Optional[int] = Field(
-        None,
+        default=None,
         alias="requiredAcks",
         description=(
             "RequiredAcks used in producer to tell the broker how many replica acknowledgements\nDefaults to 1 (Only"
             " wait for the leader to ack).\n+optional."
         ),
     )
-    sasl: Optional[SASLConfig] = Field(None, title="SASL configuration for the kafka client\n+optional")
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the Kafka producer.\n+optional")
+    sasl: Optional[SASLConfig] = Field(default=None, title="SASL configuration for the kafka client\n+optional")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the Kafka producer.\n+optional")
     topic: Optional[str] = Field(
-        None, title="Name of the topic.\nMore info at https://kafka.apache.org/documentation/#intro_topics"
+        default=None, title="Name of the topic.\nMore info at https://kafka.apache.org/documentation/#intro_topics"
     )
-    url: Optional[str] = Field(None, description="URL of the Kafka broker, multiple URLs separated by comma.")
+    url: Optional[str] = Field(default=None, description="URL of the Kafka broker, multiple URLs separated by comma.")
     version: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Specify what kafka version is being connected to enables certain features in sarama, defaults to"
             " 1.0.0\n+optional"
         ),
     )
 
 
 class MQTTEventSource(BaseModel):
-    client_id: Optional[str] = Field(None, alias="clientId", title="ClientID is the id of the client")
+    client_id: Optional[str] = Field(default=None, alias="clientId", title="ClientID is the id of the client")
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", description="ConnectionBackoff holds backoff applied to connection."
+        default=None, alias="connectionBackoff", description="ConnectionBackoff holds backoff applied to connection."
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the mqtt client.\n+optional")
-    topic: Optional[str] = Field(None, title="Topic name")
-    url: Optional[str] = Field(None, title="URL to connect to broker")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the mqtt client.\n+optional")
+    topic: Optional[str] = Field(default=None, title="Topic name")
+    url: Optional[str] = Field(default=None, title="URL to connect to broker")
 
 
 class NATSEventsSource(BaseModel):
-    auth: Optional[NATSAuth] = Field(None, title="Auth information\n+optional")
+    auth: Optional[NATSAuth] = Field(default=None, title="Auth information\n+optional")
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", description="ConnectionBackoff holds backoff applied to connection."
+        default=None, alias="connectionBackoff", description="ConnectionBackoff holds backoff applied to connection."
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     subject: Optional[str] = Field(
-        None, title="Subject holds the name of the subject onto which messages are published"
+        default=None, title="Subject holds the name of the subject onto which messages are published"
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the nats client.\n+optional")
-    url: Optional[str] = Field(None, title="URL to connect to NATS cluster")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the nats client.\n+optional")
+    url: Optional[str] = Field(default=None, title="URL to connect to NATS cluster")
 
 
 class NATSTrigger(BaseModel):
     parameters: Optional[List[TriggerParameter]] = None
     payload: Optional[List[TriggerParameter]] = None
-    subject: Optional[str] = Field(None, description="Name of the subject to put message on.")
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the NATS producer.\n+optional")
-    url: Optional[str] = Field(None, description="URL of the NATS cluster.")
+    subject: Optional[str] = Field(default=None, description="Name of the subject to put message on.")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the NATS producer.\n+optional")
+    url: Optional[str] = Field(default=None, description="URL of the NATS cluster.")
 
 
 class NSQEventSource(BaseModel):
-    channel: Optional[str] = Field(None, title="Channel used for subscription")
+    channel: Optional[str] = Field(default=None, title="Channel used for subscription")
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
+        default=None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     host_address: Optional[str] = Field(
-        None, alias="hostAddress", title="HostAddress is the address of the host for NSQ lookup"
+        default=None, alias="hostAddress", title="HostAddress is the address of the host for NSQ lookup"
     )
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the nsq client.\n+optional")
-    topic: Optional[str] = Field(None, description="Topic to subscribe to.")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the nsq client.\n+optional")
+    topic: Optional[str] = Field(default=None, description="Topic to subscribe to.")
 
 
 class OpenWhiskTrigger(BaseModel):
-    action_name: Optional[str] = Field(None, alias="actionName", description="Name of the action/function.")
+    action_name: Optional[str] = Field(default=None, alias="actionName", description="Name of the action/function.")
     auth_token: Optional[v1.SecretKeySelector] = Field(
-        None, alias="authToken", title="AuthToken for authentication.\n+optional"
+        default=None, alias="authToken", title="AuthToken for authentication.\n+optional"
+    )
+    host: Optional[str] = Field(default=None, description="Host URL of the OpenWhisk.")
+    namespace: Optional[str] = Field(
+        default=None, description='Namespace for the action.\nDefaults to "_".\n+optional.'
     )
-    host: Optional[str] = Field(None, description="Host URL of the OpenWhisk.")
-    namespace: Optional[str] = Field(None, description='Namespace for the action.\nDefaults to "_".\n+optional.')
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         title=(
             "Parameters is the list of key-value extracted from event's payload that are applied to\nthe trigger"
             " resource.\n+optional"
         ),
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
-    version: Optional[str] = Field(None, title="Version for the API.\nDefaults to v1.\n+optional")
+    version: Optional[str] = Field(default=None, title="Version for the API.\nDefaults to v1.\n+optional")
 
 
 class PulsarEventSource(BaseModel):
     auth_token_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="authTokenSecret", title="Authentication token for the pulsar client.\n+optional"
+        default=None, alias="authTokenSecret", title="Authentication token for the pulsar client.\n+optional"
     )
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
+        default=None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the pulsar client.\n+optional")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the pulsar client.\n+optional")
     tls_allow_insecure_connection: Optional[bool] = Field(
-        None,
+        default=None,
         alias="tlsAllowInsecureConnection",
         title="Whether the Pulsar client accept untrusted TLS certificate from broker.\n+optional",
     )
     tls_trust_certs_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="tlsTrustCertsSecret", title="Trusted TLS certificate secret.\n+optional"
+        default=None, alias="tlsTrustCertsSecret", title="Trusted TLS certificate secret.\n+optional"
     )
     tls_validate_hostname: Optional[bool] = Field(
-        None,
+        default=None,
         alias="tlsValidateHostname",
         title="Whether the Pulsar client verify the validity of the host name from broker.\n+optional",
     )
-    topics: Optional[List[str]] = Field(None, title="Name of the topics to subscribe to.\n+required")
+    topics: Optional[List[str]] = Field(default=None, title="Name of the topics to subscribe to.\n+required")
     type: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             'Type of the subscription.\nOnly "exclusive" and "shared" is supported.\nDefaults to exclusive.\n+optional'
         ),
     )
-    url: Optional[str] = Field(None, title="Configure the service URL for the Pulsar service.\n+required")
+    url: Optional[str] = Field(default=None, title="Configure the service URL for the Pulsar service.\n+required")
 
 
 class PulsarTrigger(BaseModel):
     auth_token_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="authTokenSecret", title="Authentication token for the pulsar client.\n+optional"
+        default=None, alias="authTokenSecret", title="Authentication token for the pulsar client.\n+optional"
     )
     connection_backoff: Optional[Backoff] = Field(
-        None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
+        default=None, alias="connectionBackoff", title="Backoff holds parameters applied to connection.\n+optional"
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None, description="Parameters is the list of parameters that is applied to resolved Kafka trigger object."
+        default=None,
+        description="Parameters is the list of parameters that is applied to resolved Kafka trigger object.",
     )
     payload: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Payload is the list of key-value extracted from an event payload to construct the request payload."
         ),
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the pulsar client.\n+optional")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the pulsar client.\n+optional")
     tls_allow_insecure_connection: Optional[bool] = Field(
-        None,
+        default=None,
         alias="tlsAllowInsecureConnection",
         title="Whether the Pulsar client accept untrusted TLS certificate from broker.\n+optional",
     )
     tls_trust_certs_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="tlsTrustCertsSecret", title="Trusted TLS certificate secret.\n+optional"
+        default=None, alias="tlsTrustCertsSecret", title="Trusted TLS certificate secret.\n+optional"
     )
     tls_validate_hostname: Optional[bool] = Field(
-        None,
+        default=None,
         alias="tlsValidateHostname",
         title="Whether the Pulsar client verify the validity of the host name from broker.\n+optional",
     )
     topic: Optional[str] = Field(
-        None, title="Name of the topic.\nSee https://pulsar.apache.org/docs/en/concepts-messaging/"
+        default=None, title="Name of the topic.\nSee https://pulsar.apache.org/docs/en/concepts-messaging/"
     )
-    url: Optional[str] = Field(None, title="Configure the service URL for the Pulsar service.\n+required")
+    url: Optional[str] = Field(default=None, title="Configure the service URL for the Pulsar service.\n+required")
 
 
 class RedisEventSource(BaseModel):
     channels: Optional[List[str]] = None
-    db: Optional[int] = Field(None, title="DB to use. If not specified, default DB 0 will be used.\n+optional")
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    db: Optional[int] = Field(default=None, title="DB to use. If not specified, default DB 0 will be used.\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     host_address: Optional[str] = Field(
-        None, alias="hostAddress", title="HostAddress refers to the address of the Redis host/server"
+        default=None, alias="hostAddress", title="HostAddress refers to the address of the Redis host/server"
     )
     json_body: Optional[bool] = Field(
-        None,
+        default=None,
         alias="jsonBody",
         title="JSONBody specifies that all event body payload coming from this\nsource will be JSON\n+optional",
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     namespace: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Namespace to use to retrieve the password from. It should only be specified if password is"
             " declared\n+optional"
         ),
     )
     password: Optional[v1.SecretKeySelector] = Field(
-        None, title="Password required for authentication if any.\n+optional"
+        default=None, title="Password required for authentication if any.\n+optional"
+    )
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the redis client.\n+optional")
+    username: Optional[str] = Field(
+        default=None, title="Username required for ACL style authentication if any.\n+optional"
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the redis client.\n+optional")
-    username: Optional[str] = Field(None, title="Username required for ACL style authentication if any.\n+optional")
 
 
 class RedisStreamEventSource(BaseModel):
     consumer_group: Optional[str] = Field(
-        None,
+        default=None,
         alias="consumerGroup",
         title=(
             "ConsumerGroup refers to the Redis stream consumer group that will be\ncreated on all redis streams."
             " Messages are read through this group. Defaults to 'argo-events-cg'\n+optional"
         ),
     )
-    db: Optional[int] = Field(None, title="DB to use. If not specified, default DB 0 will be used.\n+optional")
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    db: Optional[int] = Field(default=None, title="DB to use. If not specified, default DB 0 will be used.\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     host_address: Optional[str] = Field(
-        None, alias="hostAddress", title="HostAddress refers to the address of the Redis host/server (master instance)"
+        default=None,
+        alias="hostAddress",
+        title="HostAddress refers to the address of the Redis host/server (master instance)",
     )
     max_msg_count_per_read: Optional[int] = Field(
-        None,
+        default=None,
         alias="maxMsgCountPerRead",
         title=(
             "MaxMsgCountPerRead holds the maximum number of messages per stream that will be read in each XREADGROUP"
             " of all streams\nExample: if there are 2 streams and MaxMsgCountPerRead=10, then each XREADGROUP may read"
             " upto a total of 20 messages.\nSame as COUNT option in XREADGROUP(https://redis.io/topics/streams-intro)."
             " Defaults to 10\n+optional"
         ),
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     password: Optional[v1.SecretKeySelector] = Field(
-        None, title="Password required for authentication if any.\n+optional"
+        default=None, title="Password required for authentication if any.\n+optional"
     )
     streams: Optional[List[str]] = Field(
-        None,
+        default=None,
         description="Streams to look for entries. XREADGROUP is used on all streams using a single consumer group.",
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the redis client.\n+optional")
-    username: Optional[str] = Field(None, title="Username required for ACL style authentication if any.\n+optional")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the redis client.\n+optional")
+    username: Optional[str] = Field(
+        default=None, title="Username required for ACL style authentication if any.\n+optional"
+    )
 
 
 class ResourceEventSource(BaseModel):
     event_types: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="eventTypes",
         description="EventTypes is the list of event type to watch.\nPossible values are - ADD, UPDATE and DELETE.",
     )
     filter: Optional[ResourceFilter] = Field(
-        None,
+        default=None,
         title=(
             "Filter is applied on the metadata of the resource\nIf you apply filter, then the internal event informer"
             " will only monitor objects that pass the filter.\n+optional"
         ),
     )
     group_version_resource: Optional[v1_1.GroupVersionResource] = Field(
-        None, alias="groupVersionResource", title="Group of the resource"
+        default=None, alias="groupVersionResource", title="Group of the resource"
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    namespace: Optional[str] = Field(None, title="Namespace where resource is deployed")
+    namespace: Optional[str] = Field(default=None, title="Namespace where resource is deployed")
 
 
 class SNSEventSource(BaseModel):
     access_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key"
+        default=None, alias="accessKey", title="AccessKey refers K8s secret containing aws access key"
     )
     endpoint: Optional[str] = Field(
-        None, title="Endpoint configures connection to a specific SNS endpoint instead of Amazons servers\n+optional"
+        default=None,
+        title="Endpoint configures connection to a specific SNS endpoint instead of Amazons servers\n+optional",
     )
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    region: Optional[str] = Field(None, title="Region is AWS region")
+    region: Optional[str] = Field(default=None, title="Region is AWS region")
     role_arn: Optional[str] = Field(
-        None, alias="roleARN", title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional"
+        default=None,
+        alias="roleARN",
+        title="RoleARN is the Amazon Resource Name (ARN) of the role to assume.\n+optional",
     )
     secret_key: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key"
+        default=None, alias="secretKey", title="SecretKey refers K8s secret containing aws secret key"
     )
-    topic_arn: Optional[str] = Field(None, alias="topicArn", title="TopicArn")
+    topic_arn: Optional[str] = Field(default=None, alias="topicArn", title="TopicArn")
     validate_signature: Optional[bool] = Field(
-        None,
+        default=None,
         alias="validateSignature",
         title="ValidateSignature is boolean that can be set to true for SNS signature verification\n+optional",
     )
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook configuration for http server")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook configuration for http server")
 
 
 class SecureHeader(BaseModel):
     name: Optional[str] = None
     value_from: Optional[ValueFromSource] = Field(
-        None, alias="valueFrom", title="Values can be read from either secrets or configmaps"
+        default=None, alias="valueFrom", title="Values can be read from either secrets or configmaps"
     )
 
 
 class SensorStatus(BaseModel):
     status: Optional[Status] = None
 
 
 class Service(BaseModel):
     cluster_ip: Optional[str] = Field(
-        None,
+        default=None,
         alias="clusterIP",
         title=(
             "clusterIP is the IP address of the service and is usually assigned\nrandomly by the master. If an address"
             " is specified manually and is not in\nuse by others, it will be allocated to the service; otherwise,"
             " creation\nof the service will fail. This field can not be changed through updates.\nValid values are"
             ' "None", empty string (""), or a valid IP address. "None"\ncan be specified for headless services when'
             " proxying is not required.\nMore info:"
             " https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies\n+optional"
         ),
     )
     ports: Optional[List[v1.ServicePort]] = Field(
-        None,
+        default=None,
         title=(
             "The list of ports that are exposed by this ClusterIP"
             " service.\n+patchMergeKey=port\n+patchStrategy=merge\n+listType=map\n+listMapKey=port\n+listMapKey=protocol"
         ),
     )
 
 
 class SlackEventSource(BaseModel):
-    filter: Optional[EventSourceFilter] = Field(None, title="Filter\n+optional")
+    filter: Optional[EventSourceFilter] = Field(default=None, title="Filter\n+optional")
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
     signing_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="signingSecret", title="Slack App signing secret"
+        default=None, alias="signingSecret", title="Slack App signing secret"
     )
-    token: Optional[v1.SecretKeySelector] = Field(None, title="Token for URL verification handshake")
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook holds configuration for a REST endpoint")
+    token: Optional[v1.SecretKeySelector] = Field(default=None, title="Token for URL verification handshake")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook holds configuration for a REST endpoint")
 
 
 class SlackTrigger(BaseModel):
     channel: Optional[str] = Field(
-        None, title="Channel refers to which Slack channel to send slack message.\n+optional"
+        default=None, title="Channel refers to which Slack channel to send slack message.\n+optional"
     )
     message: Optional[str] = Field(
-        None, title="Message refers to the message to send to the Slack channel.\n+optional"
+        default=None, title="Message refers to the message to send to the Slack channel.\n+optional"
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         title=(
             "Parameters is the list of key-value extracted from event's payload that are applied to\nthe trigger"
             " resource.\n+optional"
         ),
     )
     slack_token: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="slackToken",
         description="SlackToken refers to the Kubernetes secret that holds the slack token required to send messages.",
     )
 
 
 class StorageGridEventSource(BaseModel):
-    api_url: Optional[str] = Field(None, alias="apiURL", description="APIURL is the url of the storagegrid api.")
-    auth_token: Optional[v1.SecretKeySelector] = Field(None, alias="authToken", title="Auth token for storagegrid api")
-    bucket: Optional[str] = Field(None, description="Name of the bucket to register notifications for.")
+    api_url: Optional[str] = Field(
+        default=None, alias="apiURL", description="APIURL is the url of the storagegrid api."
+    )
+    auth_token: Optional[v1.SecretKeySelector] = Field(
+        default=None, alias="authToken", title="Auth token for storagegrid api"
+    )
+    bucket: Optional[str] = Field(default=None, description="Name of the bucket to register notifications for.")
     events: Optional[List[str]] = None
     filter: Optional[StorageGridFilter] = Field(
-        None, description="Filter on object key which caused the notification."
+        default=None, description="Filter on object key which caused the notification."
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    region: Optional[str] = Field(None, title="S3 region.\nDefaults to us-east-1\n+optional")
-    topic_arn: Optional[str] = Field(None, alias="topicArn", title="TopicArn")
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook holds configuration for a REST endpoint")
+    region: Optional[str] = Field(default=None, title="S3 region.\nDefaults to us-east-1\n+optional")
+    topic_arn: Optional[str] = Field(default=None, alias="topicArn", title="TopicArn")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook holds configuration for a REST endpoint")
 
 
 class StripeEventSource(BaseModel):
     api_key: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="apiKey",
         title=(
             "APIKey refers to K8s secret that holds Stripe API key. Used only if CreateWebhook is enabled.\n+optional"
         ),
     )
     create_webhook: Optional[bool] = Field(
-        None,
+        default=None,
         alias="createWebhook",
         title="CreateWebhook if specified creates a new webhook programmatically.\n+optional",
     )
     event_filter: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="eventFilter",
         title=(
             "EventFilter describes the type of events to listen to. If not specified, all types of events will be"
             " processed.\nMore info at https://stripe.com/docs/api/events/list\n+optional"
         ),
     )
     metadata: Optional[Dict[str, str]] = Field(
-        None, title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional"
+        default=None,
+        title="Metadata holds the user defined metadata which will passed along the event payload.\n+optional",
     )
-    webhook: Optional[WebhookContext] = Field(None, title="Webhook holds configuration for a REST endpoint")
+    webhook: Optional[WebhookContext] = Field(default=None, title="Webhook holds configuration for a REST endpoint")
 
 
 class ArtifactLocation(BaseModel):
-    configmap: Optional[v1.ConfigMapKeySelector] = Field(None, title="Configmap that stores the artifact")
-    file: Optional[FileArtifact] = Field(None, title="File artifact is artifact stored in a file")
-    git: Optional[GitArtifact] = Field(None, title="Git repository hosting the artifact")
-    inline: Optional[str] = Field(None, title="Inline artifact is embedded in sensor spec as a string")
-    resource: Optional[Resource] = Field(None, title="Resource is generic template for K8s resource")
-    s3: Optional[S3Artifact] = Field(None, title="S3 compliant artifact")
-    url: Optional[URLArtifact] = Field(None, title="URL to fetch the artifact from")
+    configmap: Optional[v1.ConfigMapKeySelector] = Field(default=None, title="Configmap that stores the artifact")
+    file: Optional[FileArtifact] = Field(default=None, title="File artifact is artifact stored in a file")
+    git: Optional[GitArtifact] = Field(default=None, title="Git repository hosting the artifact")
+    inline: Optional[str] = Field(default=None, title="Inline artifact is embedded in sensor spec as a string")
+    resource: Optional[Resource] = Field(default=None, title="Resource is generic template for K8s resource")
+    s3: Optional[S3Artifact] = Field(default=None, title="S3 compliant artifact")
+    url: Optional[URLArtifact] = Field(default=None, title="URL to fetch the artifact from")
 
 
 class EventDependency(BaseModel):
-    event_name: Optional[str] = Field(None, alias="eventName", title="EventName is the name of the event")
+    event_name: Optional[str] = Field(default=None, alias="eventName", title="EventName is the name of the event")
     event_source_name: Optional[str] = Field(
-        None, alias="eventSourceName", title="EventSourceName is the name of EventSource that Sensor depends on"
+        default=None,
+        alias="eventSourceName",
+        title="EventSourceName is the name of EventSource that Sensor depends on",
     )
     filters: Optional[EventDependencyFilter] = Field(
-        None,
+        default=None,
         title="Filters and rules governing toleration of success and constraints on the context and data of an event",
     )
     filters_logical_operator: Optional[str] = Field(
-        None,
+        default=None,
         alias="filtersLogicalOperator",
         description=(
             "FiltersLogicalOperator defines how different filters are evaluated together.\nAvailable values: and (&&),"
             " or (||)\nIs optional and if left blank treated as and (&&)."
         ),
     )
-    name: Optional[str] = Field(None, title="Name is a unique name of this dependency")
-    transform: Optional[EventDependencyTransformer] = Field(None, title="Transform transforms the event data")
+    name: Optional[str] = Field(default=None, title="Name is a unique name of this dependency")
+    transform: Optional[EventDependencyTransformer] = Field(default=None, title="Transform transforms the event data")
 
 
 class HTTPTrigger(BaseModel):
     basic_auth: Optional[BasicAuth] = Field(
-        None, alias="basicAuth", title="BasicAuth configuration for the http request.\n+optional"
+        default=None, alias="basicAuth", title="BasicAuth configuration for the http request.\n+optional"
     )
-    headers: Optional[Dict[str, str]] = Field(None, title="Headers for the HTTP request.\n+optional")
+    headers: Optional[Dict[str, str]] = Field(default=None, title="Headers for the HTTP request.\n+optional")
     method: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Method refers to the type of the HTTP request.\nRefer https://golang.org/src/net/http/method.go for more"
             " io.argoproj.workflow.v1alpha1.\nDefault value is POST.\n+optional"
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None,
+        default=None,
         description=(
             "Parameters is the list of key-value extracted from event's payload that are applied to\nthe HTTP trigger"
             " resource."
         ),
     )
     payload: Optional[List[TriggerParameter]] = None
     secure_headers: Optional[List[SecureHeader]] = Field(
-        None,
+        default=None,
         alias="secureHeaders",
         title="Secure Headers stored in Kubernetes Secrets for the HTTP requests.\n+optional",
     )
     timeout: Optional[str] = Field(
-        None, title="Timeout refers to the HTTP request timeout in seconds.\nDefault value is 60 seconds.\n+optional"
+        default=None,
+        title="Timeout refers to the HTTP request timeout in seconds.\nDefault value is 60 seconds.\n+optional",
     )
-    tls: Optional[TLSConfig] = Field(None, title="TLS configuration for the HTTP client.\n+optional")
-    url: Optional[str] = Field(None, description="URL refers to the URL to send HTTP request to.")
+    tls: Optional[TLSConfig] = Field(default=None, title="TLS configuration for the HTTP client.\n+optional")
+    url: Optional[str] = Field(default=None, description="URL refers to the URL to send HTTP request to.")
 
 
 class StandardK8STrigger(BaseModel):
     live_object: Optional[bool] = Field(
-        None,
+        default=None,
         alias="liveObject",
         title=(
             "LiveObject specifies whether the resource should be directly fetched from K8s instead\nof being marshaled"
             " from the resource artifact. If set to true, the resource artifact\nmust contain the information required"
             ' to uniquely identify the resource in the cluster,\nthat is, you must specify "apiVersion", "kind" as'
             ' well as "name" and "namespace" meta\ndata.\nOnly valid for operation type `update`\n+optional'
         ),
     )
     operation: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Operation refers to the type of operation performed on the k8s resource.\nDefault value is"
             " Create.\n+optional"
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None, description="Parameters is the list of parameters that is applied to resolved K8s trigger object."
+        default=None,
+        description="Parameters is the list of parameters that is applied to resolved K8s trigger object.",
     )
     patch_strategy: Optional[str] = Field(
-        None,
+        default=None,
         alias="patchStrategy",
         title=(
             "PatchStrategy controls the K8s object patching strategy when the trigger operation is specified as"
             " patch.\npossible"
             ' values:\n"application/json-patch+json"\n"application/merge-patch+json"\n"application/strategic-merge-patch+json"\n"application/apply-patch+yaml".\nDefaults'
             ' to "application/merge-patch+json"\n+optional'
         ),
     )
-    source: Optional[ArtifactLocation] = Field(None, title="Source of the K8s resource file(s)")
+    source: Optional[ArtifactLocation] = Field(default=None, title="Source of the K8s resource file(s)")
 
 
 class ArgoWorkflowTrigger(BaseModel):
-    args: Optional[List[str]] = Field(None, title="Args is the list of arguments to pass to the argo CLI")
+    args: Optional[List[str]] = Field(default=None, title="Args is the list of arguments to pass to the argo CLI")
     operation: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             "Operation refers to the type of operation performed on the argo workflow resource.\nDefault value is"
             " Submit.\n+optional"
         ),
     )
     parameters: Optional[List[TriggerParameter]] = Field(
-        None, title="Parameters is the list of parameters to pass to resolved Argo Workflow object"
+        default=None, title="Parameters is the list of parameters to pass to resolved Argo Workflow object"
     )
-    source: Optional[ArtifactLocation] = Field(None, title="Source of the K8s resource file(s)")
+    source: Optional[ArtifactLocation] = Field(default=None, title="Source of the K8s resource file(s)")
 
 
 class TriggerTemplate(BaseModel):
     argo_workflow: Optional[ArgoWorkflowTrigger] = Field(
-        None,
+        default=None,
         alias="argoWorkflow",
         title=(
             "ArgoWorkflow refers to the trigger that can perform various operations on an Argo"
             " io.argoproj.workflow.v1alpha1.\n+optional"
         ),
     )
     aws_lambda: Optional[AWSLambdaTrigger] = Field(
-        None,
+        default=None,
         alias="awsLambda",
         title=(
             "AWSLambda refers to the trigger designed to invoke AWS Lambda function with with on-the-fly constructable"
             " payload.\n+optional"
         ),
     )
     azure_event_hubs: Optional[AzureEventHubsTrigger] = Field(
-        None,
+        default=None,
         alias="azureEventHubs",
         title="AzureEventHubs refers to the trigger send an event to an Azure Event Hub.\n+optional",
     )
     conditions: Optional[str] = Field(
-        None,
+        default=None,
         title=(
             'Conditions is the conditions to execute the trigger.\nFor example: "(dep01 || dep02) && dep04"\n+optional'
         ),
     )
     conditions_reset: Optional[List[ConditionsResetCriteria]] = Field(
-        None, alias="conditionsReset", title="Criteria to reset the conditons\n+optional"
+        default=None, alias="conditionsReset", title="Criteria to reset the conditons\n+optional"
     )
     custom: Optional[CustomTrigger] = Field(
-        None,
+        default=None,
         title=(
             "CustomTrigger refers to the trigger designed to connect to a gRPC trigger server and execute a custom"
             " trigger.\n+optional"
         ),
     )
     http: Optional[HTTPTrigger] = Field(
-        None,
+        default=None,
         title=(
             "HTTP refers to the trigger designed to dispatch a HTTP request with on-the-fly constructable"
             " payload.\n+optional"
         ),
     )
     k8s: Optional[StandardK8STrigger] = Field(
-        None,
+        default=None,
         title=(
             "StandardK8STrigger refers to the trigger designed to create or update a generic Kubernetes"
             " resource.\n+optional"
         ),
     )
     kafka: Optional[KafkaTrigger] = Field(
-        None, description="Kafka refers to the trigger designed to place messages on Kafka topic.\n+optional."
+        default=None, description="Kafka refers to the trigger designed to place messages on Kafka topic.\n+optional."
     )
     log: Optional[LogTrigger] = Field(
-        None, title="Log refers to the trigger designed to invoke log the io.argoproj.workflow.v1alpha1.\n+optional"
+        default=None,
+        title="Log refers to the trigger designed to invoke log the io.argoproj.workflow.v1alpha1.\n+optional",
     )
-    name: Optional[str] = Field(None, description="Name is a unique name of the action to take.")
+    name: Optional[str] = Field(default=None, description="Name is a unique name of the action to take.")
     nats: Optional[NATSTrigger] = Field(
-        None, description="NATS refers to the trigger designed to place message on NATS subject.\n+optional."
+        default=None, description="NATS refers to the trigger designed to place message on NATS subject.\n+optional."
     )
     open_whisk: Optional[OpenWhiskTrigger] = Field(
-        None,
+        default=None,
         alias="openWhisk",
         title="OpenWhisk refers to the trigger designed to invoke OpenWhisk action.\n+optional",
     )
     pulsar: Optional[PulsarTrigger] = Field(
-        None, title="Pulsar refers to the trigger designed to place messages on Pulsar topic.\n+optional"
+        default=None, title="Pulsar refers to the trigger designed to place messages on Pulsar topic.\n+optional"
     )
     slack: Optional[SlackTrigger] = Field(
-        None, title="Slack refers to the trigger designed to send slack notification message.\n+optional"
+        default=None, title="Slack refers to the trigger designed to send slack notification message.\n+optional"
     )
 
 
 class Template(BaseModel):
-    affinity: Optional[v1.Affinity] = Field(None, title="If specified, the pod's scheduling constraints\n+optional")
+    affinity: Optional[v1.Affinity] = Field(
+        default=None, title="If specified, the pod's scheduling constraints\n+optional"
+    )
     container: Optional[v1.Container] = Field(
-        None, title="Container is the main container image to run in the sensor pod\n+optional"
+        default=None, title="Container is the main container image to run in the sensor pod\n+optional"
     )
     image_pull_secrets: Optional[List[v1.LocalObjectReference]] = Field(
-        None,
+        default=None,
         alias="imagePullSecrets",
         title=(
             "ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling"
             " any of the images used by this PodSpec.\nIf specified, these secrets will be passed to individual puller"
             " implementations for them to use. For example,\nin the case of docker, only DockerConfig type secrets are"
             " honored.\nMore info:"
             " https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod\n+optional\n+patchMergeKey=name\n+patchStrategy=merge"
         ),
     )
-    metadata: Optional[Metadata] = Field(None, title="Metadata sets the pods's metadata, i.e. annotations and labels")
+    metadata: Optional[Metadata] = Field(
+        default=None, title="Metadata sets the pods's metadata, i.e. annotations and labels"
+    )
     node_selector: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         alias="nodeSelector",
         title=(
             "NodeSelector is a selector which must be true for the pod to fit on a node.\nSelector which must match a"
             " node's labels for the pod to be scheduled on that node.\nMore info:"
             " https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n+optional"
         ),
     )
     priority: Optional[int] = Field(
-        None,
+        default=None,
         title=(
             "The priority value. Various system components use this field to find the\npriority of the EventSource"
             " pod. When Priority Admission Controller is enabled,\nit prevents users from setting this field. The"
             " admission controller populates\nthis field from PriorityClassName.\nThe higher the value, the higher the"
             " priority.\nMore info:"
             " https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n+optional"
         ),
     )
     priority_class_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="priorityClassName",
         title=(
             'If specified, indicates the EventSource pod\'s priority. "system-node-critical"\nand'
             ' "system-cluster-critical" are two special keywords which indicate the\nhighest priorities with the'
             " former being the highest priority. Any other\nname must be defined by creating a PriorityClass object"
             " with that name.\nIf not specified, the pod priority will be default or zero if there is"
             " no\ndefault.\nMore info:"
             " https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/\n+optional"
         ),
     )
     security_context: Optional[v1.PodSecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         title=(
             "SecurityContext holds pod-level security attributes and common container settings.\nOptional: Defaults to"
             " empty.  See type description for default values of each field.\n+optional"
         ),
     )
     service_account_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="serviceAccountName",
         title=(
             "ServiceAccountName is the name of the ServiceAccount to use to run sensor pod.\nMore info:"
             " https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\n+optional"
         ),
     )
-    tolerations: Optional[List[v1.Toleration]] = Field(None, title="If specified, the pod's tolerations.\n+optional")
+    tolerations: Optional[List[v1.Toleration]] = Field(
+        default=None, title="If specified, the pod's tolerations.\n+optional"
+    )
     volumes: Optional[List[v1.Volume]] = Field(
-        None,
+        default=None,
         title=(
             "Volumes is a list of volumes that can be mounted by containers in a"
             " io.argoproj.workflow.v1alpha1.\n+patchStrategy=merge\n+patchMergeKey=name\n+optional"
         ),
     )
 
 
 class Trigger(BaseModel):
     parameters: Optional[List[TriggerParameter]] = Field(
-        None, title="Parameters is the list of parameters applied to the trigger template definition"
+        default=None, title="Parameters is the list of parameters applied to the trigger template definition"
     )
     policy: Optional[TriggerPolicy] = Field(
-        None, title="Policy to configure backoff and execution criteria for the trigger\n+optional"
+        default=None, title="Policy to configure backoff and execution criteria for the trigger\n+optional"
     )
     rate_limit: Optional[RateLimit] = Field(
-        None, alias="rateLimit", title="Rate limit, default unit is Second\n+optional"
+        default=None, alias="rateLimit", title="Rate limit, default unit is Second\n+optional"
     )
     retry_strategy: Optional[Backoff] = Field(
-        None, alias="retryStrategy", title="Retry strategy, defaults to no retry\n+optional"
+        default=None, alias="retryStrategy", title="Retry strategy, defaults to no retry\n+optional"
+    )
+    template: Optional[TriggerTemplate] = Field(
+        default=None, description="Template describes the trigger specification."
     )
-    template: Optional[TriggerTemplate] = Field(None, description="Template describes the trigger specification.")
 
 
 class EventSourceSpec(BaseModel):
-    amqp: Optional[Dict[str, AMQPEventSource]] = Field(None, title="AMQP event sources")
+    amqp: Optional[Dict[str, AMQPEventSource]] = Field(default=None, title="AMQP event sources")
     azure_events_hub: Optional[Dict[str, AzureEventsHubEventSource]] = Field(
-        None, alias="azureEventsHub", title="AzureEventsHub event sources"
+        default=None, alias="azureEventsHub", title="AzureEventsHub event sources"
     )
-    bitbucket: Optional[Dict[str, BitbucketEventSource]] = Field(None, title="Bitbucket event sources")
+    bitbucket: Optional[Dict[str, BitbucketEventSource]] = Field(default=None, title="Bitbucket event sources")
     bitbucketserver: Optional[Dict[str, BitbucketServerEventSource]] = Field(
-        None, title="Bitbucket Server event sources"
+        default=None, title="Bitbucket Server event sources"
     )
-    calendar: Optional[Dict[str, CalendarEventSource]] = Field(None, title="Calendar event sources")
-    emitter: Optional[Dict[str, EmitterEventSource]] = Field(None, title="Emitter event source")
+    calendar: Optional[Dict[str, CalendarEventSource]] = Field(default=None, title="Calendar event sources")
+    emitter: Optional[Dict[str, EmitterEventSource]] = Field(default=None, title="Emitter event source")
     event_bus_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="eventBusName",
         title='EventBusName references to a EventBus name. By default the value is "default"',
     )
-    file: Optional[Dict[str, FileEventSource]] = Field(None, title="File event sources")
-    generic: Optional[Dict[str, GenericEventSource]] = Field(None, title="Generic event source")
-    github: Optional[Dict[str, GithubEventSource]] = Field(None, title="Github event sources")
-    gitlab: Optional[Dict[str, GitlabEventSource]] = Field(None, title="Gitlab event sources")
-    hdfs: Optional[Dict[str, HDFSEventSource]] = Field(None, title="HDFS event sources")
-    kafka: Optional[Dict[str, KafkaEventSource]] = Field(None, title="Kafka event sources")
-    minio: Optional[Dict[str, S3Artifact]] = Field(None, title="Minio event sources")
-    mqtt: Optional[Dict[str, MQTTEventSource]] = Field(None, title="MQTT event sources")
-    nats: Optional[Dict[str, NATSEventsSource]] = Field(None, title="NATS event sources")
-    nsq: Optional[Dict[str, NSQEventSource]] = Field(None, title="NSQ event source")
-    pub_sub: Optional[Dict[str, PubSubEventSource]] = Field(None, alias="pubSub", title="PubSub event sources")
-    pulsar: Optional[Dict[str, PulsarEventSource]] = Field(None, title="Pulsar event source")
-    redis: Optional[Dict[str, RedisEventSource]] = Field(None, title="Redis event source")
+    file: Optional[Dict[str, FileEventSource]] = Field(default=None, title="File event sources")
+    generic: Optional[Dict[str, GenericEventSource]] = Field(default=None, title="Generic event source")
+    github: Optional[Dict[str, GithubEventSource]] = Field(default=None, title="Github event sources")
+    gitlab: Optional[Dict[str, GitlabEventSource]] = Field(default=None, title="Gitlab event sources")
+    hdfs: Optional[Dict[str, HDFSEventSource]] = Field(default=None, title="HDFS event sources")
+    kafka: Optional[Dict[str, KafkaEventSource]] = Field(default=None, title="Kafka event sources")
+    minio: Optional[Dict[str, S3Artifact]] = Field(default=None, title="Minio event sources")
+    mqtt: Optional[Dict[str, MQTTEventSource]] = Field(default=None, title="MQTT event sources")
+    nats: Optional[Dict[str, NATSEventsSource]] = Field(default=None, title="NATS event sources")
+    nsq: Optional[Dict[str, NSQEventSource]] = Field(default=None, title="NSQ event source")
+    pub_sub: Optional[Dict[str, PubSubEventSource]] = Field(default=None, alias="pubSub", title="PubSub event sources")
+    pulsar: Optional[Dict[str, PulsarEventSource]] = Field(default=None, title="Pulsar event source")
+    redis: Optional[Dict[str, RedisEventSource]] = Field(default=None, title="Redis event source")
     redis_stream: Optional[Dict[str, RedisStreamEventSource]] = Field(
-        None, alias="redisStream", title="Redis stream source"
+        default=None, alias="redisStream", title="Redis stream source"
     )
-    replicas: Optional[int] = Field(None, title="Replicas is the event source deployment replicas")
-    resource: Optional[Dict[str, ResourceEventSource]] = Field(None, title="Resource event sources")
+    replicas: Optional[int] = Field(default=None, title="Replicas is the event source deployment replicas")
+    resource: Optional[Dict[str, ResourceEventSource]] = Field(default=None, title="Resource event sources")
     service: Optional[Service] = Field(
-        None, title="Service is the specifications of the service to expose the event source\n+optional"
+        default=None, title="Service is the specifications of the service to expose the event source\n+optional"
     )
-    slack: Optional[Dict[str, SlackEventSource]] = Field(None, title="Slack event sources")
-    sns: Optional[Dict[str, SNSEventSource]] = Field(None, title="SNS event sources")
-    sqs: Optional[Dict[str, SQSEventSource]] = Field(None, title="SQS event sources")
+    slack: Optional[Dict[str, SlackEventSource]] = Field(default=None, title="Slack event sources")
+    sns: Optional[Dict[str, SNSEventSource]] = Field(default=None, title="SNS event sources")
+    sqs: Optional[Dict[str, SQSEventSource]] = Field(default=None, title="SQS event sources")
     storage_grid: Optional[Dict[str, StorageGridEventSource]] = Field(
-        None, alias="storageGrid", title="StorageGrid event sources"
+        default=None, alias="storageGrid", title="StorageGrid event sources"
     )
-    stripe: Optional[Dict[str, StripeEventSource]] = Field(None, title="Stripe event sources")
+    stripe: Optional[Dict[str, StripeEventSource]] = Field(default=None, title="Stripe event sources")
     template: Optional[Template] = Field(
-        None, title="Template is the pod specification for the event source\n+optional"
+        default=None, title="Template is the pod specification for the event source\n+optional"
     )
-    webhook: Optional[Dict[str, WebhookEventSource]] = Field(None, title="Webhook event sources")
+    webhook: Optional[Dict[str, WebhookEventSource]] = Field(default=None, title="Webhook event sources")
 
 
 class SensorSpec(BaseModel):
     dependencies: Optional[List[EventDependency]] = Field(
-        None, description="Dependencies is a list of the events that this sensor is dependent on."
+        default=None, description="Dependencies is a list of the events that this sensor is dependent on."
     )
     error_on_failed_round: Optional[bool] = Field(
-        None,
+        default=None,
         alias="errorOnFailedRound",
         description=(
             "ErrorOnFailedRound if set to true, marks sensor state as `error` if the previous trigger round"
             " fails.\nOnce sensor state is set to `error`, no further triggers will be processed."
         ),
     )
     event_bus_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="eventBusName",
         title='EventBusName references to a EventBus name. By default the value is "default"',
     )
-    replicas: Optional[int] = Field(None, title="Replicas is the sensor deployment replicas")
-    template: Optional[Template] = Field(None, title="Template is the pod specification for the sensor\n+optional")
+    replicas: Optional[int] = Field(default=None, title="Replicas is the sensor deployment replicas")
+    template: Optional[Template] = Field(
+        default=None, title="Template is the pod specification for the sensor\n+optional"
+    )
     triggers: Optional[List[Trigger]] = Field(
-        None,
+        default=None,
         description=(
             "Triggers is a list of the things that this sensor evokes. These are the outputs from this sensor."
         ),
     )
 
 
 class EventSource(BaseModel):
     metadata: Optional[v1_1.ObjectMeta] = None
     spec: Optional[EventSourceSpec] = None
-    status: Optional[EventSourceStatus] = Field(None, title="+optional")
+    status: Optional[EventSourceStatus] = Field(default=None, title="+optional")
 
 
 class EventSourceList(BaseModel):
     items: Optional[List[EventSource]] = None
     metadata: Optional[v1_1.ListMeta] = None
 
 
 class Sensor(BaseModel):
     metadata: Optional[v1_1.ObjectMeta] = None
     spec: Optional[SensorSpec] = None
-    status: Optional[SensorStatus] = Field(None, title="+optional")
+    status: Optional[SensorStatus] = Field(default=None, title="+optional")
 
 
 class SensorList(BaseModel):
     items: Optional[List[Sensor]] = None
     metadata: Optional[v1_1.ListMeta] = None
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/events/v1alpha1.pyi` & `hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/events/v1alpha1.pyi`

 * *Files 0% similar despite different names*

```diff
@@ -1,13 +1,11 @@
-from typing import Dict, List, Optional
-
-from hera.shared._base_model import BaseModel as BaseModel
-
 from ...k8s.api.core import v1 as v1
 from ...k8s.apimachinery.pkg.apis.meta import v1 as v1_1
+from hera.shared._base_model import BaseModel as BaseModel
+from typing import Dict, List, Optional
 
 class AMQPConsumeConfig(BaseModel):
     auto_ack: Optional[bool]
     consumer_tag: Optional[str]
     exclusive: Optional[bool]
     no_local: Optional[bool]
     no_wait: Optional[bool]
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/workflow/v1alpha1.py` & `hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/workflow/v1alpha1.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,16 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from typing import Any, Dict, List, Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from ...k8s.api.core import v1
 from ...k8s.api.policy import v1beta1
 from ...k8s.apimachinery.pkg.apis.meta import v1 as v1_1
 from ...k8s.apimachinery.pkg.util import intstr
 
 
@@ -21,59 +20,61 @@
 
 class ArchivedWorkflowDeletedResponse(BaseModel):
     pass
 
 
 class ArtGCStatus(BaseModel):
     not_specified: Optional[bool] = Field(
-        None,
+        default=None,
         alias="notSpecified",
         description="if this is true, we already checked to see if we need to do it and we don't",
     )
     pods_recouped: Optional[Dict[str, bool]] = Field(
-        None,
+        default=None,
         alias="podsRecouped",
         description=(
             "have completed Pods been processed? (mapped by Pod name) used to prevent re-processing the Status of a"
             " Pod more than once"
         ),
     )
     strategies_processed: Optional[Dict[str, bool]] = Field(
-        None,
+        default=None,
         alias="strategiesProcessed",
         description=(
             "have Pods been started to perform this strategy? (enables us not to re-process what we've already done)"
         ),
     )
 
 
 class ArtifactRepositoryRef(BaseModel):
     config_map: Optional[str] = Field(
-        None, alias="configMap", description='The name of the config map. Defaults to "artifact-repositories".'
+        default=None, alias="configMap", description='The name of the config map. Defaults to "artifact-repositories".'
     )
     key: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'The config map key. Defaults to the value of the "workflows.argoproj.io/default-artifact-repository"'
             " annotation."
         ),
     )
 
 
 class ArtifactResult(BaseModel):
     error: Optional[str] = Field(
-        None, description="Error is an optional error message which should be set if Success==false"
+        default=None, description="Error is an optional error message which should be set if Success==false"
     )
     name: str = Field(..., description="Name is the name of the Artifact")
-    success: Optional[bool] = Field(None, description="Success describes whether the deletion succeeded")
+    success: Optional[bool] = Field(default=None, description="Success describes whether the deletion succeeded")
 
 
 class ArtifactResultNodeStatus(BaseModel):
     artifact_results: Optional[Dict[str, ArtifactResult]] = Field(
-        None, alias="artifactResults", description="ArtifactResults maps Artifact name to result of the deletion"
+        default=None,
+        alias="artifactResults",
+        description="ArtifactResults maps Artifact name to result of the deletion",
     )
 
 
 class ClusterWorkflowTemplateDeleteResponse(BaseModel):
     pass
 
 
@@ -82,31 +83,31 @@
 
 
 class CollectEventResponse(BaseModel):
     pass
 
 
 class Condition(BaseModel):
-    message: Optional[str] = Field(None, description="Message is the condition message")
-    status: Optional[str] = Field(None, description="Status is the status of the condition")
-    type: Optional[str] = Field(None, description="Type is the type of condition")
+    message: Optional[str] = Field(default=None, description="Message is the condition message")
+    status: Optional[str] = Field(default=None, description="Status is the status of the condition")
+    type: Optional[str] = Field(default=None, description="Type is the type of condition")
 
 
 class ContinueOn(BaseModel):
     error: Optional[bool] = None
     failed: Optional[bool] = None
 
 
 class Counter(BaseModel):
     value: str = Field(..., description="Value is the value of the metric")
 
 
 class CreateS3BucketOptions(BaseModel):
     object_locking: Optional[bool] = Field(
-        None, alias="objectLocking", description="ObjectLocking Enable object locking"
+        default=None, alias="objectLocking", description="ObjectLocking Enable object locking"
     )
 
 
 class CronWorkflowDeletedResponse(BaseModel):
     pass
 
 
@@ -132,32 +133,32 @@
 
 class EventResponse(BaseModel):
     pass
 
 
 class ExecutorConfig(BaseModel):
     service_account_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="serviceAccountName",
         description="ServiceAccountName specifies the service account name of the executor container.",
     )
 
 
 class Gauge(BaseModel):
     realtime: bool = Field(..., description="Realtime emits this metric in real time if applicable")
     value: str = Field(..., description="Value is the value of the metric")
 
 
 class GetUserInfoResponse(BaseModel):
     email: Optional[str] = None
-    email_verified: Optional[bool] = Field(None, alias="emailVerified")
+    email_verified: Optional[bool] = Field(default=None, alias="emailVerified")
     groups: Optional[List[str]] = None
     issuer: Optional[str] = None
-    service_account_name: Optional[str] = Field(None, alias="serviceAccountName")
-    service_account_namespace: Optional[str] = Field(None, alias="serviceAccountNamespace")
+    service_account_name: Optional[str] = Field(default=None, alias="serviceAccountName")
+    service_account_namespace: Optional[str] = Field(default=None, alias="serviceAccountNamespace")
     subject: Optional[str] = None
 
 
 class HTTPBodySource(BaseModel):
     bytes: Optional[str] = None
 
 
@@ -204,15 +205,15 @@
             ' "${io.argoproj.workflow.v1alpha1.metadata.annotations.userDefinedKey}"'
         ),
     )
 
 
 class LogEntry(BaseModel):
     content: Optional[str] = None
-    pod_name: Optional[str] = Field(None, alias="podName")
+    pod_name: Optional[str] = Field(default=None, alias="podName")
 
 
 class MemoizationStatus(BaseModel):
     cache_name: str = Field(..., alias="cacheName", description="Cache is the name of the cache that was used")
     hit: bool = Field(..., description="Hit indicates whether this node was created from a cache entry")
     key: str = Field(..., description="Key is the name of the key used for this node's cache")
 
@@ -224,84 +225,89 @@
 
 class MetricLabel(BaseModel):
     key: str
     value: str
 
 
 class Mutex(BaseModel):
-    name: Optional[str] = Field(None, description="name of the mutex")
+    name: Optional[str] = Field(default=None, description="name of the mutex")
 
 
 class MutexHolding(BaseModel):
     holder: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Holder is a reference to the object which holds the Mutex. Holding Scenario:\n  1. Current workflow's"
             " NodeID which is holding the lock.\n     e.g: ${NodeID}\nWaiting Scenario:\n  1. Current workflow or"
             " other workflow NodeID which is holding the lock.\n     e.g: ${WorkflowName}/${NodeID}"
         ),
     )
-    mutex: Optional[str] = Field(None, description="Reference for the mutex e.g: ${namespace}/mutex/${mutexName}")
+    mutex: Optional[str] = Field(
+        default=None, description="Reference for the mutex e.g: ${namespace}/mutex/${mutexName}"
+    )
 
 
 class MutexStatus(BaseModel):
     holding: Optional[List[MutexHolding]] = Field(
-        None,
+        default=None,
         description=(
             "Holding is a list of mutexes and their respective objects that are held by mutex lock for this"
             " io.argoproj.workflow.v1alpha1."
         ),
     )
     waiting: Optional[List[MutexHolding]] = Field(
-        None, description="Waiting is a list of mutexes and their respective objects this workflow is waiting for."
+        default=None,
+        description="Waiting is a list of mutexes and their respective objects this workflow is waiting for.",
     )
 
 
 class NodeSynchronizationStatus(BaseModel):
-    waiting: Optional[str] = Field(None, description="Waiting is the name of the lock that this node is waiting for")
+    waiting: Optional[str] = Field(
+        default=None, description="Waiting is the name of the lock that this node is waiting for"
+    )
 
 
 class NoneStrategy(BaseModel):
     pass
 
 
 class OAuth2EndpointParam(BaseModel):
     key: str = Field(..., description="Name is the header name")
-    value: Optional[str] = Field(None, description="Value is the literal value to use for the header")
+    value: Optional[str] = Field(default=None, description="Value is the literal value to use for the header")
 
 
 class OSSLifecycleRule(BaseModel):
     mark_deletion_after_days: Optional[int] = Field(
-        None,
+        default=None,
         alias="markDeletionAfterDays",
         description="MarkDeletionAfterDays is the number of days before we delete objects in the bucket",
     )
     mark_infrequent_access_after_days: Optional[int] = Field(
-        None,
+        default=None,
         alias="markInfrequentAccessAfterDays",
         description=(
             "MarkInfrequentAccessAfterDays is the number of days before we convert the objects in the bucket to"
             " Infrequent Access (IA) storage type"
         ),
     )
 
 
 class Plugin(BaseModel):
     pass
 
 
 class Prometheus(BaseModel):
-    counter: Optional[Counter] = Field(None, description="Counter is a counter metric")
-    gauge: Optional[Gauge] = Field(None, description="Gauge is a gauge metric")
+    counter: Optional[Counter] = Field(default=None, description="Counter is a counter metric")
+    gauge: Optional[Gauge] = Field(default=None, description="Gauge is a gauge metric")
     help: str = Field(..., description="Help is a string that describes the metric")
-    histogram: Optional[Histogram] = Field(None, description="Histogram is a histogram metric")
-    labels: Optional[List[MetricLabel]] = Field(None, description="Labels is a list of metric labels")
+    histogram: Optional[Histogram] = Field(default=None, description="Histogram is a histogram metric")
+    labels: Optional[List[MetricLabel]] = Field(default=None, description="Labels is a list of metric labels")
     name: str = Field(..., description="Name is the name of the metric")
     when: Optional[str] = Field(
-        None, description="When is a conditional statement that decides when to emit the metric"
+        default=None, description="When is a conditional statement that decides when to emit the metric"
     )
 
 
 class RawArtifact(BaseModel):
     data: str = Field(..., description="Data is the string contents of the artifact")
 
 
@@ -312,98 +318,101 @@
     parameters: Optional[List[str]] = None
     uid: Optional[str] = None
 
 
 class RetryArchivedWorkflowRequest(BaseModel):
     name: Optional[str] = None
     namespace: Optional[str] = None
-    node_field_selector: Optional[str] = Field(None, alias="nodeFieldSelector")
+    node_field_selector: Optional[str] = Field(default=None, alias="nodeFieldSelector")
     parameters: Optional[List[str]] = None
-    restart_successful: Optional[bool] = Field(None, alias="restartSuccessful")
+    restart_successful: Optional[bool] = Field(default=None, alias="restartSuccessful")
     uid: Optional[str] = None
 
 
 class RetryNodeAntiAffinity(BaseModel):
     pass
 
 
 class SemaphoreHolding(BaseModel):
     holders: Optional[List[str]] = Field(
-        None, description="Holders stores the list of current holder names in the io.argoproj.workflow.v1alpha1."
+        default=None,
+        description="Holders stores the list of current holder names in the io.argoproj.workflow.v1alpha1.",
     )
-    semaphore: Optional[str] = Field(None, description="Semaphore stores the semaphore name.")
+    semaphore: Optional[str] = Field(default=None, description="Semaphore stores the semaphore name.")
 
 
 class SemaphoreStatus(BaseModel):
     holding: Optional[List[SemaphoreHolding]] = Field(
-        None, description="Holding stores the list of resource acquired synchronization lock for workflows."
+        default=None, description="Holding stores the list of resource acquired synchronization lock for workflows."
     )
     waiting: Optional[List[SemaphoreHolding]] = Field(
-        None, description="Waiting indicates the list of current synchronization lock holders."
+        default=None, description="Waiting indicates the list of current synchronization lock holders."
     )
 
 
 class SuppliedValueFrom(BaseModel):
     pass
 
 
 class SuspendTemplate(BaseModel):
     duration: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Duration is the seconds to wait before automatically resuming a template. Must be a string. Default unit"
             ' is seconds. Could also be a Duration, e.g.: "2m", "6h", "1d"'
         ),
     )
 
 
 class SynchronizationStatus(BaseModel):
-    mutex: Optional[MutexStatus] = Field(None, description="Mutex stores this workflow's mutex holder details")
+    mutex: Optional[MutexStatus] = Field(default=None, description="Mutex stores this workflow's mutex holder details")
     semaphore: Optional[SemaphoreStatus] = Field(
-        None, description="Semaphore stores this workflow's Semaphore holder details"
+        default=None, description="Semaphore stores this workflow's Semaphore holder details"
     )
 
 
 class TTLStrategy(BaseModel):
     seconds_after_completion: Optional[int] = Field(
-        None,
+        default=None,
         alias="secondsAfterCompletion",
         description="SecondsAfterCompletion is the number of seconds to live after completion",
     )
     seconds_after_failure: Optional[int] = Field(
-        None,
+        default=None,
         alias="secondsAfterFailure",
         description="SecondsAfterFailure is the number of seconds to live after failure",
     )
     seconds_after_success: Optional[int] = Field(
-        None,
+        default=None,
         alias="secondsAfterSuccess",
         description="SecondsAfterSuccess is the number of seconds to live after success",
     )
 
 
 class TarStrategy(BaseModel):
     compression_level: Optional[int] = Field(
-        None,
+        default=None,
         alias="compressionLevel",
         description=(
             "CompressionLevel specifies the gzip compression level to use for the artifact. Defaults to"
             " gzip.DefaultCompression."
         ),
     )
 
 
 class TemplateRef(BaseModel):
     cluster_scope: Optional[bool] = Field(
-        None,
+        default=None,
         alias="clusterScope",
         description="ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate).",
     )
-    name: Optional[str] = Field(None, description="Name is the resource name of the template.")
-    template: Optional[str] = Field(None, description="Template is the name of referred template in the resource.")
+    name: Optional[str] = Field(default=None, description="Name is the resource name of the template.")
+    template: Optional[str] = Field(
+        default=None, description="Template is the name of referred template in the resource."
+    )
 
 
 class TransformationStep(BaseModel):
     expression: str = Field(..., description="Expression defines an expr expression to apply")
 
 
 class Version(BaseModel):
@@ -415,81 +424,81 @@
     go_version: str = Field(..., alias="goVersion")
     platform: str
     version: str
 
 
 class VolumeClaimGC(BaseModel):
     strategy: Optional[str] = Field(
-        None, description='Strategy is the strategy to use. One of "OnWorkflowCompletion", "OnWorkflowSuccess"'
+        default=None, description='Strategy is the strategy to use. One of "OnWorkflowCompletion", "OnWorkflowSuccess"'
     )
 
 
 class WorkflowDeleteResponse(BaseModel):
     pass
 
 
 class WorkflowMetadata(BaseModel):
     annotations: Optional[Dict[str, str]] = None
     labels: Optional[Dict[str, str]] = None
-    labels_from: Optional[Dict[str, LabelValueFrom]] = Field(None, alias="labelsFrom")
+    labels_from: Optional[Dict[str, LabelValueFrom]] = Field(default=None, alias="labelsFrom")
 
 
 class WorkflowResubmitRequest(BaseModel):
     memoized: Optional[bool] = None
     name: Optional[str] = None
     namespace: Optional[str] = None
     parameters: Optional[List[str]] = None
 
 
 class WorkflowResumeRequest(BaseModel):
     name: Optional[str] = None
     namespace: Optional[str] = None
-    node_field_selector: Optional[str] = Field(None, alias="nodeFieldSelector")
+    node_field_selector: Optional[str] = Field(default=None, alias="nodeFieldSelector")
 
 
 class WorkflowRetryRequest(BaseModel):
     name: Optional[str] = None
     namespace: Optional[str] = None
-    node_field_selector: Optional[str] = Field(None, alias="nodeFieldSelector")
+    node_field_selector: Optional[str] = Field(default=None, alias="nodeFieldSelector")
     parameters: Optional[List[str]] = None
-    restart_successful: Optional[bool] = Field(None, alias="restartSuccessful")
+    restart_successful: Optional[bool] = Field(default=None, alias="restartSuccessful")
 
 
 class WorkflowSetRequest(BaseModel):
     message: Optional[str] = None
     name: Optional[str] = None
     namespace: Optional[str] = None
-    node_field_selector: Optional[str] = Field(None, alias="nodeFieldSelector")
-    output_parameters: Optional[str] = Field(None, alias="outputParameters")
+    node_field_selector: Optional[str] = Field(default=None, alias="nodeFieldSelector")
+    output_parameters: Optional[str] = Field(default=None, alias="outputParameters")
     phase: Optional[str] = None
 
 
 class WorkflowStopRequest(BaseModel):
     message: Optional[str] = None
     name: Optional[str] = None
     namespace: Optional[str] = None
-    node_field_selector: Optional[str] = Field(None, alias="nodeFieldSelector")
+    node_field_selector: Optional[str] = Field(default=None, alias="nodeFieldSelector")
 
 
 class WorkflowSuspendRequest(BaseModel):
     name: Optional[str] = None
     namespace: Optional[str] = None
 
 
 class WorkflowTemplateDeleteResponse(BaseModel):
     pass
 
 
 class WorkflowTemplateRef(BaseModel):
     cluster_scope: Optional[bool] = Field(
-        None,
+        default=None,
         alias="clusterScope",
         description="ClusterScope indicates the referred template is cluster scoped (i.e. a ClusterWorkflowTemplate).",
     )
-    name: Optional[str] = Field(None, description="Name is the resource name of the workflow template.")
+    name: Optional[str] = Field(default=None, description="Name is the resource name of the workflow template.")
 
 
 class WorkflowTerminateRequest(BaseModel):
     name: Optional[str] = None
     namespace: Optional[str] = None
 
 
@@ -501,61 +510,71 @@
     none: Optional[NoneStrategy] = None
     tar: Optional[TarStrategy] = None
     zip: Optional[ZipStrategy] = None
 
 
 class ArtifactGC(BaseModel):
     pod_metadata: Optional[Metadata] = Field(
-        None,
+        default=None,
         alias="podMetadata",
         description=(
             "PodMetadata is an optional field for specifying the Labels and Annotations that should be assigned to the"
             " Pod doing the deletion"
         ),
     )
     service_account_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="serviceAccountName",
         description=(
             "ServiceAccountName is an optional field for specifying the Service Account that should be assigned to the"
             " Pod doing the deletion"
         ),
     )
-    strategy: Optional[str] = Field(None, description="Strategy is the strategy to use.")
+    strategy: Optional[str] = Field(default=None, description="Strategy is the strategy to use.")
 
 
 class ArtifactGCStatus(BaseModel):
     artifact_results_by_node: Optional[Dict[str, ArtifactResultNodeStatus]] = Field(
-        None, alias="artifactResultsByNode", description="ArtifactResultsByNode maps Node name to result"
+        default=None, alias="artifactResultsByNode", description="ArtifactResultsByNode maps Node name to result"
     )
 
 
 class ArtifactoryArtifact(BaseModel):
     password_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="passwordSecret", description="PasswordSecret is the secret selector to the repository password"
+        default=None,
+        alias="passwordSecret",
+        description="PasswordSecret is the secret selector to the repository password",
     )
     url: str = Field(..., description="URL of the artifact")
     username_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="usernameSecret", description="UsernameSecret is the secret selector to the repository username"
+        default=None,
+        alias="usernameSecret",
+        description="UsernameSecret is the secret selector to the repository username",
     )
 
 
 class ArtifactoryArtifactRepository(BaseModel):
     password_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="passwordSecret", description="PasswordSecret is the secret selector to the repository password"
+        default=None,
+        alias="passwordSecret",
+        description="PasswordSecret is the secret selector to the repository password",
+    )
+    repo_url: Optional[str] = Field(
+        default=None, alias="repoURL", description="RepoURL is the url for artifactory repo."
     )
-    repo_url: Optional[str] = Field(None, alias="repoURL", description="RepoURL is the url for artifactory repo.")
     username_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="usernameSecret", description="UsernameSecret is the secret selector to the repository username"
+        default=None,
+        alias="usernameSecret",
+        description="UsernameSecret is the secret selector to the repository username",
     )
 
 
 class AzureArtifact(BaseModel):
     account_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="accountKeySecret",
         description="AccountKeySecret is the secret selector to the Azure Blob Storage account access key",
     )
     blob: str = Field(
         ..., description="Blob is the blob name (i.e., path) in the container where the artifact resides"
     )
     container: str = Field(..., description="Container is the container where resources will be stored")
@@ -563,89 +582,93 @@
         ...,
         description=(
             "Endpoint is the service url associated with an account. It is most likely"
             ' "https://<ACCOUNT_NAME>.blob.core.windows.net"'
         ),
     )
     use_sdk_creds: Optional[bool] = Field(
-        None,
+        default=None,
         alias="useSDKCreds",
         description="UseSDKCreds tells the driver to figure out credentials based on sdk defaults.",
     )
 
 
 class AzureArtifactRepository(BaseModel):
     account_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="accountKeySecret",
         description="AccountKeySecret is the secret selector to the Azure Blob Storage account access key",
     )
     blob_name_format: Optional[str] = Field(
-        None,
+        default=None,
         alias="blobNameFormat",
         description=(
             "BlobNameFormat is defines the format of how to store blob names. Can reference workflow variables"
         ),
     )
     container: str = Field(..., description="Container is the container where resources will be stored")
     endpoint: str = Field(
         ...,
         description=(
             "Endpoint is the service url associated with an account. It is most likely"
             ' "https://<ACCOUNT_NAME>.blob.core.windows.net"'
         ),
     )
     use_sdk_creds: Optional[bool] = Field(
-        None,
+        default=None,
         alias="useSDKCreds",
         description="UseSDKCreds tells the driver to figure out credentials based on sdk defaults.",
     )
 
 
 class Backoff(BaseModel):
     duration: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Duration is the amount to back off. Default unit is seconds, but could also be a duration (e.g. "2m",'
             ' "1h")'
         ),
     )
     factor: Optional[intstr.IntOrString] = Field(
-        None, description="Factor is a factor to multiply the base duration after each failed retry"
+        default=None, description="Factor is a factor to multiply the base duration after each failed retry"
     )
     max_duration: Optional[str] = Field(
-        None,
+        default=None,
         alias="maxDuration",
         description="MaxDuration is the maximum amount of time allowed for the backoff strategy",
     )
 
 
 class BasicAuth(BaseModel):
     password_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="passwordSecret", description="PasswordSecret is the secret selector to the repository password"
+        default=None,
+        alias="passwordSecret",
+        description="PasswordSecret is the secret selector to the repository password",
     )
     username_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="usernameSecret", description="UsernameSecret is the secret selector to the repository username"
+        default=None,
+        alias="usernameSecret",
+        description="UsernameSecret is the secret selector to the repository username",
     )
 
 
 class Cache(BaseModel):
     config_map: v1.ConfigMapKeySelector = Field(
         ..., alias="configMap", description="ConfigMap sets a ConfigMap-based cache"
     )
 
 
 class ClientCertAuth(BaseModel):
-    client_cert_secret: Optional[v1.SecretKeySelector] = Field(None, alias="clientCertSecret")
-    client_key_secret: Optional[v1.SecretKeySelector] = Field(None, alias="clientKeySecret")
+    client_cert_secret: Optional[v1.SecretKeySelector] = Field(default=None, alias="clientCertSecret")
+    client_key_secret: Optional[v1.SecretKeySelector] = Field(default=None, alias="clientKeySecret")
 
 
 class ContainerSetRetryStrategy(BaseModel):
     duration: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Duration is the time between each retry, examples values are "300ms", "1s" or "5m". Valid time units are'
             ' "ns", "us" (or "s"), "ms", "s", "m", "h".'
         ),
     )
     retries: intstr.IntOrString = Field(..., description="Nbr of retries")
 
@@ -659,200 +682,210 @@
     )
     last_scheduled_time: v1_1.Time = Field(
         ..., alias="lastScheduledTime", description="LastScheduleTime is the last time the CronWorkflow was scheduled"
     )
 
 
 class GCSArtifact(BaseModel):
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     key: str = Field(..., description="Key is the path in the bucket where the artifact resides")
     service_account_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="serviceAccountKeySecret",
         description="ServiceAccountKeySecret is the secret selector to the bucket's service account key",
     )
 
 
 class GCSArtifactRepository(BaseModel):
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     key_format: Optional[str] = Field(
-        None,
+        default=None,
         alias="keyFormat",
         description="KeyFormat is defines the format of how to store keys. Can reference workflow variables",
     )
     service_account_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="serviceAccountKeySecret",
         description="ServiceAccountKeySecret is the secret selector to the bucket's service account key",
     )
 
 
 class GitArtifact(BaseModel):
-    branch: Optional[str] = Field(None, description="Branch is the branch to fetch when `SingleBranch` is enabled")
+    branch: Optional[str] = Field(
+        default=None, description="Branch is the branch to fetch when `SingleBranch` is enabled"
+    )
     depth: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Depth specifies clones/fetches should be shallow and include the given number of commits from the"
             " branch tip"
         ),
     )
     disable_submodules: Optional[bool] = Field(
-        None, alias="disableSubmodules", description="DisableSubmodules disables submodules during git clone"
+        default=None, alias="disableSubmodules", description="DisableSubmodules disables submodules during git clone"
     )
     fetch: Optional[List[str]] = Field(
-        None, description="Fetch specifies a number of refs that should be fetched before checkout"
+        default=None, description="Fetch specifies a number of refs that should be fetched before checkout"
     )
     insecure_ignore_host_key: Optional[bool] = Field(
-        None,
+        default=None,
         alias="insecureIgnoreHostKey",
         description="InsecureIgnoreHostKey disables SSH strict host key checking during git clone",
     )
     password_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="passwordSecret", description="PasswordSecret is the secret selector to the repository password"
+        default=None,
+        alias="passwordSecret",
+        description="PasswordSecret is the secret selector to the repository password",
     )
     repo: str = Field(..., description="Repo is the git repository")
-    revision: Optional[str] = Field(None, description="Revision is the git commit, tag, branch to checkout")
+    revision: Optional[str] = Field(default=None, description="Revision is the git commit, tag, branch to checkout")
     single_branch: Optional[bool] = Field(
-        None,
+        default=None,
         alias="singleBranch",
         description="SingleBranch enables single branch clone, using the `branch` parameter",
     )
     ssh_private_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="sshPrivateKeySecret",
         description="SSHPrivateKeySecret is the secret selector to the repository ssh private key",
     )
     username_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="usernameSecret", description="UsernameSecret is the secret selector to the repository username"
+        default=None,
+        alias="usernameSecret",
+        description="UsernameSecret is the secret selector to the repository username",
     )
 
 
 class HDFSArtifact(BaseModel):
-    addresses: Optional[List[str]] = Field(None, description="Addresses is accessible addresses of HDFS name nodes")
-    force: Optional[bool] = Field(None, description="Force copies a file forcibly even if it exists")
+    addresses: Optional[List[str]] = Field(
+        default=None, description="Addresses is accessible addresses of HDFS name nodes"
+    )
+    force: Optional[bool] = Field(default=None, description="Force copies a file forcibly even if it exists")
     hdfs_user: Optional[str] = Field(
-        None,
+        default=None,
         alias="hdfsUser",
         description=(
             "HDFSUser is the user to access HDFS file system. It is ignored if either ccache or keytab is used."
         ),
     )
     krb_c_cache_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbCCacheSecret",
         description=(
             "KrbCCacheSecret is the secret selector for Kerberos ccache Either ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_config_config_map: Optional[v1.ConfigMapKeySelector] = Field(
-        None,
+        default=None,
         alias="krbConfigConfigMap",
         description=(
             "KrbConfig is the configmap selector for Kerberos config as string It must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_keytab_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbKeytabSecret",
         description=(
             "KrbKeytabSecret is the secret selector for Kerberos keytab Either ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_realm: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbRealm",
         description="KrbRealm is the Kerberos realm used with Kerberos keytab It must be set if keytab is used.",
     )
     krb_service_principal_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbServicePrincipalName",
         description=(
             "KrbServicePrincipalName is the principal name of Kerberos service It must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_username: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbUsername",
         description="KrbUsername is the Kerberos username used with Kerberos keytab It must be set if keytab is used.",
     )
     path: str = Field(..., description="Path is a file path in HDFS")
 
 
 class HDFSArtifactRepository(BaseModel):
-    addresses: Optional[List[str]] = Field(None, description="Addresses is accessible addresses of HDFS name nodes")
-    force: Optional[bool] = Field(None, description="Force copies a file forcibly even if it exists")
+    addresses: Optional[List[str]] = Field(
+        default=None, description="Addresses is accessible addresses of HDFS name nodes"
+    )
+    force: Optional[bool] = Field(default=None, description="Force copies a file forcibly even if it exists")
     hdfs_user: Optional[str] = Field(
-        None,
+        default=None,
         alias="hdfsUser",
         description=(
             "HDFSUser is the user to access HDFS file system. It is ignored if either ccache or keytab is used."
         ),
     )
     krb_c_cache_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbCCacheSecret",
         description=(
             "KrbCCacheSecret is the secret selector for Kerberos ccache Either ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_config_config_map: Optional[v1.ConfigMapKeySelector] = Field(
-        None,
+        default=None,
         alias="krbConfigConfigMap",
         description=(
             "KrbConfig is the configmap selector for Kerberos config as string It must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_keytab_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="krbKeytabSecret",
         description=(
             "KrbKeytabSecret is the secret selector for Kerberos keytab Either ccache or keytab can be set to use"
             " Kerberos."
         ),
     )
     krb_realm: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbRealm",
         description="KrbRealm is the Kerberos realm used with Kerberos keytab It must be set if keytab is used.",
     )
     krb_service_principal_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbServicePrincipalName",
         description=(
             "KrbServicePrincipalName is the principal name of Kerberos service It must be set if either ccache or"
             " keytab is used."
         ),
     )
     krb_username: Optional[str] = Field(
-        None,
+        default=None,
         alias="krbUsername",
         description="KrbUsername is the Kerberos username used with Kerberos keytab It must be set if keytab is used.",
     )
     path_format: Optional[str] = Field(
-        None,
+        default=None,
         alias="pathFormat",
         description="PathFormat is defines the format of path to store a file. Can reference workflow variables",
     )
 
 
 class HTTPHeaderSource(BaseModel):
-    secret_key_ref: Optional[v1.SecretKeySelector] = Field(None, alias="secretKeyRef")
+    secret_key_ref: Optional[v1.SecretKeySelector] = Field(default=None, alias="secretKeyRef")
 
 
 class InfoResponse(BaseModel):
     links: Optional[List[Link]] = None
-    managed_namespace: Optional[str] = Field(None, alias="managedNamespace")
-    modals: Optional[Dict[str, bool]] = Field(None, title="which modals to show")
-    nav_color: Optional[str] = Field(None, alias="navColor")
+    managed_namespace: Optional[str] = Field(default=None, alias="managedNamespace")
+    modals: Optional[Dict[str, bool]] = Field(default=None, title="which modals to show")
+    nav_color: Optional[str] = Field(default=None, alias="navColor")
 
 
 class Memoize(BaseModel):
     cache: Cache = Field(..., description="Cache sets and configures the kind of cache")
     key: str = Field(..., description="Key is the key to use as the caching key")
     max_age: str = Field(
         ...,
@@ -865,953 +898,994 @@
 
 
 class Metrics(BaseModel):
     prometheus: List[Prometheus] = Field(..., description="Prometheus is a list of prometheus metrics to be emitted")
 
 
 class OAuth2Auth(BaseModel):
-    client_id_secret: Optional[v1.SecretKeySelector] = Field(None, alias="clientIDSecret")
-    client_secret_secret: Optional[v1.SecretKeySelector] = Field(None, alias="clientSecretSecret")
-    endpoint_params: Optional[List[OAuth2EndpointParam]] = Field(None, alias="endpointParams")
+    client_id_secret: Optional[v1.SecretKeySelector] = Field(default=None, alias="clientIDSecret")
+    client_secret_secret: Optional[v1.SecretKeySelector] = Field(default=None, alias="clientSecretSecret")
+    endpoint_params: Optional[List[OAuth2EndpointParam]] = Field(default=None, alias="endpointParams")
     scopes: Optional[List[str]] = None
-    token_url_secret: Optional[v1.SecretKeySelector] = Field(None, alias="tokenURLSecret")
+    token_url_secret: Optional[v1.SecretKeySelector] = Field(default=None, alias="tokenURLSecret")
 
 
 class OSSArtifact(BaseModel):
     access_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKeySecret", description="AccessKeySecret is the secret selector to the bucket's access key"
+        default=None,
+        alias="accessKeySecret",
+        description="AccessKeySecret is the secret selector to the bucket's access key",
     )
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     create_bucket_if_not_present: Optional[bool] = Field(
-        None,
+        default=None,
         alias="createBucketIfNotPresent",
         description=(
             "CreateBucketIfNotPresent tells the driver to attempt to create the OSS bucket for output artifacts, if it"
             " doesn't exist"
         ),
     )
-    endpoint: Optional[str] = Field(None, description="Endpoint is the hostname of the bucket endpoint")
+    endpoint: Optional[str] = Field(default=None, description="Endpoint is the hostname of the bucket endpoint")
     key: str = Field(..., description="Key is the path in the bucket where the artifact resides")
     lifecycle_rule: Optional[OSSLifecycleRule] = Field(
-        None, alias="lifecycleRule", description="LifecycleRule specifies how to manage bucket's lifecycle"
+        default=None, alias="lifecycleRule", description="LifecycleRule specifies how to manage bucket's lifecycle"
     )
     secret_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKeySecret", description="SecretKeySecret is the secret selector to the bucket's secret key"
+        default=None,
+        alias="secretKeySecret",
+        description="SecretKeySecret is the secret selector to the bucket's secret key",
     )
     security_token: Optional[str] = Field(
-        None,
+        default=None,
         alias="securityToken",
         description=(
             "SecurityToken is the user's temporary security token. For more details, check out:"
             " https://www.alibabacloud.com/help/doc-detail/100624.htm"
         ),
     )
 
 
 class OSSArtifactRepository(BaseModel):
     access_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKeySecret", description="AccessKeySecret is the secret selector to the bucket's access key"
+        default=None,
+        alias="accessKeySecret",
+        description="AccessKeySecret is the secret selector to the bucket's access key",
     )
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     create_bucket_if_not_present: Optional[bool] = Field(
-        None,
+        default=None,
         alias="createBucketIfNotPresent",
         description=(
             "CreateBucketIfNotPresent tells the driver to attempt to create the OSS bucket for output artifacts, if it"
             " doesn't exist"
         ),
     )
-    endpoint: Optional[str] = Field(None, description="Endpoint is the hostname of the bucket endpoint")
+    endpoint: Optional[str] = Field(default=None, description="Endpoint is the hostname of the bucket endpoint")
     key_format: Optional[str] = Field(
-        None,
+        default=None,
         alias="keyFormat",
         description="KeyFormat is defines the format of how to store keys. Can reference workflow variables",
     )
     lifecycle_rule: Optional[OSSLifecycleRule] = Field(
-        None, alias="lifecycleRule", description="LifecycleRule specifies how to manage bucket's lifecycle"
+        default=None, alias="lifecycleRule", description="LifecycleRule specifies how to manage bucket's lifecycle"
     )
     secret_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKeySecret", description="SecretKeySecret is the secret selector to the bucket's secret key"
+        default=None,
+        alias="secretKeySecret",
+        description="SecretKeySecret is the secret selector to the bucket's secret key",
     )
     security_token: Optional[str] = Field(
-        None,
+        default=None,
         alias="securityToken",
         description=(
             "SecurityToken is the user's temporary security token. For more details, check out:"
             " https://www.alibabacloud.com/help/doc-detail/100624.htm"
         ),
     )
 
 
 class RetryAffinity(BaseModel):
-    node_anti_affinity: Optional[RetryNodeAntiAffinity] = Field(None, alias="nodeAntiAffinity")
+    node_anti_affinity: Optional[RetryNodeAntiAffinity] = Field(default=None, alias="nodeAntiAffinity")
 
 
 class RetryStrategy(BaseModel):
     affinity: Optional[RetryAffinity] = Field(
-        None, description="Affinity prevents running workflow's step on the same host"
+        default=None, description="Affinity prevents running workflow's step on the same host"
     )
-    backoff: Optional[Backoff] = Field(None, description="Backoff is a backoff strategy")
+    backoff: Optional[Backoff] = Field(default=None, description="Backoff is a backoff strategy")
     expression: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Expression is a condition expression for when a node will be retried. If it evaluates to false, the node"
             " will not be retried and the retry strategy will be ignored"
         ),
     )
     limit: Optional[intstr.IntOrString] = Field(
-        None,
+        default=None,
         description=(
             "Limit is the maximum number of retry attempts when retrying a container. It does not include the original"
             " container; the maximum number of total attempts will be `limit + 1`."
         ),
     )
     retry_policy: Optional[str] = Field(
-        None, alias="retryPolicy", description="RetryPolicy is a policy of NodePhase statuses that will be retried"
+        default=None,
+        alias="retryPolicy",
+        description="RetryPolicy is a policy of NodePhase statuses that will be retried",
     )
 
 
 class S3EncryptionOptions(BaseModel):
     enable_encryption: Optional[bool] = Field(
-        None,
+        default=None,
         alias="enableEncryption",
         description=(
             "EnableEncryption tells the driver to encrypt objects if set to true. If kmsKeyId and"
             " serverSideCustomerKeySecret are not set, SSE-S3 will be used"
         ),
     )
     kms_encryption_context: Optional[str] = Field(
-        None,
+        default=None,
         alias="kmsEncryptionContext",
         description=(
             "KmsEncryptionContext is a json blob that contains an encryption context. See"
             " https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#encrypt_context for more information"
         ),
     )
     kms_key_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="kmsKeyId",
         description="KMSKeyId tells the driver to encrypt the object using the specified KMS Key.",
     )
     server_side_customer_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None,
+        default=None,
         alias="serverSideCustomerKeySecret",
         description=(
             "ServerSideCustomerKeySecret tells the driver to encrypt the output artifacts using SSE-C with the"
             " specified secret."
         ),
     )
 
 
 class SemaphoreRef(BaseModel):
     config_map_key_ref: Optional[v1.ConfigMapKeySelector] = Field(
-        None, alias="configMapKeyRef", description="ConfigMapKeyRef is configmap selector for Semaphore configuration"
+        default=None,
+        alias="configMapKeyRef",
+        description="ConfigMapKeyRef is configmap selector for Semaphore configuration",
     )
 
 
 class Sequence(BaseModel):
     count: Optional[intstr.IntOrString] = Field(
-        None, description="Count is number of elements in the sequence (default: 0). Not to be used with end"
+        default=None, description="Count is number of elements in the sequence (default: 0). Not to be used with end"
     )
     end: Optional[intstr.IntOrString] = Field(
-        None, description="Number at which to end the sequence (default: 0). Not to be used with Count"
+        default=None, description="Number at which to end the sequence (default: 0). Not to be used with Count"
     )
     format: Optional[str] = Field(
-        None, description="Format is a printf format string to format the value in the sequence"
+        default=None, description="Format is a printf format string to format the value in the sequence"
+    )
+    start: Optional[intstr.IntOrString] = Field(
+        default=None, description="Number at which to start the sequence (default: 0)"
     )
-    start: Optional[intstr.IntOrString] = Field(None, description="Number at which to start the sequence (default: 0)")
 
 
 class SubmitOpts(BaseModel):
-    annotations: Optional[str] = Field(None, description="Annotations adds to metadata.labels")
+    annotations: Optional[str] = Field(default=None, description="Annotations adds to metadata.labels")
     dry_run: Optional[bool] = Field(
-        None,
+        default=None,
         alias="dryRun",
         description=(
             "DryRun validates the workflow on the client-side without creating it. This option is not supported in API"
         ),
     )
-    entry_point: Optional[str] = Field(None, alias="entryPoint", description="Entrypoint overrides spec.entrypoint")
+    entry_point: Optional[str] = Field(
+        default=None, alias="entryPoint", description="Entrypoint overrides spec.entrypoint"
+    )
     generate_name: Optional[str] = Field(
-        None, alias="generateName", description="GenerateName overrides metadata.generateName"
+        default=None, alias="generateName", description="GenerateName overrides metadata.generateName"
     )
-    labels: Optional[str] = Field(None, description="Labels adds to metadata.labels")
-    name: Optional[str] = Field(None, description="Name overrides metadata.name")
+    labels: Optional[str] = Field(default=None, description="Labels adds to metadata.labels")
+    name: Optional[str] = Field(default=None, description="Name overrides metadata.name")
     owner_reference: Optional[v1_1.OwnerReference] = Field(
-        None, alias="ownerReference", description="OwnerReference creates a metadata.ownerReference"
+        default=None, alias="ownerReference", description="OwnerReference creates a metadata.ownerReference"
     )
-    parameters: Optional[List[str]] = Field(None, description="Parameters passes input parameters to workflow")
+    parameters: Optional[List[str]] = Field(default=None, description="Parameters passes input parameters to workflow")
     pod_priority_class_name: Optional[str] = Field(
-        None, alias="podPriorityClassName", description="Set the podPriorityClassName of the workflow"
+        default=None, alias="podPriorityClassName", description="Set the podPriorityClassName of the workflow"
     )
     priority: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Priority is used if controller is configured to process limited number of workflows in parallel, higher"
             " priority workflows are processed first."
         ),
     )
     server_dry_run: Optional[bool] = Field(
-        None,
+        default=None,
         alias="serverDryRun",
         description="ServerDryRun validates the workflow on the server-side without creating it",
     )
     service_account: Optional[str] = Field(
-        None,
+        default=None,
         alias="serviceAccount",
         description="ServiceAccount runs all pods in the workflow using specified ServiceAccount.",
     )
 
 
 class Synchronization(BaseModel):
-    mutex: Optional[Mutex] = Field(None, description="Mutex holds the Mutex lock details")
-    semaphore: Optional[SemaphoreRef] = Field(None, description="Semaphore holds the Semaphore configuration")
+    mutex: Optional[Mutex] = Field(default=None, description="Mutex holds the Mutex lock details")
+    semaphore: Optional[SemaphoreRef] = Field(default=None, description="Semaphore holds the Semaphore configuration")
 
 
 class ValueFrom(BaseModel):
     config_map_key_ref: Optional[v1.ConfigMapKeySelector] = Field(
-        None,
+        default=None,
         alias="configMapKeyRef",
         description="ConfigMapKeyRef is configmap selector for input parameter configuration",
     )
     default: Optional[str] = Field(
-        None,
+        default=None,
         description="Default specifies a value to be used if retrieving the value from the specified source fails",
     )
     event: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Selector (https://github.com/antonmedv/expr) that is evaluated against the event to get the value of the"
             " parameter. E.g. `payload.message`"
         ),
     )
     expression: Optional[str] = Field(
-        None, description="Expression, if defined, is evaluated to specify the value for the parameter"
+        default=None, description="Expression, if defined, is evaluated to specify the value for the parameter"
     )
     jq_filter: Optional[str] = Field(
-        None, alias="jqFilter", description="JQFilter expression against the resource object in resource templates"
+        default=None,
+        alias="jqFilter",
+        description="JQFilter expression against the resource object in resource templates",
     )
     json_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="jsonPath",
         description="JSONPath of a resource to retrieve an output parameter value from in resource templates",
     )
     parameter: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Parameter reference to a step or dag task in which to retrieve an output parameter value from (e.g."
             " '{{steps.mystep.outputs.myparam}}')"
         ),
     )
     path: Optional[str] = Field(
-        None, description="Path in the container to retrieve an output parameter value from in container templates"
+        default=None,
+        description="Path in the container to retrieve an output parameter value from in container templates",
     )
     supplied: Optional[SuppliedValueFrom] = Field(
-        None, description="Supplied value to be filled in directly, either through the CLI, API, etc."
+        default=None, description="Supplied value to be filled in directly, either through the CLI, API, etc."
     )
 
 
 class WorkflowSubmitRequest(BaseModel):
     namespace: Optional[str] = None
-    resource_kind: Optional[str] = Field(None, alias="resourceKind")
-    resource_name: Optional[str] = Field(None, alias="resourceName")
-    submit_options: Optional[SubmitOpts] = Field(None, alias="submitOptions")
+    resource_kind: Optional[str] = Field(default=None, alias="resourceKind")
+    resource_name: Optional[str] = Field(default=None, alias="resourceName")
+    submit_options: Optional[SubmitOpts] = Field(default=None, alias="submitOptions")
 
 
 class HTTPAuth(BaseModel):
-    basic_auth: Optional[BasicAuth] = Field(None, alias="basicAuth")
-    client_cert: Optional[ClientCertAuth] = Field(None, alias="clientCert")
+    basic_auth: Optional[BasicAuth] = Field(default=None, alias="basicAuth")
+    client_cert: Optional[ClientCertAuth] = Field(default=None, alias="clientCert")
     oauth2: Optional[OAuth2Auth] = None
 
 
 class HTTPHeader(BaseModel):
     name: str
     value: Optional[str] = None
-    value_from: Optional[HTTPHeaderSource] = Field(None, alias="valueFrom")
+    value_from: Optional[HTTPHeaderSource] = Field(default=None, alias="valueFrom")
 
 
 class Parameter(BaseModel):
     default: Optional[str] = Field(
-        None, description="Default is the default value to use for an input parameter if a value was not supplied"
+        default=None,
+        description="Default is the default value to use for an input parameter if a value was not supplied",
     )
-    description: Optional[str] = Field(None, description="Description is the parameter description")
+    description: Optional[str] = Field(default=None, description="Description is the parameter description")
     enum: Optional[List[str]] = Field(
-        None, description="Enum holds a list of string values to choose from, for the actual value of the parameter"
+        default=None,
+        description="Enum holds a list of string values to choose from, for the actual value of the parameter",
     )
     global_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="globalName",
         description=(
             "GlobalName exports an output parameter to the global scope, making it available as"
             " '{{io.argoproj.workflow.v1alpha1.outputs.parameters.XXXX}} and in workflow.status.outputs.parameters"
         ),
     )
     name: str = Field(..., description="Name is the parameter name")
     value: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Value is the literal value to use for the parameter. If specified in the context of an input parameter,"
             " the value takes precedence over any passed values"
         ),
     )
     value_from: Optional[ValueFrom] = Field(
-        None, alias="valueFrom", description="ValueFrom is the source for the output parameter's value"
+        default=None, alias="valueFrom", description="ValueFrom is the source for the output parameter's value"
     )
 
 
 class PodGC(BaseModel):
     label_selector: Optional[v1_1.LabelSelector] = Field(
-        None,
+        default=None,
         alias="labelSelector",
         description=(
             "LabelSelector is the label selector to check if the pods match the labels before being added to the pod"
             " GC queue."
         ),
     )
     strategy: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Strategy is the strategy to use. One of "OnPodCompletion", "OnPodSuccess", "OnWorkflowCompletion",'
             ' "OnWorkflowSuccess"'
         ),
     )
 
 
 class S3Artifact(BaseModel):
     access_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKeySecret", description="AccessKeySecret is the secret selector to the bucket's access key"
+        default=None,
+        alias="accessKeySecret",
+        description="AccessKeySecret is the secret selector to the bucket's access key",
     )
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     create_bucket_if_not_present: Optional[CreateS3BucketOptions] = Field(
-        None,
+        default=None,
         alias="createBucketIfNotPresent",
         description=(
             "CreateBucketIfNotPresent tells the driver to attempt to create the S3 bucket for output artifacts, if it"
             " doesn't exist. Setting Enabled Encryption will apply either SSE-S3 to the bucket if KmsKeyId is not set"
             " or SSE-KMS if it is."
         ),
     )
-    encryption_options: Optional[S3EncryptionOptions] = Field(None, alias="encryptionOptions")
-    endpoint: Optional[str] = Field(None, description="Endpoint is the hostname of the bucket endpoint")
-    insecure: Optional[bool] = Field(None, description="Insecure will connect to the service with TLS")
-    key: Optional[str] = Field(None, description="Key is the key in the bucket where the artifact resides")
-    region: Optional[str] = Field(None, description="Region contains the optional bucket region")
+    encryption_options: Optional[S3EncryptionOptions] = Field(default=None, alias="encryptionOptions")
+    endpoint: Optional[str] = Field(default=None, description="Endpoint is the hostname of the bucket endpoint")
+    insecure: Optional[bool] = Field(default=None, description="Insecure will connect to the service with TLS")
+    key: Optional[str] = Field(default=None, description="Key is the key in the bucket where the artifact resides")
+    region: Optional[str] = Field(default=None, description="Region contains the optional bucket region")
     role_arn: Optional[str] = Field(
-        None, alias="roleARN", description="RoleARN is the Amazon Resource Name (ARN) of the role to assume."
+        default=None, alias="roleARN", description="RoleARN is the Amazon Resource Name (ARN) of the role to assume."
     )
     secret_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKeySecret", description="SecretKeySecret is the secret selector to the bucket's secret key"
+        default=None,
+        alias="secretKeySecret",
+        description="SecretKeySecret is the secret selector to the bucket's secret key",
     )
     use_sdk_creds: Optional[bool] = Field(
-        None,
+        default=None,
         alias="useSDKCreds",
         description="UseSDKCreds tells the driver to figure out credentials based on sdk defaults.",
     )
 
 
 class S3ArtifactRepository(BaseModel):
     access_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="accessKeySecret", description="AccessKeySecret is the secret selector to the bucket's access key"
+        default=None,
+        alias="accessKeySecret",
+        description="AccessKeySecret is the secret selector to the bucket's access key",
     )
-    bucket: Optional[str] = Field(None, description="Bucket is the name of the bucket")
+    bucket: Optional[str] = Field(default=None, description="Bucket is the name of the bucket")
     create_bucket_if_not_present: Optional[CreateS3BucketOptions] = Field(
-        None,
+        default=None,
         alias="createBucketIfNotPresent",
         description=(
             "CreateBucketIfNotPresent tells the driver to attempt to create the S3 bucket for output artifacts, if it"
             " doesn't exist. Setting Enabled Encryption will apply either SSE-S3 to the bucket if KmsKeyId is not set"
             " or SSE-KMS if it is."
         ),
     )
-    encryption_options: Optional[S3EncryptionOptions] = Field(None, alias="encryptionOptions")
-    endpoint: Optional[str] = Field(None, description="Endpoint is the hostname of the bucket endpoint")
-    insecure: Optional[bool] = Field(None, description="Insecure will connect to the service with TLS")
+    encryption_options: Optional[S3EncryptionOptions] = Field(default=None, alias="encryptionOptions")
+    endpoint: Optional[str] = Field(default=None, description="Endpoint is the hostname of the bucket endpoint")
+    insecure: Optional[bool] = Field(default=None, description="Insecure will connect to the service with TLS")
     key_format: Optional[str] = Field(
-        None,
+        default=None,
         alias="keyFormat",
         description="KeyFormat is defines the format of how to store keys. Can reference workflow variables",
     )
     key_prefix: Optional[str] = Field(
-        None,
+        default=None,
         alias="keyPrefix",
         description=(
             "KeyPrefix is prefix used as part of the bucket key in which the controller will store artifacts."
             " DEPRECATED. Use KeyFormat instead"
         ),
     )
-    region: Optional[str] = Field(None, description="Region contains the optional bucket region")
+    region: Optional[str] = Field(default=None, description="Region contains the optional bucket region")
     role_arn: Optional[str] = Field(
-        None, alias="roleARN", description="RoleARN is the Amazon Resource Name (ARN) of the role to assume."
+        default=None, alias="roleARN", description="RoleARN is the Amazon Resource Name (ARN) of the role to assume."
     )
     secret_key_secret: Optional[v1.SecretKeySelector] = Field(
-        None, alias="secretKeySecret", description="SecretKeySecret is the secret selector to the bucket's secret key"
+        default=None,
+        alias="secretKeySecret",
+        description="SecretKeySecret is the secret selector to the bucket's secret key",
     )
     use_sdk_creds: Optional[bool] = Field(
-        None,
+        default=None,
         alias="useSDKCreds",
         description="UseSDKCreds tells the driver to figure out credentials based on sdk defaults.",
     )
 
 
 class ArtifactRepository(BaseModel):
-    archive_logs: Optional[bool] = Field(None, alias="archiveLogs", description="ArchiveLogs enables log archiving")
+    archive_logs: Optional[bool] = Field(
+        default=None, alias="archiveLogs", description="ArchiveLogs enables log archiving"
+    )
     artifactory: Optional[ArtifactoryArtifactRepository] = Field(
-        None, description="Artifactory stores artifacts to JFrog Artifactory"
+        default=None, description="Artifactory stores artifacts to JFrog Artifactory"
     )
     azure: Optional[AzureArtifactRepository] = Field(
-        None, description="Azure stores artifact in an Azure Storage account"
+        default=None, description="Azure stores artifact in an Azure Storage account"
     )
-    gcs: Optional[GCSArtifactRepository] = Field(None, description="GCS stores artifact in a GCS object store")
-    hdfs: Optional[HDFSArtifactRepository] = Field(None, description="HDFS stores artifacts in HDFS")
+    gcs: Optional[GCSArtifactRepository] = Field(default=None, description="GCS stores artifact in a GCS object store")
+    hdfs: Optional[HDFSArtifactRepository] = Field(default=None, description="HDFS stores artifacts in HDFS")
     oss: Optional[OSSArtifactRepository] = Field(
-        None, description="OSS stores artifact in a OSS-compliant object store"
+        default=None, description="OSS stores artifact in a OSS-compliant object store"
+    )
+    s3: Optional[S3ArtifactRepository] = Field(
+        default=None, description="S3 stores artifact in a S3-compliant object store"
     )
-    s3: Optional[S3ArtifactRepository] = Field(None, description="S3 stores artifact in a S3-compliant object store")
 
 
 class ArtifactRepositoryRefStatus(BaseModel):
     artifact_repository: Optional[ArtifactRepository] = Field(
-        None,
+        default=None,
         alias="artifactRepository",
         description="The repository the workflow will use. This maybe empty before v3.1.",
     )
     config_map: Optional[str] = Field(
-        None, alias="configMap", description='The name of the config map. Defaults to "artifact-repositories".'
+        default=None, alias="configMap", description='The name of the config map. Defaults to "artifact-repositories".'
     )
     default: Optional[bool] = Field(
-        None, description="If this ref represents the default artifact repository, rather than a config map."
+        default=None, description="If this ref represents the default artifact repository, rather than a config map."
     )
     key: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'The config map key. Defaults to the value of the "workflows.argoproj.io/default-artifact-repository"'
             " annotation."
         ),
     )
     namespace: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "The namespace of the config map. Defaults to the workflow's namespace, or the controller's namespace (if"
             " found)."
         ),
     )
 
 
 class HTTP(BaseModel):
-    body: Optional[str] = Field(None, description="Body is content of the HTTP Request")
+    body: Optional[str] = Field(default=None, description="Body is content of the HTTP Request")
     body_from: Optional[HTTPBodySource] = Field(
-        None, alias="bodyFrom", description="BodyFrom is  content of the HTTP Request as Bytes"
+        default=None, alias="bodyFrom", description="BodyFrom is  content of the HTTP Request as Bytes"
     )
     headers: Optional[List[HTTPHeader]] = Field(
-        None, description="Headers are an optional list of headers to send with HTTP requests"
+        default=None, description="Headers are an optional list of headers to send with HTTP requests"
     )
     insecure_skip_verify: Optional[bool] = Field(
-        None,
+        default=None,
         alias="insecureSkipVerify",
         description="InsecureSkipVerify is a bool when if set to true will skip TLS verification for the HTTP client",
     )
-    method: Optional[str] = Field(None, description="Method is HTTP methods for HTTP Request")
+    method: Optional[str] = Field(default=None, description="Method is HTTP methods for HTTP Request")
     success_condition: Optional[str] = Field(
-        None,
+        default=None,
         alias="successCondition",
         description="SuccessCondition is an expression if evaluated to true is considered successful",
     )
     timeout_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="timeoutSeconds",
         description="TimeoutSeconds is request timeout for HTTP Request. Default is 30 seconds",
     )
     url: str = Field(..., description="URL of the HTTP Request")
 
 
 class HTTPArtifact(BaseModel):
-    auth: Optional[HTTPAuth] = Field(None, description="Auth contains information for client authentication")
+    auth: Optional[HTTPAuth] = Field(default=None, description="Auth contains information for client authentication")
     headers: Optional[List[Header]] = Field(
-        None, description="Headers are an optional list of headers to send with HTTP requests for artifacts"
+        default=None, description="Headers are an optional list of headers to send with HTTP requests for artifacts"
     )
     url: str = Field(..., description="URL of the artifact")
 
 
 class Artifact(BaseModel):
     archive: Optional[ArchiveStrategy] = Field(
-        None, description="Archive controls how the artifact will be saved to the artifact repository."
+        default=None, description="Archive controls how the artifact will be saved to the artifact repository."
     )
     archive_logs: Optional[bool] = Field(
-        None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
+        default=None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
     )
     artifact_gc: Optional[ArtifactGC] = Field(
-        None,
+        default=None,
         alias="artifactGC",
         description=(
             "ArtifactGC describes the strategy to use when to deleting an artifact from completed or deleted workflows"
         ),
     )
     artifactory: Optional[ArtifactoryArtifact] = Field(
-        None, description="Artifactory contains artifactory artifact location details"
+        default=None, description="Artifactory contains artifactory artifact location details"
     )
-    azure: Optional[AzureArtifact] = Field(None, description="Azure contains Azure Storage artifact location details")
-    deleted: Optional[bool] = Field(None, description="Has this been deleted?")
+    azure: Optional[AzureArtifact] = Field(
+        default=None, description="Azure contains Azure Storage artifact location details"
+    )
+    deleted: Optional[bool] = Field(default=None, description="Has this been deleted?")
     from_: Optional[str] = Field(
-        None, alias="from", description="From allows an artifact to reference an artifact from a previous step"
+        default=None, alias="from", description="From allows an artifact to reference an artifact from a previous step"
     )
     from_expression: Optional[str] = Field(
-        None,
+        default=None,
         alias="fromExpression",
         description="FromExpression, if defined, is evaluated to specify the value for the artifact",
     )
-    gcs: Optional[GCSArtifact] = Field(None, description="GCS contains GCS artifact location details")
-    git: Optional[GitArtifact] = Field(None, description="Git contains git artifact location details")
+    gcs: Optional[GCSArtifact] = Field(default=None, description="GCS contains GCS artifact location details")
+    git: Optional[GitArtifact] = Field(default=None, description="Git contains git artifact location details")
     global_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="globalName",
         description=(
             "GlobalName exports an output artifact to the global scope, making it available as"
             " '{{io.argoproj.workflow.v1alpha1.outputs.artifacts.XXXX}} and in workflow.status.outputs.artifacts"
         ),
     )
-    hdfs: Optional[HDFSArtifact] = Field(None, description="HDFS contains HDFS artifact location details")
-    http: Optional[HTTPArtifact] = Field(None, description="HTTP contains HTTP artifact location details")
+    hdfs: Optional[HDFSArtifact] = Field(default=None, description="HDFS contains HDFS artifact location details")
+    http: Optional[HTTPArtifact] = Field(default=None, description="HTTP contains HTTP artifact location details")
     mode: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "mode bits to use on this file, must be a value between 0 and 0777 set when loading input artifacts."
         ),
     )
     name: str = Field(..., description="name of the artifact. must be unique within a template's inputs/outputs.")
     optional: Optional[bool] = Field(
-        None, description="Make Artifacts optional, if Artifacts doesn't generate or exist"
+        default=None, description="Make Artifacts optional, if Artifacts doesn't generate or exist"
     )
-    oss: Optional[OSSArtifact] = Field(None, description="OSS contains OSS artifact location details")
-    path: Optional[str] = Field(None, description="Path is the container path to the artifact")
-    raw: Optional[RawArtifact] = Field(None, description="Raw contains raw artifact location details")
+    oss: Optional[OSSArtifact] = Field(default=None, description="OSS contains OSS artifact location details")
+    path: Optional[str] = Field(default=None, description="Path is the container path to the artifact")
+    raw: Optional[RawArtifact] = Field(default=None, description="Raw contains raw artifact location details")
     recurse_mode: Optional[bool] = Field(
-        None,
+        default=None,
         alias="recurseMode",
         description="If mode is set, apply the permission recursively into the artifact if it is a folder",
     )
-    s3: Optional[S3Artifact] = Field(None, description="S3 contains S3 artifact location details")
+    s3: Optional[S3Artifact] = Field(default=None, description="S3 contains S3 artifact location details")
     sub_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="subPath",
         description="SubPath allows an artifact to be sourced from a subpath within the specified source",
     )
 
 
 class ArtifactLocation(BaseModel):
     archive_logs: Optional[bool] = Field(
-        None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
+        default=None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
     )
     artifactory: Optional[ArtifactoryArtifact] = Field(
-        None, description="Artifactory contains artifactory artifact location details"
+        default=None, description="Artifactory contains artifactory artifact location details"
+    )
+    azure: Optional[AzureArtifact] = Field(
+        default=None, description="Azure contains Azure Storage artifact location details"
     )
-    azure: Optional[AzureArtifact] = Field(None, description="Azure contains Azure Storage artifact location details")
-    gcs: Optional[GCSArtifact] = Field(None, description="GCS contains GCS artifact location details")
-    git: Optional[GitArtifact] = Field(None, description="Git contains git artifact location details")
-    hdfs: Optional[HDFSArtifact] = Field(None, description="HDFS contains HDFS artifact location details")
-    http: Optional[HTTPArtifact] = Field(None, description="HTTP contains HTTP artifact location details")
-    oss: Optional[OSSArtifact] = Field(None, description="OSS contains OSS artifact location details")
-    raw: Optional[RawArtifact] = Field(None, description="Raw contains raw artifact location details")
-    s3: Optional[S3Artifact] = Field(None, description="S3 contains S3 artifact location details")
+    gcs: Optional[GCSArtifact] = Field(default=None, description="GCS contains GCS artifact location details")
+    git: Optional[GitArtifact] = Field(default=None, description="Git contains git artifact location details")
+    hdfs: Optional[HDFSArtifact] = Field(default=None, description="HDFS contains HDFS artifact location details")
+    http: Optional[HTTPArtifact] = Field(default=None, description="HTTP contains HTTP artifact location details")
+    oss: Optional[OSSArtifact] = Field(default=None, description="OSS contains OSS artifact location details")
+    raw: Optional[RawArtifact] = Field(default=None, description="Raw contains raw artifact location details")
+    s3: Optional[S3Artifact] = Field(default=None, description="S3 contains S3 artifact location details")
 
 
 class ArtifactNodeSpec(BaseModel):
     archive_location: Optional[ArtifactLocation] = Field(
-        None,
+        default=None,
         alias="archiveLocation",
         description="ArchiveLocation is the template-level Artifact location specification",
     )
     artifacts: Optional[Dict[str, Artifact]] = Field(
-        None, description="Artifacts maps artifact name to Artifact description"
+        default=None, description="Artifacts maps artifact name to Artifact description"
     )
 
 
 class ArtifactPaths(BaseModel):
     archive: Optional[ArchiveStrategy] = Field(
-        None, description="Archive controls how the artifact will be saved to the artifact repository."
+        default=None, description="Archive controls how the artifact will be saved to the artifact repository."
     )
     archive_logs: Optional[bool] = Field(
-        None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
+        default=None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
     )
     artifact_gc: Optional[ArtifactGC] = Field(
-        None,
+        default=None,
         alias="artifactGC",
         description=(
             "ArtifactGC describes the strategy to use when to deleting an artifact from completed or deleted workflows"
         ),
     )
     artifactory: Optional[ArtifactoryArtifact] = Field(
-        None, description="Artifactory contains artifactory artifact location details"
+        default=None, description="Artifactory contains artifactory artifact location details"
     )
-    azure: Optional[AzureArtifact] = Field(None, description="Azure contains Azure Storage artifact location details")
-    deleted: Optional[bool] = Field(None, description="Has this been deleted?")
+    azure: Optional[AzureArtifact] = Field(
+        default=None, description="Azure contains Azure Storage artifact location details"
+    )
+    deleted: Optional[bool] = Field(default=None, description="Has this been deleted?")
     from_: Optional[str] = Field(
-        None, alias="from", description="From allows an artifact to reference an artifact from a previous step"
+        default=None, alias="from", description="From allows an artifact to reference an artifact from a previous step"
     )
     from_expression: Optional[str] = Field(
-        None,
+        default=None,
         alias="fromExpression",
         description="FromExpression, if defined, is evaluated to specify the value for the artifact",
     )
-    gcs: Optional[GCSArtifact] = Field(None, description="GCS contains GCS artifact location details")
-    git: Optional[GitArtifact] = Field(None, description="Git contains git artifact location details")
+    gcs: Optional[GCSArtifact] = Field(default=None, description="GCS contains GCS artifact location details")
+    git: Optional[GitArtifact] = Field(default=None, description="Git contains git artifact location details")
     global_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="globalName",
         description=(
             "GlobalName exports an output artifact to the global scope, making it available as"
             " '{{io.argoproj.workflow.v1alpha1.outputs.artifacts.XXXX}} and in workflow.status.outputs.artifacts"
         ),
     )
-    hdfs: Optional[HDFSArtifact] = Field(None, description="HDFS contains HDFS artifact location details")
-    http: Optional[HTTPArtifact] = Field(None, description="HTTP contains HTTP artifact location details")
+    hdfs: Optional[HDFSArtifact] = Field(default=None, description="HDFS contains HDFS artifact location details")
+    http: Optional[HTTPArtifact] = Field(default=None, description="HTTP contains HTTP artifact location details")
     mode: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "mode bits to use on this file, must be a value between 0 and 0777 set when loading input artifacts."
         ),
     )
     name: str = Field(..., description="name of the artifact. must be unique within a template's inputs/outputs.")
     optional: Optional[bool] = Field(
-        None, description="Make Artifacts optional, if Artifacts doesn't generate or exist"
+        default=None, description="Make Artifacts optional, if Artifacts doesn't generate or exist"
     )
-    oss: Optional[OSSArtifact] = Field(None, description="OSS contains OSS artifact location details")
-    path: Optional[str] = Field(None, description="Path is the container path to the artifact")
-    raw: Optional[RawArtifact] = Field(None, description="Raw contains raw artifact location details")
+    oss: Optional[OSSArtifact] = Field(default=None, description="OSS contains OSS artifact location details")
+    path: Optional[str] = Field(default=None, description="Path is the container path to the artifact")
+    raw: Optional[RawArtifact] = Field(default=None, description="Raw contains raw artifact location details")
     recurse_mode: Optional[bool] = Field(
-        None,
+        default=None,
         alias="recurseMode",
         description="If mode is set, apply the permission recursively into the artifact if it is a folder",
     )
-    s3: Optional[S3Artifact] = Field(None, description="S3 contains S3 artifact location details")
+    s3: Optional[S3Artifact] = Field(default=None, description="S3 contains S3 artifact location details")
     sub_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="subPath",
         description="SubPath allows an artifact to be sourced from a subpath within the specified source",
     )
 
 
 class ContainerNode(BaseModel):
     args: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Arguments to the entrypoint. The container image's CMD is used if this is not provided. Variable"
             " references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be"
             " resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $,"
             ' which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal'
             ' "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable exists or'
             " not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     command: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Entrypoint array. Not executed within a shell. The container image's ENTRYPOINT is used if this is not"
             " provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable"
             " cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a"
             ' single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string'
             ' literal "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable'
             " exists or not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     dependencies: Optional[List[str]] = None
     env: Optional[List[v1.EnvVar]] = Field(
-        None, description="List of environment variables to set in the container. Cannot be updated."
+        default=None, description="List of environment variables to set in the container. Cannot be updated."
     )
     env_from: Optional[List[v1.EnvFromSource]] = Field(
-        None,
+        default=None,
         alias="envFrom",
         description=(
             "List of sources to populate environment variables in the container. The keys defined within a source must"
             " be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a"
             " key exists in multiple sources, the value associated with the last source will take precedence. Values"
             " defined by an Env with a duplicate key will take precedence. Cannot be updated."
         ),
     )
     image: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Container image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is"
             " optional to allow higher level config management to default or override container images in workload"
             " controllers like Deployments and StatefulSets."
         ),
     )
     image_pull_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="imagePullPolicy",
         description=(
             "Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or"
             " IfNotPresent otherwise. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/containers/images#updating-images"
         ),
     )
     lifecycle: Optional[v1.Lifecycle] = Field(
-        None,
+        default=None,
         description=(
             "Actions that the management system should take in response to container lifecycle events. Cannot be"
             " updated."
         ),
     )
     liveness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="livenessProbe",
         description=(
             "Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated."
             " More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     name: str = Field(
         ...,
         description=(
             "Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name"
             " (DNS_LABEL). Cannot be updated."
         ),
     )
     ports: Optional[List[v1.ContainerPort]] = Field(
-        None,
+        default=None,
         description=(
             "List of ports to expose from the container. Exposing a port here gives the system additional information"
             " about the network connections a container uses, but is primarily informational. Not specifying a port"
             ' here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0"'
             " address inside a container will be accessible from the network. Cannot be updated."
         ),
     )
     readiness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="readinessProbe",
         description=(
             "Periodic probe of container service readiness. Container will be removed from service endpoints if the"
             " probe fails. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     resources: Optional[v1.ResourceRequirements] = Field(
-        None,
+        default=None,
         description=(
             "Compute Resources required by this container. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
     security_context: Optional[v1.SecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext defines the security options the container should be run with. If set, the fields of"
             " SecurityContext override the equivalent fields of PodSecurityContext. More info:"
             " https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"
         ),
     )
     startup_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="startupProbe",
         description=(
             "StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are"
             " executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if"
             " the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a"
             " Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state"
             " operation. This cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     stdin: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a buffer for stdin in the container runtime. If this is not set,"
             " reads from stdin in the container will always result in EOF. Default is false."
         ),
     )
     stdin_once: Optional[bool] = Field(
-        None,
+        default=None,
         alias="stdinOnce",
         description=(
             "Whether the container runtime should close the stdin channel after it has been opened by a single attach."
             " When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is"
             " set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and"
             " then remains open and accepts data until the client disconnects, at which time stdin is closed and"
             " remains closed until the container is restarted. If this flag is false, a container processes that reads"
             " from stdin will never receive an EOF. Default is false"
         ),
     )
     termination_message_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePath",
         description=(
             "Optional: Path at which the file to which the container's termination message will be written is mounted"
             " into the container's filesystem. Message written is intended to be brief final status, such as an"
             " assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message"
             " length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be"
             " updated."
         ),
     )
     termination_message_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePolicy",
         description=(
             "Indicate how the termination message should be populated. File will use the contents of"
             " terminationMessagePath to populate the container status message on both success and failure."
             " FallbackToLogsOnError will use the last chunk of container log output if the termination message file is"
             " empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines,"
             " whichever is smaller. Defaults to File. Cannot be updated."
         ),
     )
     tty: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is"
             " false."
         ),
     )
     volume_devices: Optional[List[v1.VolumeDevice]] = Field(
-        None,
+        default=None,
         alias="volumeDevices",
         description="volumeDevices is the list of block devices to be used by the container.",
     )
     volume_mounts: Optional[List[v1.VolumeMount]] = Field(
-        None,
+        default=None,
         alias="volumeMounts",
         description="Pod volumes to mount into the container's filesystem. Cannot be updated.",
     )
     working_dir: Optional[str] = Field(
-        None,
+        default=None,
         alias="workingDir",
         description=(
             "Container's working directory. If not specified, the container runtime's default will be used, which"
             " might be configured in the container image. Cannot be updated."
         ),
     )
 
 
 class ContainerSetTemplate(BaseModel):
     containers: List[ContainerNode]
     retry_strategy: Optional[ContainerSetRetryStrategy] = Field(
-        None,
+        default=None,
         alias="retryStrategy",
         description=(
             "RetryStrategy describes how to retry a container nodes in the container set if it fails. Nbr of"
             " retries(default 0) and sleep duration between retries(default 0s, instant retry) can be set."
         ),
     )
-    volume_mounts: Optional[List[v1.VolumeMount]] = Field(None, alias="volumeMounts")
+    volume_mounts: Optional[List[v1.VolumeMount]] = Field(default=None, alias="volumeMounts")
 
 
 class DataSource(BaseModel):
     artifact_paths: Optional[ArtifactPaths] = Field(
-        None,
+        default=None,
         alias="artifactPaths",
         description="ArtifactPaths is a data transformation that collects a list of artifact paths",
     )
 
 
 class Inputs(BaseModel):
-    artifacts: Optional[List[Artifact]] = Field(None, description="Artifact are a list of artifacts passed as inputs")
+    artifacts: Optional[List[Artifact]] = Field(
+        default=None, description="Artifact are a list of artifacts passed as inputs"
+    )
     parameters: Optional[List[Parameter]] = Field(
-        None, description="Parameters are a list of parameters passed as inputs"
+        default=None, description="Parameters are a list of parameters passed as inputs"
     )
 
 
 class ManifestFrom(BaseModel):
     artifact: Artifact = Field(..., description="Artifact contains the artifact to use")
 
 
 class Outputs(BaseModel):
     artifacts: Optional[List[Artifact]] = Field(
-        None, description="Artifacts holds the list of output artifacts produced by a step"
+        default=None, description="Artifacts holds the list of output artifacts produced by a step"
     )
     exit_code: Optional[str] = Field(
-        None, alias="exitCode", description="ExitCode holds the exit code of a script template"
+        default=None, alias="exitCode", description="ExitCode holds the exit code of a script template"
     )
     parameters: Optional[List[Parameter]] = Field(
-        None, description="Parameters holds the list of output parameters produced by a step"
+        default=None, description="Parameters holds the list of output parameters produced by a step"
     )
-    result: Optional[str] = Field(None, description="Result holds the result (stdout) of a script template")
+    result: Optional[str] = Field(default=None, description="Result holds the result (stdout) of a script template")
 
 
 class ResourceTemplate(BaseModel):
     action: str = Field(
         ...,
         description=(
             "Action is the action to perform to the resource. Must be one of: get, create, apply, delete, replace,"
             " patch"
         ),
     )
     failure_condition: Optional[str] = Field(
-        None,
+        default=None,
         alias="failureCondition",
         description=(
             "FailureCondition is a label selector expression which describes the conditions of the k8s resource in"
             " which the step was considered failed"
         ),
     )
     flags: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Flags is a set of additional options passed to kubectl before submitting a resource I.e. to disable"
             ' resource validation: flags: [\n\t"--validate=false"  # disable resource validation\n]'
         ),
     )
-    manifest: Optional[str] = Field(None, description="Manifest contains the kubernetes manifest")
+    manifest: Optional[str] = Field(default=None, description="Manifest contains the kubernetes manifest")
     manifest_from: Optional[ManifestFrom] = Field(
-        None, alias="manifestFrom", description="ManifestFrom is the source for a single kubernetes manifest"
+        default=None, alias="manifestFrom", description="ManifestFrom is the source for a single kubernetes manifest"
     )
     merge_strategy: Optional[str] = Field(
-        None,
+        default=None,
         alias="mergeStrategy",
         description=(
             'MergeStrategy is the strategy used to merge a patch. It defaults to "strategic" Must be one of:'
             " strategic, merge, json"
         ),
     )
     set_owner_reference: Optional[bool] = Field(
-        None,
+        default=None,
         alias="setOwnerReference",
         description=(
             "SetOwnerReference sets the reference to the workflow on the OwnerReference of generated resource."
         ),
     )
     success_condition: Optional[str] = Field(
-        None,
+        default=None,
         alias="successCondition",
         description=(
             "SuccessCondition is a label selector expression which describes the conditions of the k8s resource in"
             " which it is acceptable to proceed to the following step"
         ),
     )
 
 
 class ScriptTemplate(BaseModel):
     args: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Arguments to the entrypoint. The container image's CMD is used if this is not provided. Variable"
             " references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be"
             " resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $,"
             ' which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal'
             ' "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable exists or'
             " not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     command: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Entrypoint array. Not executed within a shell. The container image's ENTRYPOINT is used if this is not"
             " provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable"
             " cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a"
             ' single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string'
             ' literal "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable'
             " exists or not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     env: Optional[List[v1.EnvVar]] = Field(
-        None, description="List of environment variables to set in the container. Cannot be updated."
+        default=None, description="List of environment variables to set in the container. Cannot be updated."
     )
     env_from: Optional[List[v1.EnvFromSource]] = Field(
-        None,
+        default=None,
         alias="envFrom",
         description=(
             "List of sources to populate environment variables in the container. The keys defined within a source must"
             " be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a"
             " key exists in multiple sources, the value associated with the last source will take precedence. Values"
             " defined by an Env with a duplicate key will take precedence. Cannot be updated."
         ),
@@ -1821,231 +1895,231 @@
         description=(
             "Container image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is"
             " optional to allow higher level config management to default or override container images in workload"
             " controllers like Deployments and StatefulSets."
         ),
     )
     image_pull_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="imagePullPolicy",
         description=(
             "Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or"
             " IfNotPresent otherwise. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/containers/images#updating-images"
         ),
     )
     lifecycle: Optional[v1.Lifecycle] = Field(
-        None,
+        default=None,
         description=(
             "Actions that the management system should take in response to container lifecycle events. Cannot be"
             " updated."
         ),
     )
     liveness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="livenessProbe",
         description=(
             "Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated."
             " More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name"
             " (DNS_LABEL). Cannot be updated."
         ),
     )
     ports: Optional[List[v1.ContainerPort]] = Field(
-        None,
+        default=None,
         description=(
             "List of ports to expose from the container. Exposing a port here gives the system additional information"
             " about the network connections a container uses, but is primarily informational. Not specifying a port"
             ' here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0"'
             " address inside a container will be accessible from the network. Cannot be updated."
         ),
     )
     readiness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="readinessProbe",
         description=(
             "Periodic probe of container service readiness. Container will be removed from service endpoints if the"
             " probe fails. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     resources: Optional[v1.ResourceRequirements] = Field(
-        None,
+        default=None,
         description=(
             "Compute Resources required by this container. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
     security_context: Optional[v1.SecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext defines the security options the container should be run with. If set, the fields of"
             " SecurityContext override the equivalent fields of PodSecurityContext. More info:"
             " https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"
         ),
     )
     source: str = Field(..., description="Source contains the source code of the script to execute")
     startup_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="startupProbe",
         description=(
             "StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are"
             " executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if"
             " the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a"
             " Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state"
             " operation. This cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     stdin: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a buffer for stdin in the container runtime. If this is not set,"
             " reads from stdin in the container will always result in EOF. Default is false."
         ),
     )
     stdin_once: Optional[bool] = Field(
-        None,
+        default=None,
         alias="stdinOnce",
         description=(
             "Whether the container runtime should close the stdin channel after it has been opened by a single attach."
             " When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is"
             " set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and"
             " then remains open and accepts data until the client disconnects, at which time stdin is closed and"
             " remains closed until the container is restarted. If this flag is false, a container processes that reads"
             " from stdin will never receive an EOF. Default is false"
         ),
     )
     termination_message_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePath",
         description=(
             "Optional: Path at which the file to which the container's termination message will be written is mounted"
             " into the container's filesystem. Message written is intended to be brief final status, such as an"
             " assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message"
             " length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be"
             " updated."
         ),
     )
     termination_message_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePolicy",
         description=(
             "Indicate how the termination message should be populated. File will use the contents of"
             " terminationMessagePath to populate the container status message on both success and failure."
             " FallbackToLogsOnError will use the last chunk of container log output if the termination message file is"
             " empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines,"
             " whichever is smaller. Defaults to File. Cannot be updated."
         ),
     )
     tty: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is"
             " false."
         ),
     )
     volume_devices: Optional[List[v1.VolumeDevice]] = Field(
-        None,
+        default=None,
         alias="volumeDevices",
         description="volumeDevices is the list of block devices to be used by the container.",
     )
     volume_mounts: Optional[List[v1.VolumeMount]] = Field(
-        None,
+        default=None,
         alias="volumeMounts",
         description="Pod volumes to mount into the container's filesystem. Cannot be updated.",
     )
     working_dir: Optional[str] = Field(
-        None,
+        default=None,
         alias="workingDir",
         description=(
             "Container's working directory. If not specified, the container runtime's default will be used, which"
             " might be configured in the container image. Cannot be updated."
         ),
     )
 
 
 class UserContainer(BaseModel):
     args: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Arguments to the entrypoint. The container image's CMD is used if this is not provided. Variable"
             " references $(VAR_NAME) are expanded using the container's environment. If a variable cannot be"
             " resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $,"
             ' which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal'
             ' "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable exists or'
             " not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     command: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Entrypoint array. Not executed within a shell. The container image's ENTRYPOINT is used if this is not"
             " provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable"
             " cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a"
             ' single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string'
             ' literal "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable'
             " exists or not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     env: Optional[List[v1.EnvVar]] = Field(
-        None, description="List of environment variables to set in the container. Cannot be updated."
+        default=None, description="List of environment variables to set in the container. Cannot be updated."
     )
     env_from: Optional[List[v1.EnvFromSource]] = Field(
-        None,
+        default=None,
         alias="envFrom",
         description=(
             "List of sources to populate environment variables in the container. The keys defined within a source must"
             " be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a"
             " key exists in multiple sources, the value associated with the last source will take precedence. Values"
             " defined by an Env with a duplicate key will take precedence. Cannot be updated."
         ),
     )
     image: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Container image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is"
             " optional to allow higher level config management to default or override container images in workload"
             " controllers like Deployments and StatefulSets."
         ),
     )
     image_pull_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="imagePullPolicy",
         description=(
             "Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or"
             " IfNotPresent otherwise. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/containers/images#updating-images"
         ),
     )
     lifecycle: Optional[v1.Lifecycle] = Field(
-        None,
+        default=None,
         description=(
             "Actions that the management system should take in response to container lifecycle events. Cannot be"
             " updated."
         ),
     )
     liveness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="livenessProbe",
         description=(
             "Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated."
             " More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     mirror_volume_mounts: Optional[bool] = Field(
-        None,
+        default=None,
         alias="mirrorVolumeMounts",
         description=(
             "MirrorVolumeMounts will mount the same volumes specified in the main container to the container"
             " (including artifacts), at the same mountPaths. This enables dind daemon to partially see the same"
             " filesystem as the main container in order to use features such as docker volume binding"
         ),
     )
@@ -2053,223 +2127,228 @@
         ...,
         description=(
             "Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name"
             " (DNS_LABEL). Cannot be updated."
         ),
     )
     ports: Optional[List[v1.ContainerPort]] = Field(
-        None,
+        default=None,
         description=(
             "List of ports to expose from the container. Exposing a port here gives the system additional information"
             " about the network connections a container uses, but is primarily informational. Not specifying a port"
             ' here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0"'
             " address inside a container will be accessible from the network. Cannot be updated."
         ),
     )
     readiness_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="readinessProbe",
         description=(
             "Periodic probe of container service readiness. Container will be removed from service endpoints if the"
             " probe fails. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     resources: Optional[v1.ResourceRequirements] = Field(
-        None,
+        default=None,
         description=(
             "Compute Resources required by this container. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
     security_context: Optional[v1.SecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext defines the security options the container should be run with. If set, the fields of"
             " SecurityContext override the equivalent fields of PodSecurityContext. More info:"
             " https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"
         ),
     )
     startup_probe: Optional[v1.Probe] = Field(
-        None,
+        default=None,
         alias="startupProbe",
         description=(
             "StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are"
             " executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if"
             " the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a"
             " Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state"
             " operation. This cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     stdin: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a buffer for stdin in the container runtime. If this is not set,"
             " reads from stdin in the container will always result in EOF. Default is false."
         ),
     )
     stdin_once: Optional[bool] = Field(
-        None,
+        default=None,
         alias="stdinOnce",
         description=(
             "Whether the container runtime should close the stdin channel after it has been opened by a single attach."
             " When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is"
             " set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and"
             " then remains open and accepts data until the client disconnects, at which time stdin is closed and"
             " remains closed until the container is restarted. If this flag is false, a container processes that reads"
             " from stdin will never receive an EOF. Default is false"
         ),
     )
     termination_message_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePath",
         description=(
             "Optional: Path at which the file to which the container's termination message will be written is mounted"
             " into the container's filesystem. Message written is intended to be brief final status, such as an"
             " assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message"
             " length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be"
             " updated."
         ),
     )
     termination_message_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePolicy",
         description=(
             "Indicate how the termination message should be populated. File will use the contents of"
             " terminationMessagePath to populate the container status message on both success and failure."
             " FallbackToLogsOnError will use the last chunk of container log output if the termination message file is"
             " empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines,"
             " whichever is smaller. Defaults to File. Cannot be updated."
         ),
     )
     tty: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is"
             " false."
         ),
     )
     volume_devices: Optional[List[v1.VolumeDevice]] = Field(
-        None,
+        default=None,
         alias="volumeDevices",
         description="volumeDevices is the list of block devices to be used by the container.",
     )
     volume_mounts: Optional[List[v1.VolumeMount]] = Field(
-        None,
+        default=None,
         alias="volumeMounts",
         description="Pod volumes to mount into the container's filesystem. Cannot be updated.",
     )
     working_dir: Optional[str] = Field(
-        None,
+        default=None,
         alias="workingDir",
         description=(
             "Container's working directory. If not specified, the container runtime's default will be used, which"
             " might be configured in the container image. Cannot be updated."
         ),
     )
 
 
 class Arguments(BaseModel):
     artifacts: Optional[List[Artifact]] = Field(
-        None, description="Artifacts is the list of artifacts to pass to the template or workflow"
+        default=None, description="Artifacts is the list of artifacts to pass to the template or workflow"
     )
     parameters: Optional[List[Parameter]] = Field(
-        None, description="Parameters is the list of parameters to pass to the template or workflow"
+        default=None, description="Parameters is the list of parameters to pass to the template or workflow"
     )
 
 
 class ArtifactGCSpec(BaseModel):
     artifacts_by_node: Optional[Dict[str, ArtifactNodeSpec]] = Field(
-        None,
+        default=None,
         alias="artifactsByNode",
         description="ArtifactsByNode maps Node name to information pertaining to Artifacts on that Node",
     )
 
 
 class Data(BaseModel):
     source: DataSource = Field(..., description="Source sources external data into a data template")
     transformation: List[TransformationStep] = Field(
         ..., description="Transformation applies a set of transformations"
     )
 
 
 class LifecycleHook(BaseModel):
-    arguments: Optional[Arguments] = Field(None, description="Arguments hold arguments to the template")
+    arguments: Optional[Arguments] = Field(default=None, description="Arguments hold arguments to the template")
     expression: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Expression is a condition expression for when a node will be retried. If it evaluates to false, the node"
             " will not be retried and the retry strategy will be ignored"
         ),
     )
-    template: Optional[str] = Field(None, description="Template is the name of the template to execute by the hook")
+    template: Optional[str] = Field(
+        default=None, description="Template is the name of the template to execute by the hook"
+    )
     template_ref: Optional[TemplateRef] = Field(
-        None,
+        default=None,
         alias="templateRef",
         description="TemplateRef is the reference to the template resource to execute by the hook",
     )
 
 
 class NodeResult(BaseModel):
     message: Optional[str] = None
     outputs: Optional[Outputs] = None
     phase: Optional[str] = None
     progress: Optional[str] = None
 
 
 class NodeStatus(BaseModel):
     boundary_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="boundaryID",
         description=(
             "BoundaryID indicates the node ID of the associated template root node in which this node belongs to"
         ),
     )
-    children: Optional[List[str]] = Field(None, description="Children is a list of child node IDs")
+    children: Optional[List[str]] = Field(default=None, description="Children is a list of child node IDs")
     daemoned: Optional[bool] = Field(
-        None, description="Daemoned tracks whether or not this node was daemoned and need to be terminated"
+        default=None, description="Daemoned tracks whether or not this node was daemoned and need to be terminated"
     )
     display_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="displayName",
         description="DisplayName is a human readable representation of the node. Unique within a template boundary",
     )
     estimated_duration: Optional[int] = Field(
-        None, alias="estimatedDuration", description="EstimatedDuration in seconds."
+        default=None, alias="estimatedDuration", description="EstimatedDuration in seconds."
+    )
+    finished_at: Optional[v1_1.Time] = Field(
+        default=None, alias="finishedAt", description="Time at which this node completed"
     )
-    finished_at: Optional[v1_1.Time] = Field(None, alias="finishedAt", description="Time at which this node completed")
     host_node_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="hostNodeName",
         description="HostNodeName name of the Kubernetes node on which the Pod is running, if applicable",
     )
     id: str = Field(
         ...,
         description=(
             "ID is a unique identifier of a node within the worklow It is implemented as a hash of the node name,"
             " which makes the ID deterministic"
         ),
     )
     inputs: Optional[Inputs] = Field(
-        None,
+        default=None,
         description=(
             "Inputs captures input parameter values and artifact locations supplied to this template invocation"
         ),
     )
     memoization_status: Optional[MemoizationStatus] = Field(
-        None, alias="memoizationStatus", description="MemoizationStatus holds information about cached nodes"
+        default=None, alias="memoizationStatus", description="MemoizationStatus holds information about cached nodes"
     )
     message: Optional[str] = Field(
-        None, description="A human readable message indicating details about why the node is in this condition."
+        default=None,
+        description="A human readable message indicating details about why the node is in this condition.",
     )
     name: str = Field(..., description="Name is unique name in the node tree used to generate the node ID")
     outbound_nodes: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="outboundNodes",
         description=(
             'OutboundNodes tracks the node IDs which are considered "outbound" nodes to a template invocation. For'
             ' every invocation of a template, there are nodes which we considered as "outbound". Essentially, these'
             " are last nodes in the execution sequence to run, before the template is considered completed. These"
             " nodes are then connected as parents to a following step.\n\nIn the case of single pod steps (i.e."
             " container, script, resource templates), this list will be nil since the pod itself is already considered"
@@ -2277,1048 +2356,1080 @@
             " children). In the case of steps, outbound nodes are all the containers involved in the last step group."
             " NOTE: since templates are composable, the list of outbound nodes are carried upwards when a DAG/steps"
             " template invokes another DAG/steps template. In other words, the outbound nodes of a template, will be a"
             " superset of the outbound nodes of its last children."
         ),
     )
     outputs: Optional[Outputs] = Field(
-        None,
+        default=None,
         description=(
             "Outputs captures output parameter values and artifact locations produced by this template invocation"
         ),
     )
     phase: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Phase a simple, high-level summary of where the node is in its lifecycle. Can be used as a state machine."
         ),
     )
     pod_ip: Optional[str] = Field(
-        None, alias="podIP", description="PodIP captures the IP of the pod for daemoned steps"
+        default=None, alias="podIP", description="PodIP captures the IP of the pod for daemoned steps"
     )
-    progress: Optional[str] = Field(None, description="Progress to completion")
+    progress: Optional[str] = Field(default=None, description="Progress to completion")
     resources_duration: Optional[Dict[str, int]] = Field(
-        None,
+        default=None,
         alias="resourcesDuration",
         description=(
             "ResourcesDuration is indicative, but not accurate, resource duration. This is populated when the nodes"
             " completes."
         ),
     )
-    started_at: Optional[v1_1.Time] = Field(None, alias="startedAt", description="Time at which this node started")
+    started_at: Optional[v1_1.Time] = Field(
+        default=None, alias="startedAt", description="Time at which this node started"
+    )
     synchronization_status: Optional[NodeSynchronizationStatus] = Field(
-        None,
+        default=None,
         alias="synchronizationStatus",
         description="SynchronizationStatus is the synchronization status of the node",
     )
     template_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="templateName",
         description=(
             "TemplateName is the template name which this node corresponds to. Not applicable to virtual nodes (e.g."
             " Retry, StepGroup)"
         ),
     )
     template_ref: Optional[TemplateRef] = Field(
-        None,
+        default=None,
         alias="templateRef",
         description=(
             "TemplateRef is the reference to the template resource which this node corresponds to. Not applicable to"
             " virtual nodes (e.g. Retry, StepGroup)"
         ),
     )
     template_scope: Optional[str] = Field(
-        None,
+        default=None,
         alias="templateScope",
         description="TemplateScope is the template scope in which the template of this node was retrieved.",
     )
     type: str = Field(..., description="Type indicates type of node")
 
 
 class Submit(BaseModel):
     arguments: Optional[Arguments] = Field(
-        None, description="Arguments extracted from the event and then set as arguments to the workflow created."
+        default=None,
+        description="Arguments extracted from the event and then set as arguments to the workflow created.",
     )
     metadata: Optional[v1_1.ObjectMeta] = Field(
-        None, description="Metadata optional means to customize select fields of the workflow metadata"
+        default=None, description="Metadata optional means to customize select fields of the workflow metadata"
     )
     workflow_template_ref: WorkflowTemplateRef = Field(
         ..., alias="workflowTemplateRef", description="WorkflowTemplateRef the workflow template to submit"
     )
 
 
 class WorkflowEventBindingSpec(BaseModel):
     event: Event = Field(..., description="Event is the event to bind to")
-    submit: Optional[Submit] = Field(None, description="Submit is the workflow template to submit")
+    submit: Optional[Submit] = Field(default=None, description="Submit is the workflow template to submit")
 
 
 class WorkflowTaskSetStatus(BaseModel):
     nodes: Optional[Dict[str, NodeResult]] = None
 
 
 class WorkflowEventBinding(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ObjectMeta
     spec: WorkflowEventBindingSpec
 
 
 class WorkflowEventBindingList(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     items: List[WorkflowEventBinding]
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ListMeta
 
 
 class ClusterWorkflowTemplate(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ObjectMeta
     spec: WorkflowSpec
 
 
 class ClusterWorkflowTemplateCreateRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
     template: Optional[ClusterWorkflowTemplate] = None
 
 
 class ClusterWorkflowTemplateLintRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
     template: Optional[ClusterWorkflowTemplate] = None
 
 
 class ClusterWorkflowTemplateList(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     items: List[ClusterWorkflowTemplate]
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ListMeta
 
 
 class ClusterWorkflowTemplateUpdateRequest(BaseModel):
-    name: Optional[str] = Field(None, description="DEPRECATED: This field is ignored.")
+    name: Optional[str] = Field(default=None, description="DEPRECATED: This field is ignored.")
     template: Optional[ClusterWorkflowTemplate] = None
 
 
 class CreateCronWorkflowRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
-    cron_workflow: Optional[CronWorkflow] = Field(None, alias="cronWorkflow")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
+    cron_workflow: Optional[CronWorkflow] = Field(default=None, alias="cronWorkflow")
     namespace: Optional[str] = None
 
 
 class CronWorkflow(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ObjectMeta
     spec: CronWorkflowSpec
     status: Optional[CronWorkflowStatus] = None
 
 
 class CronWorkflowList(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     items: List[CronWorkflow]
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ListMeta
 
 
 class CronWorkflowSpec(BaseModel):
     concurrency_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="concurrencyPolicy",
         description="ConcurrencyPolicy is the K8s-style concurrency policy that will be used",
     )
     failed_jobs_history_limit: Optional[int] = Field(
-        None,
+        default=None,
         alias="failedJobsHistoryLimit",
         description="FailedJobsHistoryLimit is the number of failed jobs to be kept at a time",
     )
     schedule: str = Field(..., description="Schedule is a schedule to run the Workflow in Cron format")
     starting_deadline_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="startingDeadlineSeconds",
         description=(
             "StartingDeadlineSeconds is the K8s-style deadline that will limit the time a CronWorkflow will be run"
             " after its original scheduled time if it is missed."
         ),
     )
     successful_jobs_history_limit: Optional[int] = Field(
-        None,
+        default=None,
         alias="successfulJobsHistoryLimit",
         description="SuccessfulJobsHistoryLimit is the number of successful jobs to be kept at a time",
     )
     suspend: Optional[bool] = Field(
-        None, description="Suspend is a flag that will stop new CronWorkflows from running if set to true"
+        default=None, description="Suspend is a flag that will stop new CronWorkflows from running if set to true"
     )
     timezone: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Timezone is the timezone against which the cron schedule will be calculated, e.g. "Asia/Tokyo". Default'
             " is machine's local time."
         ),
     )
     workflow_metadata: Optional[v1_1.ObjectMeta] = Field(
-        None, alias="workflowMetadata", description="WorkflowMetadata contains some metadata of the workflow to be run"
+        default=None,
+        alias="workflowMetadata",
+        description="WorkflowMetadata contains some metadata of the workflow to be run",
     )
     workflow_spec: WorkflowSpec = Field(
         ..., alias="workflowSpec", description="WorkflowSpec is the spec of the workflow to be run"
     )
 
 
 class DAGTask(BaseModel):
     arguments: Optional[Arguments] = Field(
-        None, description="Arguments are the parameter and artifact arguments to the template"
+        default=None, description="Arguments are the parameter and artifact arguments to the template"
     )
     continue_on: Optional[ContinueOn] = Field(
-        None,
+        default=None,
         alias="continueOn",
         description=(
             "ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed"
             " states can be specified"
         ),
     )
     dependencies: Optional[List[str]] = Field(
-        None, description="Dependencies are name of other targets which this depends on"
+        default=None, description="Dependencies are name of other targets which this depends on"
     )
-    depends: Optional[str] = Field(None, description="Depends are name of other targets which this depends on")
+    depends: Optional[str] = Field(default=None, description="Depends are name of other targets which this depends on")
     hooks: Optional[Dict[str, LifecycleHook]] = Field(
-        None,
+        default=None,
         description=(
             "Hooks hold the lifecycle hook which is invoked at lifecycle of task, irrespective of the success,"
             " failure, or error status of the primary task"
         ),
     )
     inline: Optional[Template] = Field(
-        None, description="Inline is the template. Template must be empty if this is declared (and vice-versa)."
+        default=None,
+        description="Inline is the template. Template must be empty if this is declared (and vice-versa).",
     )
     name: str = Field(..., description="Name is the name of the target")
     on_exit: Optional[str] = Field(
-        None,
+        default=None,
         alias="onExit",
         description=(
             "OnExit is a template reference which is invoked at the end of the template, irrespective of the success,"
             " failure, or error of the primary template. DEPRECATED: Use Hooks[exit].Template instead."
         ),
     )
-    template: Optional[str] = Field(None, description="Name of template to execute")
+    template: Optional[str] = Field(default=None, description="Name of template to execute")
     template_ref: Optional[TemplateRef] = Field(
-        None, alias="templateRef", description="TemplateRef is the reference to the template resource to execute."
+        default=None,
+        alias="templateRef",
+        description="TemplateRef is the reference to the template resource to execute.",
     )
     when: Optional[str] = Field(
-        None, description="When is an expression in which the task should conditionally execute"
+        default=None, description="When is an expression in which the task should conditionally execute"
     )
     with_items: Optional[List[Item]] = Field(
-        None,
+        default=None,
         alias="withItems",
         description="WithItems expands a task into multiple parallel tasks from the items in the list",
     )
     with_param: Optional[str] = Field(
-        None,
+        default=None,
         alias="withParam",
         description=(
             "WithParam expands a task into multiple parallel tasks from the value in the parameter, which is expected"
             " to be a JSON list."
         ),
     )
     with_sequence: Optional[Sequence] = Field(
-        None, alias="withSequence", description="WithSequence expands a task into a numeric sequence"
+        default=None, alias="withSequence", description="WithSequence expands a task into a numeric sequence"
     )
 
 
 class DAGTemplate(BaseModel):
     fail_fast: Optional[bool] = Field(
-        None,
+        default=None,
         alias="failFast",
         description=(
             'This flag is for DAG logic. The DAG logic has a built-in "fail fast" feature to stop scheduling new'
             " steps, as soon as it detects that one of the DAG nodes is failed. Then it waits until all DAG nodes are"
             " completed before failing the DAG itself. The FailFast flag default is true,  if set to false, it will"
             " allow a DAG to run all branches of the DAG to completion (either success or failure), regardless of the"
             " failed outcomes of branches in the DAG. More info and example about this feature at"
             " https://github.com/argoproj/argo-workflows/issues/1442"
         ),
     )
-    target: Optional[str] = Field(None, description="Target are one or more names of targets to execute in a DAG")
+    target: Optional[str] = Field(
+        default=None, description="Target are one or more names of targets to execute in a DAG"
+    )
     tasks: List[DAGTask] = Field(..., description="Tasks are a list of DAG tasks")
 
 
 class LintCronWorkflowRequest(BaseModel):
-    cron_workflow: Optional[CronWorkflow] = Field(None, alias="cronWorkflow")
+    cron_workflow: Optional[CronWorkflow] = Field(default=None, alias="cronWorkflow")
     namespace: Optional[str] = None
 
 
 class ParallelSteps(BaseModel):
     __root__: List[WorkflowStep]
 
 
 class Template(BaseModel):
     active_deadline_seconds: Optional[intstr.IntOrString] = Field(
-        None,
+        default=None,
         alias="activeDeadlineSeconds",
         description=(
             "Optional duration in seconds relative to the StartTime that the pod may be active on a node before the"
             " system actively tries to terminate the pod; value must be positive integer This field is only applicable"
             " to container and script templates."
         ),
     )
     affinity: Optional[v1.Affinity] = Field(
-        None,
+        default=None,
         description=(
             "Affinity sets the pod's scheduling constraints Overrides the affinity set at the workflow level (if any)"
         ),
     )
     archive_location: Optional[ArtifactLocation] = Field(
-        None,
+        default=None,
         alias="archiveLocation",
         description=(
             "Location in which all files related to the step will be stored (logs, artifacts, etc...). Can be"
             " overridden by individual items in Outputs. If omitted, will use the default artifact repository location"
             " configured in the controller, appended with the <workflowname>/<nodename> in the key."
         ),
     )
     automount_service_account_token: Optional[bool] = Field(
-        None,
+        default=None,
         alias="automountServiceAccountToken",
         description=(
             "AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in"
             " pods. ServiceAccountName of ExecutorConfig must be specified if this value is false."
         ),
     )
     container: Optional[v1.Container] = Field(
-        None, description="Container is the main container image to run in the pod"
+        default=None, description="Container is the main container image to run in the pod"
     )
     container_set: Optional[ContainerSetTemplate] = Field(
-        None, alias="containerSet", description="ContainerSet groups multiple containers within a single pod."
+        default=None, alias="containerSet", description="ContainerSet groups multiple containers within a single pod."
     )
     daemon: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Deamon will allow a workflow to proceed to the next step so long as the container reaches readiness"
         ),
     )
-    dag: Optional[DAGTemplate] = Field(None, description="DAG template subtype which runs a DAG")
-    data: Optional[Data] = Field(None, description="Data is a data template")
+    dag: Optional[DAGTemplate] = Field(default=None, description="DAG template subtype which runs a DAG")
+    data: Optional[Data] = Field(default=None, description="Data is a data template")
     executor: Optional[ExecutorConfig] = Field(
-        None, description="Executor holds configurations of the executor container."
+        default=None, description="Executor holds configurations of the executor container."
     )
     fail_fast: Optional[bool] = Field(
-        None,
+        default=None,
         alias="failFast",
         description=(
             "FailFast, if specified, will fail this template if any of its child pods has failed. This is useful for"
             " when this template is expanded with `withItems`, etc."
         ),
     )
     host_aliases: Optional[List[v1.HostAlias]] = Field(
-        None,
+        default=None,
         alias="hostAliases",
         description="HostAliases is an optional list of hosts and IPs that will be injected into the pod spec",
     )
-    http: Optional[HTTP] = Field(None, description="HTTP makes a HTTP request")
+    http: Optional[HTTP] = Field(default=None, description="HTTP makes a HTTP request")
     init_containers: Optional[List[UserContainer]] = Field(
-        None,
+        default=None,
         alias="initContainers",
         description="InitContainers is a list of containers which run before the main container.",
     )
     inputs: Optional[Inputs] = Field(
-        None, description="Inputs describe what inputs parameters and artifacts are supplied to this template"
+        default=None, description="Inputs describe what inputs parameters and artifacts are supplied to this template"
     )
     memoize: Optional[Memoize] = Field(
-        None, description="Memoize allows templates to use outputs generated from already executed templates"
+        default=None, description="Memoize allows templates to use outputs generated from already executed templates"
     )
     metadata: Optional[Metadata] = Field(
-        None, description="Metdata sets the pods's metadata, i.e. annotations and labels"
+        default=None, description="Metdata sets the pods's metadata, i.e. annotations and labels"
+    )
+    metrics: Optional[Metrics] = Field(
+        default=None, description="Metrics are a list of metrics emitted from this template"
     )
-    metrics: Optional[Metrics] = Field(None, description="Metrics are a list of metrics emitted from this template")
-    name: Optional[str] = Field(None, description="Name is the name of the template")
+    name: Optional[str] = Field(default=None, description="Name is the name of the template")
     node_selector: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         alias="nodeSelector",
         description=(
             "NodeSelector is a selector to schedule this step of the workflow to be run on the selected node(s)."
             " Overrides the selector set at the workflow level."
         ),
     )
     outputs: Optional[Outputs] = Field(
-        None, description="Outputs describe the parameters and artifacts that this template produces"
+        default=None, description="Outputs describe the parameters and artifacts that this template produces"
     )
     parallelism: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Parallelism limits the max total parallel pods that can execute at the same time within the boundaries of"
             " this template invocation. If additional steps/dag templates are invoked, the pods created by those"
             " templates will not be counted towards this total."
         ),
     )
-    plugin: Optional[Plugin] = Field(None, description="Plugin is a plugin template")
+    plugin: Optional[Plugin] = Field(default=None, description="Plugin is a plugin template")
     pod_spec_patch: Optional[str] = Field(
-        None,
+        default=None,
         alias="podSpecPatch",
         description=(
             "PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of"
             " container fields which are not strings (e.g. resource limits)."
         ),
     )
-    priority: Optional[int] = Field(None, description="Priority to apply to workflow pods.")
+    priority: Optional[int] = Field(default=None, description="Priority to apply to workflow pods.")
     priority_class_name: Optional[str] = Field(
-        None, alias="priorityClassName", description="PriorityClassName to apply to workflow pods."
+        default=None, alias="priorityClassName", description="PriorityClassName to apply to workflow pods."
     )
     resource: Optional[ResourceTemplate] = Field(
-        None, description="Resource template subtype which can run k8s resources"
+        default=None, description="Resource template subtype which can run k8s resources"
     )
     retry_strategy: Optional[RetryStrategy] = Field(
-        None, alias="retryStrategy", description="RetryStrategy describes how to retry a template when it fails"
+        default=None,
+        alias="retryStrategy",
+        description="RetryStrategy describes how to retry a template when it fails",
     )
     scheduler_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="schedulerName",
         description=(
             "If specified, the pod will be dispatched by specified scheduler. Or it will be dispatched by workflow"
             " scope scheduler if specified. If neither specified, the pod will be dispatched by default scheduler."
         ),
     )
-    script: Optional[ScriptTemplate] = Field(None, description="Script runs a portion of code against an interpreter")
+    script: Optional[ScriptTemplate] = Field(
+        default=None, description="Script runs a portion of code against an interpreter"
+    )
     security_context: Optional[v1.PodSecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to"
             " empty.  See type description for default values of each field."
         ),
     )
     service_account_name: Optional[str] = Field(
-        None, alias="serviceAccountName", description="ServiceAccountName to apply to workflow pods"
+        default=None, alias="serviceAccountName", description="ServiceAccountName to apply to workflow pods"
     )
     sidecars: Optional[List[UserContainer]] = Field(
-        None,
+        default=None,
         description=(
             "Sidecars is a list of containers which run alongside the main container Sidecars are automatically killed"
             " when the main container completes"
         ),
     )
     steps: Optional[List[ParallelSteps]] = Field(
-        None, description="Steps define a series of sequential/parallel workflow steps"
+        default=None, description="Steps define a series of sequential/parallel workflow steps"
     )
     suspend: Optional[SuspendTemplate] = Field(
-        None, description="Suspend template subtype which can suspend a workflow when reaching the step"
+        default=None, description="Suspend template subtype which can suspend a workflow when reaching the step"
     )
     synchronization: Optional[Synchronization] = Field(
-        None, description="Synchronization holds synchronization lock configuration for this template"
+        default=None, description="Synchronization holds synchronization lock configuration for this template"
     )
     timeout: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Timeout allows to set the total node execution timeout duration counting from the node's start time. This"
             " duration also includes time in which the node spends in Pending state. This duration may not be applied"
             " to Step or DAG templates."
         ),
     )
-    tolerations: Optional[List[v1.Toleration]] = Field(None, description="Tolerations to apply to workflow pods.")
+    tolerations: Optional[List[v1.Toleration]] = Field(
+        default=None, description="Tolerations to apply to workflow pods."
+    )
     volumes: Optional[List[v1.Volume]] = Field(
-        None, description="Volumes is a list of volumes that can be mounted by containers in a template."
+        default=None, description="Volumes is a list of volumes that can be mounted by containers in a template."
     )
 
 
 class UpdateCronWorkflowRequest(BaseModel):
-    cron_workflow: Optional[CronWorkflow] = Field(None, alias="cronWorkflow")
-    name: Optional[str] = Field(None, description="DEPRECATED: This field is ignored.")
+    cron_workflow: Optional[CronWorkflow] = Field(default=None, alias="cronWorkflow")
+    name: Optional[str] = Field(default=None, description="DEPRECATED: This field is ignored.")
     namespace: Optional[str] = None
 
 
 class Workflow(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ObjectMeta
     spec: WorkflowSpec
     status: Optional[WorkflowStatus] = None
 
 
 class WorkflowCreateRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
-    instance_id: Optional[str] = Field(None, alias="instanceID", description="This field is no longer used.")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
+    instance_id: Optional[str] = Field(default=None, alias="instanceID", description="This field is no longer used.")
     namespace: Optional[str] = None
-    server_dry_run: Optional[bool] = Field(None, alias="serverDryRun")
+    server_dry_run: Optional[bool] = Field(default=None, alias="serverDryRun")
     workflow: Optional[Workflow] = None
 
 
 class WorkflowLintRequest(BaseModel):
     namespace: Optional[str] = None
     workflow: Optional[Workflow] = None
 
 
 class WorkflowList(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     items: List[Workflow]
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ListMeta
 
 
 class WorkflowSpec(BaseModel):
     active_deadline_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="activeDeadlineSeconds",
         description=(
             "Optional duration in seconds relative to the workflow start time which the workflow is allowed to run"
             " before the controller terminates the io.argoproj.workflow.v1alpha1. A value of zero is used to terminate"
             " a Running workflow"
         ),
     )
     affinity: Optional[v1.Affinity] = Field(
-        None,
+        default=None,
         description=(
             "Affinity sets the scheduling constraints for all pods in the io.argoproj.workflow.v1alpha1. Can be"
             " overridden by an affinity specified in the template"
         ),
     )
     archive_logs: Optional[bool] = Field(
-        None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
+        default=None, alias="archiveLogs", description="ArchiveLogs indicates if the container logs should be archived"
     )
     arguments: Optional[Arguments] = Field(
-        None,
+        default=None,
         description=(
             "Arguments contain the parameters and artifacts sent to the workflow entrypoint Parameters are"
             " referencable globally using the 'workflow' variable prefix. e.g."
             " {{io.argoproj.workflow.v1alpha1.parameters.myparam}}"
         ),
     )
     artifact_gc: Optional[ArtifactGC] = Field(
-        None,
+        default=None,
         alias="artifactGC",
         description=(
             "ArtifactGC describes the strategy to use when deleting artifacts from completed or deleted workflows"
             " (applies to all output Artifacts unless Artifact.ArtifactGC is specified, which overrides this)"
         ),
     )
     artifact_repository_ref: Optional[ArtifactRepositoryRef] = Field(
-        None,
+        default=None,
         alias="artifactRepositoryRef",
         description=(
             "ArtifactRepositoryRef specifies the configMap name and key containing the artifact repository config."
         ),
     )
     automount_service_account_token: Optional[bool] = Field(
-        None,
+        default=None,
         alias="automountServiceAccountToken",
         description=(
             "AutomountServiceAccountToken indicates whether a service account token should be automatically mounted in"
             " pods. ServiceAccountName of ExecutorConfig must be specified if this value is false."
         ),
     )
     dns_config: Optional[v1.PodDNSConfig] = Field(
-        None,
+        default=None,
         alias="dnsConfig",
         description="PodDNSConfig defines the DNS parameters of a pod in addition to those generated from DNSPolicy.",
     )
     dns_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="dnsPolicy",
         description=(
             "Set DNS policy for the pod. Defaults to \"ClusterFirst\". Valid values are 'ClusterFirstWithHostNet',"
             " 'ClusterFirst', 'Default' or 'None'. DNS parameters given in DNSConfig will be merged with the"
             " policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS"
             " policy explicitly to 'ClusterFirstWithHostNet'."
         ),
     )
     entrypoint: Optional[str] = Field(
-        None,
+        default=None,
         description="Entrypoint is a template reference to the starting point of the io.argoproj.workflow.v1alpha1.",
     )
     executor: Optional[ExecutorConfig] = Field(
-        None, description="Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1."
+        default=None,
+        description="Executor holds configurations of executor containers of the io.argoproj.workflow.v1alpha1.",
     )
     hooks: Optional[Dict[str, LifecycleHook]] = Field(
-        None,
+        default=None,
         description=(
             "Hooks holds the lifecycle hook which is invoked at lifecycle of step, irrespective of the success,"
             " failure, or error status of the primary step"
         ),
     )
-    host_aliases: Optional[List[v1.HostAlias]] = Field(None, alias="hostAliases")
+    host_aliases: Optional[List[v1.HostAlias]] = Field(default=None, alias="hostAliases")
     host_network: Optional[bool] = Field(
-        None, alias="hostNetwork", description="Host networking requested for this workflow pod. Default to false."
+        default=None,
+        alias="hostNetwork",
+        description="Host networking requested for this workflow pod. Default to false.",
     )
     image_pull_secrets: Optional[List[v1.LocalObjectReference]] = Field(
-        None,
+        default=None,
         alias="imagePullSecrets",
         description=(
             "ImagePullSecrets is a list of references to secrets in the same namespace to use for pulling any images"
             " in pods that reference this ServiceAccount. ImagePullSecrets are distinct from Secrets because Secrets"
             " can be mounted in the pod, but ImagePullSecrets are only accessed by the kubelet. More info:"
             " https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod"
         ),
     )
-    metrics: Optional[Metrics] = Field(None, description="Metrics are a list of metrics emitted from this Workflow")
+    metrics: Optional[Metrics] = Field(
+        default=None, description="Metrics are a list of metrics emitted from this Workflow"
+    )
     node_selector: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         alias="nodeSelector",
         description=(
             "NodeSelector is a selector which will result in all pods of the workflow to be scheduled on the selected"
             " node(s). This is able to be overridden by a nodeSelector specified in the template."
         ),
     )
     on_exit: Optional[str] = Field(
-        None,
+        default=None,
         alias="onExit",
         description=(
             "OnExit is a template reference which is invoked at the end of the workflow, irrespective of the success,"
             " failure, or error of the primary io.argoproj.workflow.v1alpha1."
         ),
     )
     parallelism: Optional[int] = Field(
-        None,
+        default=None,
         description="Parallelism limits the max total parallel pods that can execute at the same time in a workflow",
     )
     pod_disruption_budget: Optional[v1beta1.PodDisruptionBudgetSpec] = Field(
-        None,
+        default=None,
         alias="podDisruptionBudget",
         description=(
             "PodDisruptionBudget holds the number of concurrent disruptions that you allow for Workflow's Pods."
             " Controller will automatically add the selector with workflow name, if selector is empty. Optional:"
             " Defaults to empty."
         ),
     )
     pod_gc: Optional[PodGC] = Field(
-        None, alias="podGC", description="PodGC describes the strategy to use when deleting completed pods"
+        default=None, alias="podGC", description="PodGC describes the strategy to use when deleting completed pods"
     )
     pod_metadata: Optional[Metadata] = Field(
-        None,
+        default=None,
         alias="podMetadata",
         description="PodMetadata defines additional metadata that should be applied to workflow pods",
     )
     pod_priority: Optional[int] = Field(
-        None,
+        default=None,
         alias="podPriority",
         description="Priority to apply to workflow pods. DEPRECATED: Use PodPriorityClassName instead.",
     )
     pod_priority_class_name: Optional[str] = Field(
-        None, alias="podPriorityClassName", description="PriorityClassName to apply to workflow pods."
+        default=None, alias="podPriorityClassName", description="PriorityClassName to apply to workflow pods."
     )
     pod_spec_patch: Optional[str] = Field(
-        None,
+        default=None,
         alias="podSpecPatch",
         description=(
             "PodSpecPatch holds strategic merge patch to apply against the pod spec. Allows parameterization of"
             " container fields which are not strings (e.g. resource limits)."
         ),
     )
     priority: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Priority is used if controller is configured to process limited number of workflows in parallel."
             " Workflows with higher priority are processed first."
         ),
     )
     retry_strategy: Optional[RetryStrategy] = Field(
-        None,
+        default=None,
         alias="retryStrategy",
         description="RetryStrategy for all templates in the io.argoproj.workflow.v1alpha1.",
     )
     scheduler_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="schedulerName",
         description=(
             "Set scheduler name for all pods. Will be overridden if container/script template's scheduler name is set."
             " Default scheduler will be used if neither specified."
         ),
     )
     security_context: Optional[v1.PodSecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to"
             " empty.  See type description for default values of each field."
         ),
     )
     service_account_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="serviceAccountName",
         description="ServiceAccountName is the name of the ServiceAccount to run all pods of the workflow as.",
     )
     shutdown: Optional[str] = Field(
-        None, description="Shutdown will shutdown the workflow according to its ShutdownStrategy"
+        default=None, description="Shutdown will shutdown the workflow according to its ShutdownStrategy"
     )
     suspend: Optional[bool] = Field(
-        None, description="Suspend will suspend the workflow and prevent execution of any future steps in the workflow"
+        default=None,
+        description="Suspend will suspend the workflow and prevent execution of any future steps in the workflow",
     )
     synchronization: Optional[Synchronization] = Field(
-        None, description="Synchronization holds synchronization lock configuration for this Workflow"
+        default=None, description="Synchronization holds synchronization lock configuration for this Workflow"
     )
     template_defaults: Optional[Template] = Field(
-        None,
+        default=None,
         alias="templateDefaults",
         description=(
             "TemplateDefaults holds default template values that will apply to all templates in the Workflow, unless"
             " overridden on the template-level"
         ),
     )
     templates: Optional[List[Template]] = Field(
-        None, description="Templates is a list of workflow templates used in a workflow"
+        default=None, description="Templates is a list of workflow templates used in a workflow"
+    )
+    tolerations: Optional[List[v1.Toleration]] = Field(
+        default=None, description="Tolerations to apply to workflow pods."
     )
-    tolerations: Optional[List[v1.Toleration]] = Field(None, description="Tolerations to apply to workflow pods.")
     ttl_strategy: Optional[TTLStrategy] = Field(
-        None,
+        default=None,
         alias="ttlStrategy",
         description=(
             "TTLStrategy limits the lifetime of a Workflow that has finished execution depending on if it Succeeded or"
             " Failed. If this struct is set, once the Workflow finishes, it will be deleted after the time to live"
             " expires. If this field is unset, the controller config map will hold the default values."
         ),
     )
     volume_claim_gc: Optional[VolumeClaimGC] = Field(
-        None,
+        default=None,
         alias="volumeClaimGC",
         description="VolumeClaimGC describes the strategy to use when deleting volumes from completed workflows",
     )
     volume_claim_templates: Optional[List[v1.PersistentVolumeClaim]] = Field(
-        None,
+        default=None,
         alias="volumeClaimTemplates",
         description=(
             "VolumeClaimTemplates is a list of claims that containers are allowed to reference. The Workflow"
             " controller will create the claims at the beginning of the workflow and delete the claims upon completion"
             " of the workflow"
         ),
     )
     volumes: Optional[List[v1.Volume]] = Field(
-        None,
+        default=None,
         description=(
             "Volumes is a list of volumes that can be mounted by containers in a io.argoproj.workflow.v1alpha1."
         ),
     )
     workflow_metadata: Optional[WorkflowMetadata] = Field(
-        None,
+        default=None,
         alias="workflowMetadata",
         description="WorkflowMetadata contains some metadata of the workflow to refer to",
     )
     workflow_template_ref: Optional[WorkflowTemplateRef] = Field(
-        None,
+        default=None,
         alias="workflowTemplateRef",
         description="WorkflowTemplateRef holds a reference to a WorkflowTemplate for execution",
     )
 
 
 class WorkflowStatus(BaseModel):
     artifact_gc_status: Optional[ArtGCStatus] = Field(
-        None,
+        default=None,
         alias="artifactGCStatus",
         description="ArtifactGCStatus maintains the status of Artifact Garbage Collection",
     )
     artifact_repository_ref: Optional[ArtifactRepositoryRefStatus] = Field(
-        None,
+        default=None,
         alias="artifactRepositoryRef",
         description=(
             "ArtifactRepositoryRef is used to cache the repository to use so we do not need to determine it everytime"
             " we reconcile."
         ),
     )
     compressed_nodes: Optional[str] = Field(
-        None, alias="compressedNodes", description="Compressed and base64 decoded Nodes map"
+        default=None, alias="compressedNodes", description="Compressed and base64 decoded Nodes map"
     )
     conditions: Optional[List[Condition]] = Field(
-        None, description="Conditions is a list of conditions the Workflow may have"
+        default=None, description="Conditions is a list of conditions the Workflow may have"
     )
     estimated_duration: Optional[int] = Field(
-        None, alias="estimatedDuration", description="EstimatedDuration in seconds."
+        default=None, alias="estimatedDuration", description="EstimatedDuration in seconds."
     )
     finished_at: Optional[v1_1.Time] = Field(
-        None, alias="finishedAt", description="Time at which this workflow completed"
+        default=None, alias="finishedAt", description="Time at which this workflow completed"
     )
     message: Optional[str] = Field(
-        None, description="A human readable message indicating details about why the workflow is in this condition."
+        default=None,
+        description="A human readable message indicating details about why the workflow is in this condition.",
     )
     nodes: Optional[Dict[str, NodeStatus]] = Field(
-        None, description="Nodes is a mapping between a node ID and the node's status."
+        default=None, description="Nodes is a mapping between a node ID and the node's status."
     )
     offload_node_status_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="offloadNodeStatusVersion",
         description=(
             "Whether on not node status has been offloaded to a database. If exists, then Nodes and CompressedNodes"
             " will be empty. This will actually be populated with a hash of the offloaded data."
         ),
     )
     outputs: Optional[Outputs] = Field(
-        None,
+        default=None,
         description=(
             "Outputs captures output values and artifact locations produced by the workflow via global outputs"
         ),
     )
     persistent_volume_claims: Optional[List[v1.Volume]] = Field(
-        None,
+        default=None,
         alias="persistentVolumeClaims",
         description=(
             "PersistentVolumeClaims tracks all PVCs that were created as part of the io.argoproj.workflow.v1alpha1."
             " The contents of this list are drained at the end of the workflow."
         ),
     )
     phase: Optional[str] = Field(
-        None, description="Phase a simple, high-level summary of where the workflow is in its lifecycle."
+        default=None, description="Phase a simple, high-level summary of where the workflow is in its lifecycle."
     )
-    progress: Optional[str] = Field(None, description="Progress to completion")
+    progress: Optional[str] = Field(default=None, description="Progress to completion")
     resources_duration: Optional[Dict[str, int]] = Field(
-        None, alias="resourcesDuration", description="ResourcesDuration is the total for the workflow"
+        default=None, alias="resourcesDuration", description="ResourcesDuration is the total for the workflow"
+    )
+    started_at: Optional[v1_1.Time] = Field(
+        default=None, alias="startedAt", description="Time at which this workflow started"
     )
-    started_at: Optional[v1_1.Time] = Field(None, alias="startedAt", description="Time at which this workflow started")
     stored_templates: Optional[Dict[str, Template]] = Field(
-        None,
+        default=None,
         alias="storedTemplates",
         description="StoredTemplates is a mapping between a template ref and the node's status.",
     )
     stored_workflow_template_spec: Optional[WorkflowSpec] = Field(
-        None,
+        default=None,
         alias="storedWorkflowTemplateSpec",
         description="StoredWorkflowSpec stores the WorkflowTemplate spec for future execution.",
     )
     synchronization: Optional[SynchronizationStatus] = Field(
-        None, description="Synchronization stores the status of synchronization locks"
+        default=None, description="Synchronization stores the status of synchronization locks"
     )
 
 
 class WorkflowStep(BaseModel):
-    arguments: Optional[Arguments] = Field(None, description="Arguments hold arguments to the template")
+    arguments: Optional[Arguments] = Field(default=None, description="Arguments hold arguments to the template")
     continue_on: Optional[ContinueOn] = Field(
-        None,
+        default=None,
         alias="continueOn",
         description=(
             "ContinueOn makes argo to proceed with the following step even if this step fails. Errors and Failed"
             " states can be specified"
         ),
     )
     hooks: Optional[Dict[str, LifecycleHook]] = Field(
-        None,
+        default=None,
         description=(
             "Hooks holds the lifecycle hook which is invoked at lifecycle of step, irrespective of the success,"
             " failure, or error status of the primary step"
         ),
     )
     inline: Optional[Template] = Field(
-        None, description="Inline is the template. Template must be empty if this is declared (and vice-versa)."
+        default=None,
+        description="Inline is the template. Template must be empty if this is declared (and vice-versa).",
     )
-    name: Optional[str] = Field(None, description="Name of the step")
+    name: Optional[str] = Field(default=None, description="Name of the step")
     on_exit: Optional[str] = Field(
-        None,
+        default=None,
         alias="onExit",
         description=(
             "OnExit is a template reference which is invoked at the end of the template, irrespective of the success,"
             " failure, or error of the primary template. DEPRECATED: Use Hooks[exit].Template instead."
         ),
     )
-    template: Optional[str] = Field(None, description="Template is the name of the template to execute as the step")
+    template: Optional[str] = Field(
+        default=None, description="Template is the name of the template to execute as the step"
+    )
     template_ref: Optional[TemplateRef] = Field(
-        None,
+        default=None,
         alias="templateRef",
         description="TemplateRef is the reference to the template resource to execute as the step.",
     )
     when: Optional[str] = Field(
-        None, description="When is an expression in which the step should conditionally execute"
+        default=None, description="When is an expression in which the step should conditionally execute"
     )
     with_items: Optional[List[Item]] = Field(
-        None,
+        default=None,
         alias="withItems",
         description="WithItems expands a step into multiple parallel steps from the items in the list",
     )
     with_param: Optional[str] = Field(
-        None,
+        default=None,
         alias="withParam",
         description=(
             "WithParam expands a step into multiple parallel steps from the value in the parameter, which is expected"
             " to be a JSON list."
         ),
     )
     with_sequence: Optional[Sequence] = Field(
-        None, alias="withSequence", description="WithSequence expands a step into a numeric sequence"
+        default=None, alias="withSequence", description="WithSequence expands a step into a numeric sequence"
     )
 
 
 class WorkflowTaskSetSpec(BaseModel):
     tasks: Optional[Dict[str, Template]] = None
 
 
 class WorkflowTemplate(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ObjectMeta
     spec: WorkflowSpec
 
 
 class WorkflowTemplateCreateRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
     namespace: Optional[str] = None
     template: Optional[WorkflowTemplate] = None
 
 
 class WorkflowTemplateLintRequest(BaseModel):
-    create_options: Optional[v1_1.CreateOptions] = Field(None, alias="createOptions")
+    create_options: Optional[v1_1.CreateOptions] = Field(default=None, alias="createOptions")
     namespace: Optional[str] = None
     template: Optional[WorkflowTemplate] = None
 
 
 class WorkflowTemplateList(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     items: List[WorkflowTemplate]
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.io.k8s.community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: v1_1.ListMeta
 
 
 class WorkflowTemplateUpdateRequest(BaseModel):
-    name: Optional[str] = Field(None, description="DEPRECATED: This field is ignored.")
+    name: Optional[str] = Field(default=None, description="DEPRECATED: This field is ignored.")
     namespace: Optional[str] = None
     template: Optional[WorkflowTemplate] = None
 
 
 class WorkflowWatchEvent(BaseModel):
-    object: Optional[Workflow] = Field(None, title="the workflow")
-    type: Optional[str] = Field(None, title="the type of change")
+    object: Optional[Workflow] = Field(default=None, title="the workflow")
+    type: Optional[str] = Field(default=None, title="the type of change")
 
 
 ClusterWorkflowTemplate.update_forward_refs()
 CreateCronWorkflowRequest.update_forward_refs()
 CronWorkflow.update_forward_refs()
 CronWorkflowSpec.update_forward_refs()
 DAGTask.update_forward_refs()
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/io/argoproj/workflow/v1alpha1.pyi` & `hera_workflows-5.6.0/src/hera/workflows/models/io/argoproj/workflow/v1alpha1.pyi`

 * *Files 0% similar despite different names*

```diff
@@ -1,15 +1,13 @@
-from typing import Any, Dict, List, Optional
-
-from hera.shared._base_model import BaseModel as BaseModel
-
 from ...k8s.api.core import v1 as v1
 from ...k8s.api.policy import v1beta1 as v1beta1
 from ...k8s.apimachinery.pkg.apis.meta import v1 as v1_1
 from ...k8s.apimachinery.pkg.util import intstr as intstr
+from hera.shared._base_model import BaseModel as BaseModel
+from typing import Any, Dict, List, Optional
 
 class Amount(BaseModel):
     __root__: float
 
 class ArchivedWorkflowDeletedResponse(BaseModel): ...
 
 class ArtGCStatus(BaseModel):
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/api/core/v1.py` & `hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/api/core/v1.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,43 +2,42 @@
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from enum import Enum
 from typing import Dict, List, Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from ...apimachinery.pkg.api import resource
 from ...apimachinery.pkg.apis.meta import v1
 from ...apimachinery.pkg.util import intstr
 
 
 class AWSElasticBlockStoreVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported"
             ' by the host operating system. Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if'
             " unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore"
         ),
     )
     partition: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "The partition in the volume that you want to mount. If omitted, the default is to mount by volume name."
             ' Examples: For volume /dev/sda1, you specify the partition as "1". Similarly, the volume partition for'
             ' /dev/sda is "0" (or you can leave the property empty).'
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             'Specify "true" to force and set the ReadOnly property in VolumeMounts to "true". If omitted, the default'
             ' is "false". More info: https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore'
         ),
     )
     volume_id: str = Field(
@@ -49,78 +48,80 @@
             " https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore"
         ),
     )
 
 
 class AzureDiskVolumeSource(BaseModel):
     caching_mode: Optional[str] = Field(
-        None, alias="cachingMode", description="Host Caching mode: None, Read Only, Read Write."
+        default=None, alias="cachingMode", description="Host Caching mode: None, Read Only, Read Write."
     )
     disk_name: str = Field(..., alias="diskName", description="The Name of the data disk in the blob storage")
     disk_uri: str = Field(..., alias="diskURI", description="The URI the data disk in the blob storage")
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Expected values Shared: multiple blob disks per storage account  Dedicated: single blob disk per storage"
             " account  Managed: azure managed data disk (only in managed availability set). defaults to shared"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.",
     )
 
 
 class AzureFileVolumeSource(BaseModel):
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.",
     )
     secret_name: str = Field(
         ..., alias="secretName", description="the name of secret that contains Azure Storage Account Name and Key"
     )
     share_name: str = Field(..., alias="shareName", description="Share Name")
 
 
 class Capabilities(BaseModel):
-    add: Optional[List[str]] = Field(None, description="Added capabilities")
-    drop: Optional[List[str]] = Field(None, description="Removed capabilities")
+    add: Optional[List[str]] = Field(default=None, description="Added capabilities")
+    drop: Optional[List[str]] = Field(default=None, description="Removed capabilities")
 
 
 class ConfigMapEnvSource(BaseModel):
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the ConfigMap must be defined")
+    optional: Optional[bool] = Field(default=None, description="Specify whether the ConfigMap must be defined")
 
 
 class ConfigMapKeySelector(BaseModel):
     key: str = Field(..., description="The key to select.")
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the ConfigMap or its key must be defined")
+    optional: Optional[bool] = Field(
+        default=None, description="Specify whether the ConfigMap or its key must be defined"
+    )
 
 
 class ImagePullPolicy(Enum):
     always = "Always"
     if_not_present = "IfNotPresent"
     never = "Never"
 
@@ -140,111 +141,115 @@
     container_port: int = Field(
         ...,
         alias="containerPort",
         description=(
             "Number of port to expose on the pod's IP address. This must be a valid port number, 0 < x < 65536."
         ),
     )
-    host_ip: Optional[str] = Field(None, alias="hostIP", description="What host IP to bind the external port to.")
+    host_ip: Optional[str] = Field(
+        default=None, alias="hostIP", description="What host IP to bind the external port to."
+    )
     host_port: Optional[int] = Field(
-        None,
+        default=None,
         alias="hostPort",
         description=(
             "Number of port to expose on the host. If specified, this must be a valid port number, 0 < x < 65536. If"
             " HostNetwork is specified, this must match ContainerPort. Most containers do not need this."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "If specified, this must be an IANA_SVC_NAME and unique within the pod. Each named port in a pod must have"
             " a unique name. Name for the port that can be referred to by services."
         ),
     )
     protocol: Optional[Protocol] = Field(
-        None,
+        default=None,
         description=(
             'Protocol for port. Must be UDP, TCP, or SCTP. Defaults to "TCP".\n\nPossible enum values:\n - `"SCTP"` is'
             ' the SCTP protocol.\n - `"TCP"` is the TCP protocol.\n - `"UDP"` is the UDP protocol.'
         ),
     )
 
 
 class EventSource(BaseModel):
-    component: Optional[str] = Field(None, description="Component from which the event is generated.")
-    host: Optional[str] = Field(None, description="Node name on which the event is generated.")
+    component: Optional[str] = Field(default=None, description="Component from which the event is generated.")
+    host: Optional[str] = Field(default=None, description="Node name on which the event is generated.")
 
 
 class ExecAction(BaseModel):
     command: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Command is the command line to execute inside the container, the working directory for the command  is"
             " root ('/') in the container's filesystem. The command is simply exec'd, it is not run inside a shell, so"
             " traditional shell instructions ('|', etc) won't work. To use a shell, you need to explicitly call out to"
             " that shell. Exit status of 0 is treated as live/healthy and non-zero is unhealthy."
         ),
     )
 
 
 class FCVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
-    lun: Optional[int] = Field(None, description="Optional: FC target lun number")
+    lun: Optional[int] = Field(default=None, description="Optional: FC target lun number")
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts."
         ),
     )
     target_ww_ns: Optional[List[str]] = Field(
-        None, alias="targetWWNs", description="Optional: FC target worldwide names (WWNs)"
+        default=None, alias="targetWWNs", description="Optional: FC target worldwide names (WWNs)"
     )
     wwids: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Optional: FC volume world wide identifiers (wwids) Either wwids or combination of targetWWNs and lun must"
             " be set, but not both simultaneously."
         ),
     )
 
 
 class FlockerVolumeSource(BaseModel):
     dataset_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="datasetName",
         description=(
             "Name of the dataset stored as metadata -> name on the dataset for Flocker should be considered as"
             " deprecated"
         ),
     )
     dataset_uuid: Optional[str] = Field(
-        None, alias="datasetUUID", description="UUID of the dataset. This is unique identifier of a Flocker dataset"
+        default=None,
+        alias="datasetUUID",
+        description="UUID of the dataset. This is unique identifier of a Flocker dataset",
     )
 
 
 class GCEPersistentDiskVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported"
             ' by the host operating system. Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if'
             " unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk"
         ),
     )
     partition: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "The partition in the volume that you want to mount. If omitted, the default is to mount by volume name."
             ' Examples: For volume /dev/sda1, you specify the partition as "1". Similarly, the volume partition for'
             ' /dev/sda is "0" (or you can leave the property empty). More info:'
             " https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk"
         ),
     )
@@ -253,46 +258,46 @@
         alias="pdName",
         description=(
             "Unique name of the PD resource in GCE. Used to identify the disk in GCE. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk"
         ),
     )
 
 
 class GRPCAction(BaseModel):
     port: int = Field(..., description="Port number of the gRPC service. Number must be in the range 1 to 65535.")
     service: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Service is the name of the service to place in the gRPC HealthCheckRequest (see"
             " https://github.com/grpc/grpc/blob/master/doc/health-checking.md).\n\nIf this is not specified, the"
             " default behavior is defined by gRPC."
         ),
     )
 
 
 class GitRepoVolumeSource(BaseModel):
     directory: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Target directory name. Must not contain or start with '..'.  If '.' is supplied, the volume directory"
             " will be the git repository.  Otherwise, if specified, the volume will contain the git repository in the"
             " subdirectory with the given name."
         ),
     )
     repository: str = Field(..., description="Repository URL")
-    revision: Optional[str] = Field(None, description="Commit hash for the specified revision.")
+    revision: Optional[str] = Field(default=None, description="Commit hash for the specified revision.")
 
 
 class GlusterfsVolumeSource(BaseModel):
     endpoints: str = Field(
         ...,
         description=(
             "EndpointsName is the endpoint name that details Glusterfs topology. More info:"
@@ -303,15 +308,15 @@
         ...,
         description=(
             "Path is the Glusterfs volume path. More info:"
             " https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "ReadOnly here will force the Glusterfs volume to be mounted with read-only permissions. Defaults to"
             " false. More info: https://examples.k8s.io/volumes/glusterfs/README.md#create-a-pod"
         ),
     )
 
@@ -323,39 +328,39 @@
 
 class HTTPHeader(BaseModel):
     name: str = Field(..., description="The header field name")
     value: str = Field(..., description="The header field value")
 
 
 class HostAlias(BaseModel):
-    hostnames: Optional[List[str]] = Field(None, description="Hostnames for the above IP address.")
-    ip: Optional[str] = Field(None, description="IP address of the host file entry.")
+    hostnames: Optional[List[str]] = Field(default=None, description="Hostnames for the above IP address.")
+    ip: Optional[str] = Field(default=None, description="IP address of the host file entry.")
 
 
 class HostPathVolumeSource(BaseModel):
     path: str = Field(
         ...,
         description=(
             "Path of the directory on the host. If the path is a symlink, it will follow the link to the real path."
             " More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath"
         ),
     )
     type: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Type for HostPath Volume Defaults to "" More info:'
             " https://kubernetes.io/docs/concepts/storage/volumes#hostpath"
         ),
     )
 
 
 class KeyToPath(BaseModel):
     key: str = Field(..., description="The key to project.")
     mode: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Optional: mode bits used to set permissions on this file. Must be an octal value between 0000 and 0777 or"
             " a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal"
             " values for mode bits. If not specified, the volume defaultMode will be used. This might be in conflict"
             " with other options that affect the file mode, like fsGroup, and the result can be other mode bits set."
         ),
     )
@@ -366,15 +371,15 @@
             " element '..'. May not start with the string '..'."
         ),
     )
 
 
 class LocalObjectReference(BaseModel):
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
 
 
@@ -383,15 +388,15 @@
         ...,
         description=(
             "Path that is exported by the NFS server. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#nfs"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "ReadOnly here will force the NFS export to be mounted with read-only permissions. Defaults to false. More"
             " info: https://kubernetes.io/docs/concepts/storage/volumes#nfs"
         ),
     )
     server: str = Field(
@@ -419,89 +424,89 @@
         description=(
             "Represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists, DoesNotExist."
             ' Gt, and Lt.\n\nPossible enum values:\n - `"DoesNotExist"`\n - `"Exists"`\n - `"Gt"`\n - `"In"`\n -'
             ' `"Lt"`\n - `"NotIn"`'
         ),
     )
     values: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the"
             " operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the"
             " values array must have a single element, which will be interpreted as an integer. This array is replaced"
             " during a strategic merge patch."
         ),
     )
 
 
 class NodeSelectorTerm(BaseModel):
     match_expressions: Optional[List[NodeSelectorRequirement]] = Field(
-        None, alias="matchExpressions", description="A list of node selector requirements by node's labels."
+        default=None, alias="matchExpressions", description="A list of node selector requirements by node's labels."
     )
     match_fields: Optional[List[NodeSelectorRequirement]] = Field(
-        None, alias="matchFields", description="A list of node selector requirements by node's fields."
+        default=None, alias="matchFields", description="A list of node selector requirements by node's fields."
     )
 
 
 class ObjectFieldSelector(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description='Version of the schema the FieldPath is written in terms of, defaults to "v1".',
     )
     field_path: str = Field(
         ..., alias="fieldPath", description="Path of the field to select in the specified API version."
     )
 
 
 class ObjectReference(BaseModel):
-    api_version: Optional[str] = Field(None, alias="apiVersion", description="API version of the referent.")
+    api_version: Optional[str] = Field(default=None, alias="apiVersion", description="API version of the referent.")
     field_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="fieldPath",
         description=(
             "If referring to a piece of an object instead of an entire object, this string should contain a valid"
             " JSON/Go field access statement, such as desiredState.manifest.containers[2]. For example, if the object"
             ' reference is to a container within a pod, this would take on a value like: "spec.containers{name}"'
             ' (where "name" refers to the name of the container that triggered the event) or if no container name is'
             ' specified "spec.containers[2]" (container with index 2 in this pod). This syntax is chosen only to have'
             " some well-defined way of referencing a part of an object."
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind of the referent. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
     namespace: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Namespace of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"
         ),
     )
     resource_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="resourceVersion",
         description=(
             "Specific resourceVersion to which this reference is made, if any. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"
         ),
     )
     uid: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "UID of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#uids"
         ),
     )
 
 
@@ -522,46 +527,46 @@
         alias="claimName",
         description=(
             "ClaimName is the name of a PersistentVolumeClaim in the same namespace as the pod using this volume. More"
             " info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims"
         ),
     )
     read_only: Optional[bool] = Field(
-        None, alias="readOnly", description="Will force the ReadOnly setting in VolumeMounts. Default false."
+        default=None, alias="readOnly", description="Will force the ReadOnly setting in VolumeMounts. Default false."
     )
 
 
 class PhotonPersistentDiskVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
     pd_id: str = Field(..., alias="pdID", description="ID that identifies Photon Controller persistent disk")
 
 
 class PodDNSConfigOption(BaseModel):
-    name: Optional[str] = Field(None, description="Required.")
+    name: Optional[str] = Field(default=None, description="Required.")
     value: Optional[str] = None
 
 
 class PortworxVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "FSType represents the filesystem type to mount Must be a filesystem type supported by the host operating"
             ' system. Ex. "ext4", "xfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.",
     )
     volume_id: str = Field(..., alias="volumeID", description="VolumeID uniquely identifies a Portworx volume")
 
 
 class PreferredSchedulingTerm(BaseModel):
@@ -570,152 +575,166 @@
     )
     weight: int = Field(
         ..., description="Weight associated with matching the corresponding nodeSelectorTerm, in the range 1-100."
     )
 
 
 class QuobyteVolumeSource(BaseModel):
-    group: Optional[str] = Field(None, description="Group to map volume access to Default is no group")
+    group: Optional[str] = Field(default=None, description="Group to map volume access to Default is no group")
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "ReadOnly here will force the Quobyte volume to be mounted with read-only permissions. Defaults to false."
         ),
     )
     registry: str = Field(
         ...,
         description=(
             "Registry represents a single or multiple Quobyte Registry services specified as a string as host:port"
             " pair (multiple entries are separated with commas) which acts as the central registry for volumes"
         ),
     )
     tenant: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Tenant owning the given Quobyte volume in the Backend Used with dynamically provisioned Quobyte volumes,"
             " value is set by the plugin"
         ),
     )
-    user: Optional[str] = Field(None, description="User to map volume access to Defaults to serivceaccount user")
+    user: Optional[str] = Field(
+        default=None, description="User to map volume access to Defaults to serivceaccount user"
+    )
     volume: str = Field(
         ..., description="Volume is a string that references an already created Quobyte volume by name."
     )
 
 
 class RBDVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported"
             ' by the host operating system. Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if'
             " unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#rbd"
         ),
     )
     image: str = Field(
         ..., description="The rados image name. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
     )
     keyring: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Keyring is the path to key ring for RBDUser. Default is /etc/ceph/keyring. More info:"
             " https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
     monitors: List[str] = Field(
         ...,
         description=(
             "A collection of Ceph monitors. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
     pool: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "The rados pool name. Default is rbd. More info:"
             " https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false. More info:"
             " https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="secretRef",
         description=(
             "SecretRef is name of the authentication secret for RBDUser. If provided overrides keyring. Default is"
             " nil. More info: https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
     user: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "The rados user name. Default is admin. More info:"
             " https://examples.k8s.io/volumes/rbd/README.md#how-to-use-it"
         ),
     )
 
 
 class SELinuxOptions(BaseModel):
-    level: Optional[str] = Field(None, description="Level is SELinux level label that applies to the container.")
-    role: Optional[str] = Field(None, description="Role is a SELinux role label that applies to the container.")
-    type: Optional[str] = Field(None, description="Type is a SELinux type label that applies to the container.")
-    user: Optional[str] = Field(None, description="User is a SELinux user label that applies to the container.")
+    level: Optional[str] = Field(
+        default=None, description="Level is SELinux level label that applies to the container."
+    )
+    role: Optional[str] = Field(
+        default=None, description="Role is a SELinux role label that applies to the container."
+    )
+    type: Optional[str] = Field(
+        default=None, description="Type is a SELinux type label that applies to the container."
+    )
+    user: Optional[str] = Field(
+        default=None, description="User is a SELinux user label that applies to the container."
+    )
 
 
 class ScaleIOVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Default is "xfs".'
         ),
     )
     gateway: str = Field(..., description="The host address of the ScaleIO API Gateway.")
     protection_domain: Optional[str] = Field(
-        None,
+        default=None,
         alias="protectionDomain",
         description="The name of the ScaleIO Protection Domain for the configured storage.",
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.",
     )
     secret_ref: LocalObjectReference = Field(
         ...,
         alias="secretRef",
         description=(
             "SecretRef references to the secret for ScaleIO user and other sensitive information. If this is not"
             " provided, Login operation will fail."
         ),
     )
     ssl_enabled: Optional[bool] = Field(
-        None, alias="sslEnabled", description="Flag to enable/disable SSL communication with Gateway, default false"
+        default=None,
+        alias="sslEnabled",
+        description="Flag to enable/disable SSL communication with Gateway, default false",
     )
     storage_mode: Optional[str] = Field(
-        None,
+        default=None,
         alias="storageMode",
         description=(
             "Indicates whether the storage for a volume should be ThickProvisioned or ThinProvisioned. Default is"
             " ThinProvisioned."
         ),
     )
     storage_pool: Optional[str] = Field(
-        None, alias="storagePool", description="The ScaleIO Storage Pool associated with the protection domain."
+        default=None,
+        alias="storagePool",
+        description="The ScaleIO Storage Pool associated with the protection domain.",
     )
     system: str = Field(..., description="The name of the storage system as configured in ScaleIO.")
     volume_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="volumeName",
         description=(
             "The name of a volume already created in the ScaleIO system that is associated with this volume source."
         ),
     )
 
 
@@ -723,15 +742,15 @@
     localhost = "Localhost"
     runtime_default = "RuntimeDefault"
     unconfined = "Unconfined"
 
 
 class SeccompProfile(BaseModel):
     localhost_profile: Optional[str] = Field(
-        None,
+        default=None,
         alias="localhostProfile",
         description=(
             "localhostProfile indicates a profile defined in a file on the node should be used. The profile must be"
             " preconfigured on the node to work. Must be a descending path, relative to the kubelet's configured"
             ' seccomp profile location. Must only be set if type is "Localhost".'
         ),
     )
@@ -746,100 +765,102 @@
             ' profile.\n - `"Unconfined"` indicates no seccomp profile is applied (A.K.A. unconfined).'
         ),
     )
 
 
 class SecretEnvSource(BaseModel):
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the Secret must be defined")
+    optional: Optional[bool] = Field(default=None, description="Specify whether the Secret must be defined")
 
 
 class SecretKeySelector(BaseModel):
     key: str = Field(..., description="The key of the secret to select from.  Must be a valid secret key.")
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the Secret or its key must be defined")
+    optional: Optional[bool] = Field(default=None, description="Specify whether the Secret or its key must be defined")
 
 
 class SecretProjection(BaseModel):
     items: Optional[List[KeyToPath]] = Field(
-        None,
+        default=None,
         description=(
             "If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the"
             " volume as a file whose name is the key and content is the value. If specified, the listed keys will be"
             " projected into the specified paths, and unlisted keys will not be present. If a key is specified which"
             " is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be"
             " relative and may not contain the '..' path or start with '..'."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the Secret or its key must be defined")
+    optional: Optional[bool] = Field(default=None, description="Specify whether the Secret or its key must be defined")
 
 
 class SecretVolumeSource(BaseModel):
     default_mode: Optional[int] = Field(
-        None,
+        default=None,
         alias="defaultMode",
         description=(
             "Optional: mode bits used to set permissions on created files by default. Must be an octal value between"
             " 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON"
             " requires decimal values for mode bits. Defaults to 0644. Directories within the path are not affected by"
             " this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and"
             " the result can be other mode bits set."
         ),
     )
     items: Optional[List[KeyToPath]] = Field(
-        None,
+        default=None,
         description=(
             "If unspecified, each key-value pair in the Data field of the referenced Secret will be projected into the"
             " volume as a file whose name is the key and content is the value. If specified, the listed keys will be"
             " projected into the specified paths, and unlisted keys will not be present. If a key is specified which"
             " is not present in the Secret, the volume setup will error unless it is marked optional. Paths must be"
             " relative and may not contain the '..' path or start with '..'."
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the Secret or its keys must be defined")
+    optional: Optional[bool] = Field(
+        default=None, description="Specify whether the Secret or its keys must be defined"
+    )
     secret_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="secretName",
         description=(
             "Name of the secret in the pod's namespace to use. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#secret"
         ),
     )
 
 
 class ServiceAccountTokenProjection(BaseModel):
     audience: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Audience is the intended audience of the token. A recipient of a token must identify itself with an"
             " identifier specified in the audience of the token, and otherwise should reject the token. The audience"
             " defaults to the identifier of the apiserver."
         ),
     )
     expiration_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="expirationSeconds",
         description=(
             "ExpirationSeconds is the requested duration of validity of the service account token. As the token"
             " approaches expiration, the kubelet volume plugin will proactively rotate the service account token. The"
             " kubelet will start trying to rotate the token if the token is older than 80 percent of its time to live"
             " or if the token is older than 24 hours.Defaults to 1 hour and must be at least 10 minutes."
         ),
@@ -847,44 +868,44 @@
     path: str = Field(
         ..., description="Path is the path relative to the mount point of the file to project the token into."
     )
 
 
 class StorageOSVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts.",
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="secretRef",
         description=(
             "SecretRef specifies the secret to use for obtaining the StorageOS API credentials.  If not specified,"
             " default values will be attempted."
         ),
     )
     volume_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="volumeName",
         description=(
             "VolumeName is the human-readable name of the StorageOS volume.  Volume names are only unique within a"
             " namespace."
         ),
     )
     volume_namespace: Optional[str] = Field(
-        None,
+        default=None,
         alias="volumeNamespace",
         description=(
             "VolumeNamespace specifies the scope of the volume within StorageOS.  If no namespace is specified then"
             " the Pod's namespace will be used.  This allows the Kubernetes name scoping to be mirrored within"
             " StorageOS for tighter integration. Set VolumeName to any name to override the default behaviour. Set to"
             ' "default" if you are not using namespaces within StorageOS. Namespaces that do not pre-exist within'
             " StorageOS will be created."
@@ -906,63 +927,63 @@
 class OperatorModel(Enum):
     equal = "Equal"
     exists = "Exists"
 
 
 class Toleration(BaseModel):
     effect: Optional[Effect] = Field(
-        None,
+        default=None,
         description=(
             "Effect indicates the taint effect to match. Empty means match all taint effects. When specified, allowed"
             ' values are NoSchedule, PreferNoSchedule and NoExecute.\n\nPossible enum values:\n - `"NoExecute"` Evict'
             " any already-running pods that do not tolerate the taint. Currently enforced by NodeController.\n -"
             ' `"NoSchedule"` Do not allow new pods to schedule onto the node unless they tolerate the taint, but allow'
             " all pods submitted to Kubelet without going through the scheduler to start, and allow all"
             ' already-running pods to continue running. Enforced by the scheduler.\n - `"PreferNoSchedule"` Like'
             " TaintEffectNoSchedule, but the scheduler tries not to schedule new pods onto the node, rather than"
             " prohibiting new pods from scheduling onto the node entirely. Enforced by the scheduler."
         ),
     )
     key: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Key is the taint key that the toleration applies to. Empty means match all taint keys. If the key is"
             " empty, operator must be Exists; this combination means to match all values and all keys."
         ),
     )
     operator: Optional[OperatorModel] = Field(
-        None,
+        default=None,
         description=(
             "Operator represents a key's relationship to the value. Valid operators are Exists and Equal. Defaults to"
             " Equal. Exists is equivalent to wildcard for value, so that a pod can tolerate all taints of a particular"
             ' category.\n\nPossible enum values:\n - `"Equal"`\n - `"Exists"`'
         ),
     )
     toleration_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="tolerationSeconds",
         description=(
             "TolerationSeconds represents the period of time the toleration (which must be of effect NoExecute,"
             " otherwise this field is ignored) tolerates the taint. By default, it is not set, which means tolerate"
             " the taint forever (do not evict). Zero and negative values will be treated as 0 (evict immediately) by"
             " the system."
         ),
     )
     value: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Value is the taint value the toleration matches to. If the operator is Exists, the value should be empty,"
             " otherwise just a regular string."
         ),
     )
 
 
 class TypedLocalObjectReference(BaseModel):
     api_group: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiGroup",
         description=(
             "APIGroup is the group for the resource being referenced. If APIGroup is not specified, the specified Kind"
             " must be in the core API group. For any other third-party types, APIGroup is required."
         ),
     )
     kind: str = Field(..., description="Kind is the type of resource being referenced")
@@ -981,94 +1002,94 @@
 class VolumeMount(BaseModel):
     mount_path: str = Field(
         ...,
         alias="mountPath",
         description="Path within the container at which the volume should be mounted.  Must not contain ':'.",
     )
     mount_propagation: Optional[str] = Field(
-        None,
+        default=None,
         alias="mountPropagation",
         description=(
             "mountPropagation determines how mounts are propagated from the host to container and the other way"
             " around. When not set, MountPropagationNone is used. This field is beta in 1.10."
         ),
     )
     name: str = Field(..., description="This must match the Name of a Volume.")
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false.",
     )
     sub_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="subPath",
         description=(
             "Path within the volume from which the container's volume should be mounted. Defaults to \"\" (volume's"
             " root)."
         ),
     )
     sub_path_expr: Optional[str] = Field(
-        None,
+        default=None,
         alias="subPathExpr",
         description=(
             "Expanded path within the volume from which the container's volume should be mounted. Behaves similarly"
             " to SubPath but environment variable references $(VAR_NAME) are expanded using the container's"
             ' environment. Defaults to "" (volume\'s root). SubPathExpr and SubPath are mutually exclusive.'
         ),
     )
 
 
 class VsphereVirtualDiskVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified.'
         ),
     )
     storage_policy_id: Optional[str] = Field(
-        None,
+        default=None,
         alias="storagePolicyID",
         description="Storage Policy Based Management (SPBM) profile ID associated with the StoragePolicyName.",
     )
     storage_policy_name: Optional[str] = Field(
-        None, alias="storagePolicyName", description="Storage Policy Based Management (SPBM) profile name."
+        default=None, alias="storagePolicyName", description="Storage Policy Based Management (SPBM) profile name."
     )
     volume_path: str = Field(..., alias="volumePath", description="Path that identifies vSphere volume vmdk")
 
 
 class WindowsSecurityContextOptions(BaseModel):
     gmsa_credential_spec: Optional[str] = Field(
-        None,
+        default=None,
         alias="gmsaCredentialSpec",
         description=(
             "GMSACredentialSpec is where the GMSA admission webhook (https://github.com/kubernetes-sigs/windows-gmsa)"
             " inlines the contents of the GMSA credential spec named by the GMSACredentialSpecName field."
         ),
     )
     gmsa_credential_spec_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="gmsaCredentialSpecName",
         description="GMSACredentialSpecName is the name of the GMSA credential spec to use.",
     )
     host_process: Optional[bool] = Field(
-        None,
+        default=None,
         alias="hostProcess",
         description=(
             "HostProcess determines if a container should be run as a 'Host Process' container. This field is"
             " alpha-level and will only be honored by components that enable the WindowsHostProcessContainers feature"
             " flag. Setting this field without the feature flag will result in errors when validating the Pod. All of"
             " a Pod's containers must have the same effective HostProcess value (it is not allowed to have a mix of"
             " HostProcess containers and non-HostProcess containers).  In addition, if HostProcess is true then"
             " HostNetwork must also be set to true."
         ),
     )
     run_as_user_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="runAsUserName",
         description=(
             "The UserName in Windows to run the entrypoint of the container process. Defaults to the user specified in"
             " image metadata if unspecified. May also be set in PodSecurityContext. If set in both SecurityContext and"
             " PodSecurityContext, the value specified in SecurityContext takes precedence."
         ),
     )
@@ -1079,38 +1100,38 @@
         ...,
         description=(
             "Driver is the name of the CSI driver that handles this volume. Consult with your admin for the correct"
             " name as registered in the cluster."
         ),
     )
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Ex. "ext4", "xfs", "ntfs". If not provided, the empty value is passed to the'
             " associated CSI driver which will determine the default filesystem to apply."
         ),
     )
     node_publish_secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="nodePublishSecretRef",
         description=(
             "NodePublishSecretRef is a reference to the secret object containing sensitive information to pass to the"
             " CSI driver to complete the CSI NodePublishVolume and NodeUnpublishVolume calls. This field is optional,"
             " and  may be empty if no secret is required. If the secret object contains more than one secret, all"
             " secret references are passed."
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="Specifies a read-only configuration for the volume. Defaults to false (read/write).",
     )
     volume_attributes: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         alias="volumeAttributes",
         description=(
             "VolumeAttributes stores driver-specific properties that are passed to the CSI driver. Consult your"
             " driver's documentation for supported values."
         ),
     )
 
@@ -1120,69 +1141,69 @@
         ...,
         description=(
             "Required: Monitors is a collection of Ceph monitors More info:"
             " https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it"
         ),
     )
     path: Optional[str] = Field(
-        None, description="Optional: Used as the mounted root, rather than the full Ceph tree, default is /"
+        default=None, description="Optional: Used as the mounted root, rather than the full Ceph tree, default is /"
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts."
             " More info: https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it"
         ),
     )
     secret_file: Optional[str] = Field(
-        None,
+        default=None,
         alias="secretFile",
         description=(
             "Optional: SecretFile is the path to key ring for User, default is /etc/ceph/user.secret More info:"
             " https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it"
         ),
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="secretRef",
         description=(
             "Optional: SecretRef is reference to the authentication secret for User, default is empty. More info:"
             " https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it"
         ),
     )
     user: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Optional: User is the rados user name, default is admin More info:"
             " https://examples.k8s.io/volumes/cephfs/README.md#how-to-use-it"
         ),
     )
 
 
 class CinderVolumeSource(BaseModel):
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "Filesystem type to mount. Must be a filesystem type supported by the host operating system. Examples:"
             ' "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if unspecified. More info:'
             " https://examples.k8s.io/mysql-cinder-pd/README.md"
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts."
             " More info: https://examples.k8s.io/mysql-cinder-pd/README.md"
         ),
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="secretRef",
         description="Optional: points to a secret object containing parameters used to connect to OpenStack.",
     )
     volume_id: str = Field(
         ...,
         alias="volumeID",
         description=(
@@ -1190,207 +1211,216 @@
             " https://examples.k8s.io/mysql-cinder-pd/README.md"
         ),
     )
 
 
 class ConfigMapProjection(BaseModel):
     items: Optional[List[KeyToPath]] = Field(
-        None,
+        default=None,
         description=(
             "If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into"
             " the volume as a file whose name is the key and content is the value. If specified, the listed keys will"
             " be projected into the specified paths, and unlisted keys will not be present. If a key is specified"
             " which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths"
             " must be relative and may not contain the '..' path or start with '..'."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the ConfigMap or its keys must be defined")
+    optional: Optional[bool] = Field(
+        default=None, description="Specify whether the ConfigMap or its keys must be defined"
+    )
 
 
 class ConfigMapVolumeSource(BaseModel):
     default_mode: Optional[int] = Field(
-        None,
+        default=None,
         alias="defaultMode",
         description=(
             "Optional: mode bits used to set permissions on created files by default. Must be an octal value between"
             " 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON"
             " requires decimal values for mode bits. Defaults to 0644. Directories within the path are not affected by"
             " this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and"
             " the result can be other mode bits set."
         ),
     )
     items: Optional[List[KeyToPath]] = Field(
-        None,
+        default=None,
         description=(
             "If unspecified, each key-value pair in the Data field of the referenced ConfigMap will be projected into"
             " the volume as a file whose name is the key and content is the value. If specified, the listed keys will"
             " be projected into the specified paths, and unlisted keys will not be present. If a key is specified"
             " which is not present in the ConfigMap, the volume setup will error unless it is marked optional. Paths"
             " must be relative and may not contain the '..' path or start with '..'."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the referent. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
-    optional: Optional[bool] = Field(None, description="Specify whether the ConfigMap or its keys must be defined")
+    optional: Optional[bool] = Field(
+        default=None, description="Specify whether the ConfigMap or its keys must be defined"
+    )
 
 
 class EmptyDirVolumeSource(BaseModel):
     medium: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'What type of storage medium should back this directory. The default is "" which means to use the node\'s'
             " default medium. Must be an empty string (default) or Memory. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#emptydir"
         ),
     )
     size_limit: Optional[resource.Quantity] = Field(
-        None,
+        default=None,
         alias="sizeLimit",
         description=(
             "Total amount of local storage required for this EmptyDir volume. The size limit is also applicable for"
             " memory medium. The maximum usage on memory medium EmptyDir would be the minimum value between the"
             " SizeLimit specified here and the sum of memory limits of all containers in a pod. The default is nil"
             " which means that the limit is undefined. More info:"
             " http://kubernetes.io/docs/user-guide/volumes#emptydir"
         ),
     )
 
 
 class EnvFromSource(BaseModel):
     config_map_ref: Optional[ConfigMapEnvSource] = Field(
-        None, alias="configMapRef", description="The ConfigMap to select from"
+        default=None, alias="configMapRef", description="The ConfigMap to select from"
     )
     prefix: Optional[str] = Field(
-        None, description="An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER."
+        default=None,
+        description="An optional identifier to prepend to each key in the ConfigMap. Must be a C_IDENTIFIER.",
+    )
+    secret_ref: Optional[SecretEnvSource] = Field(
+        default=None, alias="secretRef", description="The Secret to select from"
     )
-    secret_ref: Optional[SecretEnvSource] = Field(None, alias="secretRef", description="The Secret to select from")
 
 
 class EventSeries(BaseModel):
     count: Optional[int] = Field(
-        None, description="Number of occurrences in this series up to the last heartbeat time"
+        default=None, description="Number of occurrences in this series up to the last heartbeat time"
     )
     last_observed_time: Optional[v1.MicroTime] = Field(
-        None, alias="lastObservedTime", description="Time of the last occurrence observed"
+        default=None, alias="lastObservedTime", description="Time of the last occurrence observed"
     )
 
 
 class FlexVolumeSource(BaseModel):
     driver: str = Field(..., description="Driver is the name of the driver to use for this volume.")
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             'Filesystem type to mount. Must be a filesystem type supported by the host operating system. Ex. "ext4",'
             ' "xfs", "ntfs". The default filesystem depends on FlexVolume script.'
         ),
     )
-    options: Optional[Dict[str, str]] = Field(None, description="Optional: Extra command options if any.")
+    options: Optional[Dict[str, str]] = Field(default=None, description="Optional: Extra command options if any.")
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description=(
             "Optional: Defaults to false (read/write). ReadOnly here will force the ReadOnly setting in VolumeMounts."
         ),
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None,
+        default=None,
         alias="secretRef",
         description=(
             "Optional: SecretRef is reference to the secret object containing sensitive information to pass to the"
             " plugin scripts. This may be empty if no secret object is specified. If the secret object contains more"
             " than one secret, all secrets are passed to the plugin scripts."
         ),
     )
 
 
 class HTTPGetAction(BaseModel):
     host: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             'Host name to connect to, defaults to the pod IP. You probably want to set "Host" in httpHeaders instead.'
         ),
     )
     http_headers: Optional[List[HTTPHeader]] = Field(
-        None, alias="httpHeaders", description="Custom headers to set in the request. HTTP allows repeated headers."
+        default=None,
+        alias="httpHeaders",
+        description="Custom headers to set in the request. HTTP allows repeated headers.",
     )
-    path: Optional[str] = Field(None, description="Path to access on the HTTP server.")
+    path: Optional[str] = Field(default=None, description="Path to access on the HTTP server.")
     port: intstr.IntOrString = Field(
         ...,
         description=(
             "Name or number of the port to access on the container. Number must be in the range 1 to 65535. Name must"
             " be an IANA_SVC_NAME."
         ),
     )
     scheme: Optional[Scheme] = Field(
-        None,
+        default=None,
         description=(
             'Scheme to use for connecting to the host. Defaults to HTTP.\n\nPossible enum values:\n - `"HTTP"` means'
             ' that the scheme used will be http://\n - `"HTTPS"` means that the scheme used will be https://'
         ),
     )
 
 
 class ISCSIVolumeSource(BaseModel):
     chap_auth_discovery: Optional[bool] = Field(
-        None, alias="chapAuthDiscovery", description="whether support iSCSI Discovery CHAP authentication"
+        default=None, alias="chapAuthDiscovery", description="whether support iSCSI Discovery CHAP authentication"
     )
     chap_auth_session: Optional[bool] = Field(
-        None, alias="chapAuthSession", description="whether support iSCSI Session CHAP authentication"
+        default=None, alias="chapAuthSession", description="whether support iSCSI Session CHAP authentication"
     )
     fs_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsType",
         description=(
             "Filesystem type of the volume that you want to mount. Tip: Ensure that the filesystem type is supported"
             ' by the host operating system. Examples: "ext4", "xfs", "ntfs". Implicitly inferred to be "ext4" if'
             " unspecified. More info: https://kubernetes.io/docs/concepts/storage/volumes#iscsi"
         ),
     )
     initiator_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="initiatorName",
         description=(
             "Custom iSCSI Initiator Name. If initiatorName is specified with iscsiInterface simultaneously, new iSCSI"
             " interface <target portal>:<volume name> will be created for the connection."
         ),
     )
     iqn: str = Field(..., description="Target iSCSI Qualified Name.")
     iscsi_interface: Optional[str] = Field(
-        None,
+        default=None,
         alias="iscsiInterface",
         description="iSCSI Interface Name that uses an iSCSI transport. Defaults to 'default' (tcp).",
     )
     lun: int = Field(..., description="iSCSI Target Lun number.")
     portals: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "iSCSI Target Portal List. The portal is either an IP or ip_addr:port if the port is other than default"
             " (typically TCP ports 860 and 3260)."
         ),
     )
     read_only: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnly",
         description="ReadOnly here will force the ReadOnly setting in VolumeMounts. Defaults to false.",
     )
     secret_ref: Optional[LocalObjectReference] = Field(
-        None, alias="secretRef", description="CHAP Secret for iSCSI target and initiator authentication"
+        default=None, alias="secretRef", description="CHAP Secret for iSCSI target and initiator authentication"
     )
     target_portal: str = Field(
         ...,
         alias="targetPortal",
         description=(
             "iSCSI Target Portal. The Portal is either an IP or ip_addr:port if the port is other than default"
             " (typically TCP ports 860 and 3260)."
@@ -1402,26 +1432,26 @@
     node_selector_terms: List[NodeSelectorTerm] = Field(
         ..., alias="nodeSelectorTerms", description="Required. A list of node selector terms. The terms are ORed."
     )
 
 
 class PersistentVolumeClaimCondition(BaseModel):
     last_probe_time: Optional[v1.Time] = Field(
-        None, alias="lastProbeTime", description="Last time we probed the condition."
+        default=None, alias="lastProbeTime", description="Last time we probed the condition."
     )
     last_transition_time: Optional[v1.Time] = Field(
-        None,
+        default=None,
         alias="lastTransitionTime",
         description="Last time the condition transitioned from one status to another.",
     )
     message: Optional[str] = Field(
-        None, description="Human-readable message indicating details about last transition."
+        default=None, description="Human-readable message indicating details about last transition."
     )
     reason: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Unique, this should be a short, machine understandable string that gives the reason for condition's last"
             ' transition. If it reports "ResizeStarted" that means the underlying persistent volume is being resized.'
         ),
     )
     status: str
     type: Type = Field(
@@ -1431,394 +1461,394 @@
             ' system resize is pending on node\n - `"Resizing"` - a user trigger resize of pvc has been started'
         ),
     )
 
 
 class PersistentVolumeClaimStatus(BaseModel):
     access_modes: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="accessModes",
         description=(
             "AccessModes contains the actual access modes the volume backing the PVC has. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"
         ),
     )
     allocated_resources: Optional[Dict[str, resource.Quantity]] = Field(
-        None,
+        default=None,
         alias="allocatedResources",
         description=(
             "The storage resource within AllocatedResources tracks the capacity allocated to a PVC. It may be larger"
             " than the actual capacity when a volume expansion operation is requested. For storage quota, the larger"
             " value from allocatedResources and PVC.spec.resources is used. If allocatedResources is not set,"
             " PVC.spec.resources alone is used for quota calculation. If a volume expansion capacity request is"
             " lowered, allocatedResources is only lowered if there are no expansion operations in progress and if the"
             " actual volume capacity is equal or lower than the requested capacity. This is an alpha field and"
             " requires enabling RecoverVolumeExpansionFailure feature."
         ),
     )
     capacity: Optional[Dict[str, resource.Quantity]] = Field(
-        None, description="Represents the actual resources of the underlying volume."
+        default=None, description="Represents the actual resources of the underlying volume."
     )
     conditions: Optional[List[PersistentVolumeClaimCondition]] = Field(
-        None,
+        default=None,
         description=(
             "Current Condition of persistent volume claim. If underlying persistent volume is being resized then the"
             " Condition will be set to 'ResizeStarted'."
         ),
     )
     phase: Optional[Phase] = Field(
-        None,
+        default=None,
         description=(
             'Phase represents the current phase of PersistentVolumeClaim.\n\nPossible enum values:\n - `"Bound"` used'
             ' for PersistentVolumeClaims that are bound\n - `"Lost"` used for PersistentVolumeClaims that lost their'
             " underlying PersistentVolume. The claim was bound to a PersistentVolume and this volume does not exist"
             ' any longer and all data on it was lost.\n - `"Pending"` used for PersistentVolumeClaims that are not yet'
             " bound"
         ),
     )
     resize_status: Optional[str] = Field(
-        None,
+        default=None,
         alias="resizeStatus",
         description=(
             "ResizeStatus stores status of resize operation. ResizeStatus is not set by default but when expansion is"
             " complete resizeStatus is set to empty string by resize controller or kubelet. This is an alpha field and"
             " requires enabling RecoverVolumeExpansionFailure feature."
         ),
     )
 
 
 class PodDNSConfig(BaseModel):
     nameservers: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "A list of DNS name server IP addresses. This will be appended to the base nameservers generated from"
             " DNSPolicy. Duplicated nameservers will be removed."
         ),
     )
     options: Optional[List[PodDNSConfigOption]] = Field(
-        None,
+        default=None,
         description=(
             "A list of DNS resolver options. This will be merged with the base options generated from DNSPolicy."
             " Duplicated entries will be removed. Resolution options given in Options will override those that appear"
             " in the base DNSPolicy."
         ),
     )
     searches: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "A list of DNS search domains for host-name lookup. This will be appended to the base search paths"
             " generated from DNSPolicy. Duplicated search paths will be removed."
         ),
     )
 
 
 class PodSecurityContext(BaseModel):
     fs_group: Optional[int] = Field(
-        None,
+        default=None,
         alias="fsGroup",
         description=(
             "A special supplemental group that applies to all containers in a pod. Some volume types allow the Kubelet"
             " to change the ownership of that volume to be owned by the pod:\n\n1. The owning GID will be the FSGroup"
             " 2. The setgid bit is set (new files created in the volume will be owned by FSGroup) 3. The permission"
             " bits are OR'd with rw-rw----\n\nIf unset, the Kubelet will not modify the ownership and permissions of"
             " any volume. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     fs_group_change_policy: Optional[str] = Field(
-        None,
+        default=None,
         alias="fsGroupChangePolicy",
         description=(
             "fsGroupChangePolicy defines behavior of changing ownership and permission of the volume before being"
             " exposed inside Pod. This field will only apply to volume types which support fsGroup based ownership(and"
             " permissions). It will have no effect on ephemeral volume types such as: secret, configmaps and emptydir."
             ' Valid values are "OnRootMismatch" and "Always". If not specified, "Always" is used. Note that this field'
             " cannot be set when spec.os.name is windows."
         ),
     )
     run_as_group: Optional[int] = Field(
-        None,
+        default=None,
         alias="runAsGroup",
         description=(
             "The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in"
             " SecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in"
             " SecurityContext takes precedence for that container. Note that this field cannot be set when"
             " spec.os.name is windows."
         ),
     )
     run_as_non_root: Optional[bool] = Field(
-        None,
+        default=None,
         alias="runAsNonRoot",
         description=(
             "Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at"
             " runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If"
             " unset or false, no such validation will be performed. May also be set in SecurityContext.  If set in"
             " both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."
         ),
     )
     run_as_user: Optional[int] = Field(
-        None,
+        default=None,
         alias="runAsUser",
         description=(
             "The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if"
             " unspecified. May also be set in SecurityContext.  If set in both SecurityContext and PodSecurityContext,"
             " the value specified in SecurityContext takes precedence for that container. Note that this field cannot"
             " be set when spec.os.name is windows."
         ),
     )
     se_linux_options: Optional[SELinuxOptions] = Field(
-        None,
+        default=None,
         alias="seLinuxOptions",
         description=(
             "The SELinux context to be applied to all containers. If unspecified, the container runtime will allocate"
             " a random SELinux context for each container.  May also be set in SecurityContext.  If set in both"
             " SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence for that"
             " container. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     seccomp_profile: Optional[SeccompProfile] = Field(
-        None,
+        default=None,
         alias="seccompProfile",
         description=(
             "The seccomp options to use by the containers in this pod. Note that this field cannot be set when"
             " spec.os.name is windows."
         ),
     )
     supplemental_groups: Optional[List[int]] = Field(
-        None,
+        default=None,
         alias="supplementalGroups",
         description=(
             "A list of groups applied to the first process run in each container, in addition to the container's"
             " primary GID.  If unspecified, no groups will be added to any container. Note that this field cannot be"
             " set when spec.os.name is windows."
         ),
     )
     sysctls: Optional[List[Sysctl]] = Field(
-        None,
+        default=None,
         description=(
             "Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported sysctls (by the"
             " container runtime) might fail to launch. Note that this field cannot be set when spec.os.name is"
             " windows."
         ),
     )
     windows_options: Optional[WindowsSecurityContextOptions] = Field(
-        None,
+        default=None,
         alias="windowsOptions",
         description=(
             "The Windows specific settings applied to all containers. If unspecified, the options within a container's"
             " SecurityContext will be used. If set in both SecurityContext and PodSecurityContext, the value specified"
             " in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is linux."
         ),
     )
 
 
 class ResourceFieldSelector(BaseModel):
     container_name: Optional[str] = Field(
-        None, alias="containerName", description="Container name: required for volumes, optional for env vars"
+        default=None, alias="containerName", description="Container name: required for volumes, optional for env vars"
     )
     divisor: Optional[resource.Quantity] = Field(
-        None, description='Specifies the output format of the exposed resources, defaults to "1"'
+        default=None, description='Specifies the output format of the exposed resources, defaults to "1"'
     )
     resource: str = Field(..., description="Required: resource to select")
 
 
 class ResourceRequirements(BaseModel):
     limits: Optional[Dict[str, resource.Quantity]] = Field(
-        None,
+        default=None,
         description=(
             "Limits describes the maximum amount of compute resources allowed. More info:"
             " https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
     requests: Optional[Dict[str, resource.Quantity]] = Field(
-        None,
+        default=None,
         description=(
             "Requests describes the minimum amount of compute resources required. If Requests is omitted for a"
             " container, it defaults to Limits if that is explicitly specified, otherwise to an implementation-defined"
             " value. More info: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
 
 
 class SecurityContext(BaseModel):
     allow_privilege_escalation: Optional[bool] = Field(
-        None,
+        default=None,
         alias="allowPrivilegeEscalation",
         description=(
             "AllowPrivilegeEscalation controls whether a process can gain more privileges than its parent process."
             " This bool directly controls if the no_new_privs flag will be set on the container process."
             " AllowPrivilegeEscalation is true always when the container is: 1) run as Privileged 2) has CAP_SYS_ADMIN"
             " Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     capabilities: Optional[Capabilities] = Field(
-        None,
+        default=None,
         description=(
             "The capabilities to add/drop when running containers. Defaults to the default set of capabilities granted"
             " by the container runtime. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     privileged: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Run container in privileged mode. Processes in privileged containers are essentially equivalent to root"
             " on the host. Defaults to false. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     proc_mount: Optional[str] = Field(
-        None,
+        default=None,
         alias="procMount",
         description=(
             "procMount denotes the type of proc mount to use for the containers. The default is DefaultProcMount which"
             " uses the container runtime defaults for readonly paths and masked paths. This requires the ProcMountType"
             " feature flag to be enabled. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     read_only_root_filesystem: Optional[bool] = Field(
-        None,
+        default=None,
         alias="readOnlyRootFilesystem",
         description=(
             "Whether this container has a read-only root filesystem. Default is false. Note that this field cannot be"
             " set when spec.os.name is windows."
         ),
     )
     run_as_group: Optional[int] = Field(
-        None,
+        default=None,
         alias="runAsGroup",
         description=(
             "The GID to run the entrypoint of the container process. Uses runtime default if unset. May also be set in"
             " PodSecurityContext.  If set in both SecurityContext and PodSecurityContext, the value specified in"
             " SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is windows."
         ),
     )
     run_as_non_root: Optional[bool] = Field(
-        None,
+        default=None,
         alias="runAsNonRoot",
         description=(
             "Indicates that the container must run as a non-root user. If true, the Kubelet will validate the image at"
             " runtime to ensure that it does not run as UID 0 (root) and fail to start the container if it does. If"
             " unset or false, no such validation will be performed. May also be set in PodSecurityContext.  If set in"
             " both SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence."
         ),
     )
     run_as_user: Optional[int] = Field(
-        None,
+        default=None,
         alias="runAsUser",
         description=(
             "The UID to run the entrypoint of the container process. Defaults to user specified in image metadata if"
             " unspecified. May also be set in PodSecurityContext.  If set in both SecurityContext and"
             " PodSecurityContext, the value specified in SecurityContext takes precedence. Note that this field cannot"
             " be set when spec.os.name is windows."
         ),
     )
     se_linux_options: Optional[SELinuxOptions] = Field(
-        None,
+        default=None,
         alias="seLinuxOptions",
         description=(
             "The SELinux context to be applied to the container. If unspecified, the container runtime will allocate a"
             " random SELinux context for each container.  May also be set in PodSecurityContext.  If set in both"
             " SecurityContext and PodSecurityContext, the value specified in SecurityContext takes precedence. Note"
             " that this field cannot be set when spec.os.name is windows."
         ),
     )
     seccomp_profile: Optional[SeccompProfile] = Field(
-        None,
+        default=None,
         alias="seccompProfile",
         description=(
             "The seccomp options to use by this container. If seccomp options are provided at both the pod & container"
             " level, the container options override the pod options. Note that this field cannot be set when"
             " spec.os.name is windows."
         ),
     )
     windows_options: Optional[WindowsSecurityContextOptions] = Field(
-        None,
+        default=None,
         alias="windowsOptions",
         description=(
             "The Windows specific settings applied to all containers. If unspecified, the options from the"
             " PodSecurityContext will be used. If set in both SecurityContext and PodSecurityContext, the value"
             " specified in SecurityContext takes precedence. Note that this field cannot be set when spec.os.name is"
             " linux."
         ),
     )
 
 
 class ServicePort(BaseModel):
     app_protocol: Optional[str] = Field(
-        None,
+        default=None,
         alias="appProtocol",
         description=(
             "The application protocol for this port. This field follows standard Kubernetes label syntax. Un-prefixed"
             " names are reserved for IANA standard service names (as per RFC-6335 and"
             " http://www.iana.org/assignments/service-names). Non-standard protocols should use prefixed names such as"
             " mycompany.com/my-custom-protocol."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "The name of this port within the service. This must be a DNS_LABEL. All ports within a ServiceSpec must"
             " have unique names. When considering the endpoints for a Service, this must match the 'name' field in the"
             " EndpointPort. Optional if only one ServicePort is defined on this service."
         ),
     )
     node_port: Optional[int] = Field(
-        None,
+        default=None,
         alias="nodePort",
         description=(
             "The port on each node on which this service is exposed when type is NodePort or LoadBalancer.  Usually"
             " assigned by the system. If a value is specified, in-range, and not in use it will be used, otherwise the"
             " operation will fail.  If not specified, a port will be allocated if this Service requires one.  If this"
             " field is specified when creating a Service which does not need it, creation will fail. This field will"
             " be wiped when updating a Service to no longer need it (e.g. changing type from NodePort to ClusterIP)."
             " More info: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport"
         ),
     )
     port: int = Field(..., description="The port that will be exposed by this service.")
     protocol: Optional[Protocol] = Field(
-        None,
+        default=None,
         description=(
             'The IP protocol for this port. Supports "TCP", "UDP", and "SCTP". Default is TCP.\n\nPossible enum'
             ' values:\n - `"SCTP"` is the SCTP protocol.\n - `"TCP"` is the TCP protocol.\n - `"UDP"` is the UDP'
             " protocol."
         ),
     )
     target_port: Optional[intstr.IntOrString] = Field(
-        None,
+        default=None,
         alias="targetPort",
         description=(
             "Number or name of the port to access on the pods targeted by the service. Number must be in the range 1"
             " to 65535. Name must be an IANA_SVC_NAME. If this is a string, it will be looked up as a named port in"
             " the target Pod's container ports. If this is not specified, the value of the 'port' field is used (an"
             " identity map). This field is ignored for services with clusterIP=None, and should be omitted or set"
             " equal to the 'port' field. More info:"
             " https://kubernetes.io/docs/concepts/services-networking/service/#defining-a-service"
         ),
     )
 
 
 class TCPSocketAction(BaseModel):
-    host: Optional[str] = Field(None, description="Optional: Host name to connect to, defaults to the pod IP.")
+    host: Optional[str] = Field(default=None, description="Optional: Host name to connect to, defaults to the pod IP.")
     port: intstr.IntOrString = Field(
         ...,
         description=(
             "Number or name of the port to access on the container. Number must be in the range 1 to 65535. Name must"
             " be an IANA_SVC_NAME."
         ),
     )
 
 
 class DownwardAPIVolumeFile(BaseModel):
     field_ref: Optional[ObjectFieldSelector] = Field(
-        None,
+        default=None,
         alias="fieldRef",
         description=(
             "Required: Selects a field of the pod: only annotations, labels, name and namespace are supported."
         ),
     )
     mode: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "Optional: mode bits used to set permissions on this file, must be an octal value between 0000 and 0777 or"
             " a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal"
             " values for mode bits. If not specified, the volume defaultMode will be used. This might be in conflict"
             " with other options that affect the file mode, like fsGroup, and the result can be other mode bits set."
         ),
     )
@@ -1826,202 +1856,209 @@
         ...,
         description=(
             "Required: Path is  the relative path name of the file to be created. Must not be absolute or contain the"
             " '..' path. Must be utf-8 encoded. The first item of the relative path must not start with '..'"
         ),
     )
     resource_field_ref: Optional[ResourceFieldSelector] = Field(
-        None,
+        default=None,
         alias="resourceFieldRef",
         description=(
             "Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory,"
             " requests.cpu and requests.memory) are currently supported."
         ),
     )
 
 
 class DownwardAPIVolumeSource(BaseModel):
     default_mode: Optional[int] = Field(
-        None,
+        default=None,
         alias="defaultMode",
         description=(
             "Optional: mode bits to use on created files by default. Must be a Optional: mode bits used to set"
             " permissions on created files by default. Must be an octal value between 0000 and 0777 or a decimal value"
             " between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode"
             " bits. Defaults to 0644. Directories within the path are not affected by this setting. This might be in"
             " conflict with other options that affect the file mode, like fsGroup, and the result can be other mode"
             " bits set."
         ),
     )
     items: Optional[List[DownwardAPIVolumeFile]] = Field(
-        None, description="Items is a list of downward API volume file"
+        default=None, description="Items is a list of downward API volume file"
     )
 
 
 class EnvVarSource(BaseModel):
     config_map_key_ref: Optional[ConfigMapKeySelector] = Field(
-        None, alias="configMapKeyRef", description="Selects a key of a ConfigMap."
+        default=None, alias="configMapKeyRef", description="Selects a key of a ConfigMap."
     )
     field_ref: Optional[ObjectFieldSelector] = Field(
-        None,
+        default=None,
         alias="fieldRef",
         description=(
             "Selects a field of the pod: supports metadata.name, metadata.namespace, `metadata.labels['<KEY>']`,"
             " `metadata.annotations['<KEY>']`, spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP,"
             " status.podIPs."
         ),
     )
     resource_field_ref: Optional[ResourceFieldSelector] = Field(
-        None,
+        default=None,
         alias="resourceFieldRef",
         description=(
             "Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory,"
             " limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently"
             " supported."
         ),
     )
     secret_key_ref: Optional[SecretKeySelector] = Field(
-        None, alias="secretKeyRef", description="Selects a key of a secret in the pod's namespace"
+        default=None, alias="secretKeyRef", description="Selects a key of a secret in the pod's namespace"
     )
 
 
 class Event(BaseModel):
-    action: Optional[str] = Field(None, description="What action was taken/failed regarding to the Regarding object.")
+    action: Optional[str] = Field(
+        default=None, description="What action was taken/failed regarding to the Regarding object."
+    )
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
-    count: Optional[int] = Field(None, description="The number of times this event has occurred.")
+    count: Optional[int] = Field(default=None, description="The number of times this event has occurred.")
     event_time: Optional[v1.MicroTime] = Field(
-        None, alias="eventTime", description="Time when this Event was first observed."
+        default=None, alias="eventTime", description="Time when this Event was first observed."
     )
     first_timestamp: Optional[v1.Time] = Field(
-        None,
+        default=None,
         alias="firstTimestamp",
         description="The time at which the event was first recorded. (Time of server receipt is in TypeMeta.)",
     )
     involved_object: ObjectReference = Field(
         ..., alias="involvedObject", description="The object that this event is about."
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     last_timestamp: Optional[v1.Time] = Field(
-        None,
+        default=None,
         alias="lastTimestamp",
         description="The time at which the most recent occurrence of this event was recorded.",
     )
-    message: Optional[str] = Field(None, description="A human-readable description of the status of this operation.")
+    message: Optional[str] = Field(
+        default=None, description="A human-readable description of the status of this operation."
+    )
     metadata: v1.ObjectMeta = Field(
         ...,
         description=(
             "Standard object's metadata. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata"
         ),
     )
     reason: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "This should be a short, machine understandable string that gives the reason for the transition into the"
             " object's current status."
         ),
     )
-    related: Optional[ObjectReference] = Field(None, description="Optional secondary object for more complex actions.")
+    related: Optional[ObjectReference] = Field(
+        default=None, description="Optional secondary object for more complex actions."
+    )
     reporting_component: Optional[str] = Field(
-        None,
+        default=None,
         alias="reportingComponent",
         description="Name of the controller that emitted this Event, e.g. `kubernetes.io/kubelet`.",
     )
     reporting_instance: Optional[str] = Field(
-        None, alias="reportingInstance", description="ID of the controller instance, e.g. `kubelet-xyzf`."
+        default=None, alias="reportingInstance", description="ID of the controller instance, e.g. `kubelet-xyzf`."
     )
     series: Optional[EventSeries] = Field(
-        None, description="Data about the Event series this event represents or nil if it's a singleton Event."
+        default=None, description="Data about the Event series this event represents or nil if it's a singleton Event."
     )
     source: Optional[EventSource] = Field(
-        None, description="The component reporting this event. Should be a short machine understandable string."
+        default=None,
+        description="The component reporting this event. Should be a short machine understandable string.",
     )
     type: Optional[str] = Field(
-        None, description="Type of this event (Normal, Warning), new types could be added in the future"
+        default=None, description="Type of this event (Normal, Warning), new types could be added in the future"
     )
 
 
 class LifecycleHandler(BaseModel):
-    exec: Optional[ExecAction] = Field(None, description="Exec specifies the action to take.")
+    exec: Optional[ExecAction] = Field(default=None, description="Exec specifies the action to take.")
     http_get: Optional[HTTPGetAction] = Field(
-        None, alias="httpGet", description="HTTPGet specifies the http request to perform."
+        default=None, alias="httpGet", description="HTTPGet specifies the http request to perform."
     )
     tcp_socket: Optional[TCPSocketAction] = Field(
-        None,
+        default=None,
         alias="tcpSocket",
         description=(
             "Deprecated. TCPSocket is NOT supported as a LifecycleHandler and kept for the backward compatibility."
             " There are no validation of this field and lifecycle hooks will fail in runtime when tcp handler is"
             " specified."
         ),
     )
 
 
 class NodeAffinity(BaseModel):
     preferred_during_scheduling_ignored_during_execution: Optional[List[PreferredSchedulingTerm]] = Field(
-        None,
+        default=None,
         alias="preferredDuringSchedulingIgnoredDuringExecution",
         description=(
             "The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by"
             " this field, but it may choose a node that violates one or more of the expressions. The node that is most"
             " preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the"
             " scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute"
             ' a sum by iterating through the elements of this field and adding "weight" to the sum if the node matches'
             " the corresponding matchExpressions; the node(s) with the highest sum are the most preferred."
         ),
     )
     required_during_scheduling_ignored_during_execution: Optional[NodeSelector] = Field(
-        None,
+        default=None,
         alias="requiredDuringSchedulingIgnoredDuringExecution",
         description=(
             "If the affinity requirements specified by this field are not met at scheduling time, the pod will not be"
             " scheduled onto the node. If the affinity requirements specified by this field cease to be met at some"
             " point during pod execution (e.g. due to an update), the system may or may not try to eventually evict"
             " the pod from its node."
         ),
     )
 
 
 class PersistentVolumeClaimSpec(BaseModel):
     access_modes: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="accessModes",
         description=(
             "AccessModes contains the desired access modes the volume should have. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#access-modes-1"
         ),
     )
     data_source: Optional[TypedLocalObjectReference] = Field(
-        None,
+        default=None,
         alias="dataSource",
         description=(
             "This field can be used to specify either: * An existing VolumeSnapshot object"
             " (snapshot.storage.k8s.io/VolumeSnapshot) * An existing PVC (PersistentVolumeClaim) If the provisioner or"
             " an external controller can support the specified data source, it will create a new volume based on the"
             " contents of the specified data source. If the AnyVolumeDataSource feature gate is enabled, this field"
             " will always have the same contents as the DataSourceRef field."
         ),
     )
     data_source_ref: Optional[TypedLocalObjectReference] = Field(
-        None,
+        default=None,
         alias="dataSourceRef",
         description=(
             "Specifies the object from which to populate the volume with data, if a non-empty volume is desired. This"
             " may be any local object from a non-empty API group (non core object) or a PersistentVolumeClaim object."
             " When this field is specified, volume binding will only succeed if the type of the specified object"
             " matches some installed volume populator or dynamic provisioner. This field will replace the"
             " functionality of the DataSource field and as such if both fields are non-empty, they must have the same"
@@ -2031,51 +2068,51 @@
             " objects, DataSourceRef\n  allows any non-core object, as well as PersistentVolumeClaim objects.\n* While"
             " DataSource ignores disallowed values (dropping them), DataSourceRef\n  preserves all values, and"
             " generates an error if a disallowed value is\n  specified.\n(Alpha) Using this field requires the"
             " AnyVolumeDataSource feature gate to be enabled."
         ),
     )
     resources: Optional[ResourceRequirements] = Field(
-        None,
+        default=None,
         description=(
             "Resources represents the minimum resources the volume should have. If RecoverVolumeExpansionFailure"
             " feature is enabled users are allowed to specify resource requirements that are lower than previous value"
             " but must still be higher than capacity recorded in the status field of the claim. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#resources"
         ),
     )
     selector: Optional[v1.LabelSelector] = Field(
-        None, description="A label query over volumes to consider for binding."
+        default=None, description="A label query over volumes to consider for binding."
     )
     storage_class_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="storageClassName",
         description=(
             "Name of the StorageClass required by the claim. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1"
         ),
     )
     volume_mode: Optional[str] = Field(
-        None,
+        default=None,
         alias="volumeMode",
         description=(
             "volumeMode defines what type of volume is required by the claim. Value of Filesystem is implied when not"
             " included in claim spec."
         ),
     )
     volume_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="volumeName",
         description="VolumeName is the binding reference to the PersistentVolume backing this claim.",
     )
 
 
 class PersistentVolumeClaimTemplate(BaseModel):
     metadata: Optional[v1.ObjectMeta] = Field(
-        None,
+        default=None,
         description=(
             "May contain labels and annotations that will be copied into the PVC when creating it. No other fields are"
             " allowed and will be rejected during validation."
         ),
     )
     spec: PersistentVolumeClaimSpec = Field(
         ...,
@@ -2084,29 +2121,29 @@
             " gets created from this template. The same fields as in a PersistentVolumeClaim are also valid here."
         ),
     )
 
 
 class PodAffinityTerm(BaseModel):
     label_selector: Optional[v1.LabelSelector] = Field(
-        None, alias="labelSelector", description="A label query over a set of resources, in this case pods."
+        default=None, alias="labelSelector", description="A label query over a set of resources, in this case pods."
     )
     namespace_selector: Optional[v1.LabelSelector] = Field(
-        None,
+        default=None,
         alias="namespaceSelector",
         description=(
             "A label query over the set of namespaces that the term applies to. The term is applied to the union of"
             " the namespaces selected by this field and the ones listed in the namespaces field. null selector and"
             ' null or empty namespaces list means "this pod\'s namespace". An empty selector ({}) matches all'
             " namespaces. This field is beta-level and is only honored when PodAffinityNamespaceSelector feature is"
             " enabled."
         ),
     )
     namespaces: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "namespaces specifies a static list of namespace names that the term applies to. The term is applied to"
             " the union of the namespaces listed in this field and the ones selected by namespaceSelector. null or"
             ' empty namespaces list and null namespaceSelector means "this pod\'s namespace"'
         ),
     )
     topology_key: str = Field(
@@ -2118,73 +2155,73 @@
             " of the label with key topologyKey matches that of any node on which any of the selected pods is running."
             " Empty topologyKey is not allowed."
         ),
     )
 
 
 class Probe(BaseModel):
-    exec: Optional[ExecAction] = Field(None, description="Exec specifies the action to take.")
+    exec: Optional[ExecAction] = Field(default=None, description="Exec specifies the action to take.")
     failure_threshold: Optional[int] = Field(
-        None,
+        default=None,
         alias="failureThreshold",
         description=(
             "Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3."
             " Minimum value is 1."
         ),
     )
     grpc: Optional[GRPCAction] = Field(
-        None,
+        default=None,
         description=(
             "GRPC specifies an action involving a GRPC port. This is an alpha field and requires enabling"
             " GRPCContainerProbe feature gate."
         ),
     )
     http_get: Optional[HTTPGetAction] = Field(
-        None, alias="httpGet", description="HTTPGet specifies the http request to perform."
+        default=None, alias="httpGet", description="HTTPGet specifies the http request to perform."
     )
     initial_delay_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="initialDelaySeconds",
         description=(
             "Number of seconds after the container has started before liveness probes are initiated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     period_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="periodSeconds",
         description="How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1.",
     )
     success_threshold: Optional[int] = Field(
-        None,
+        default=None,
         alias="successThreshold",
         description=(
             "Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to"
             " 1. Must be 1 for liveness and startup. Minimum value is 1."
         ),
     )
     tcp_socket: Optional[TCPSocketAction] = Field(
-        None, alias="tcpSocket", description="TCPSocket specifies an action involving a TCP port."
+        default=None, alias="tcpSocket", description="TCPSocket specifies an action involving a TCP port."
     )
     termination_grace_period_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="terminationGracePeriodSeconds",
         description=(
             "Optional duration in seconds the pod needs to terminate gracefully upon probe failure. The grace period"
             " is the duration in seconds after the processes running in the pod are sent a termination signal and the"
             " time when the processes are forcibly halted with a kill signal. Set this value longer than the expected"
             " cleanup time for your process. If this value is nil, the pod's terminationGracePeriodSeconds will be"
             " used. Otherwise, this value overrides the value provided by the pod spec. Value must be non-negative"
             " integer. The value zero indicates stop immediately via the kill signal (no opportunity to shut down)."
             " This is a beta field and requires enabling ProbeTerminationGracePeriod feature gate. Minimum value is 1."
             " spec.terminationGracePeriodSeconds is used if unset."
         ),
     )
     timeout_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="timeoutSeconds",
         description=(
             "Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
 
@@ -2197,39 +2234,41 @@
     )
     weight: int = Field(
         ..., description="weight associated with matching the corresponding podAffinityTerm, in the range 1-100."
     )
 
 
 class DownwardAPIProjection(BaseModel):
-    items: Optional[List[DownwardAPIVolumeFile]] = Field(None, description="Items is a list of DownwardAPIVolume file")
+    items: Optional[List[DownwardAPIVolumeFile]] = Field(
+        default=None, description="Items is a list of DownwardAPIVolume file"
+    )
 
 
 class EnvVar(BaseModel):
     name: str = Field(..., description="Name of the environment variable. Must be a C_IDENTIFIER.")
     value: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the"
             " container and any service environment variables. If a variable cannot be resolved, the reference in the"
             " input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the"
             ' $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)". Escaped'
             ' references will never be expanded, regardless of whether the variable exists or not. Defaults to "".'
         ),
     )
     value_from: Optional[EnvVarSource] = Field(
-        None,
+        default=None,
         alias="valueFrom",
         description="Source for the environment variable's value. Cannot be used if value is not empty.",
     )
 
 
 class EphemeralVolumeSource(BaseModel):
     volume_claim_template: Optional[PersistentVolumeClaimTemplate] = Field(
-        None,
+        default=None,
         alias="volumeClaimTemplate",
         description=(
             "Will be used to create a stand-alone PVC to provision the volume. The pod in which this"
             " EphemeralVolumeSource is embedded will be the owner of the PVC, i.e. the PVC will be deleted together"
             " with the pod.  The name of the PVC will be `<pod name>-<volume name>` where `<volume name>` is the name"
             " from the `PodSpec.Volumes` array entry. Pod validation will reject the pod if the concatenated name is"
             " not valid for a PVC (for example, too long).\n\nAn existing PVC with that name that is not owned by the"
@@ -2241,25 +2280,25 @@
             " must not be nil."
         ),
     )
 
 
 class Lifecycle(BaseModel):
     post_start: Optional[LifecycleHandler] = Field(
-        None,
+        default=None,
         alias="postStart",
         description=(
             "PostStart is called immediately after a container is created. If the handler fails, the container is"
             " terminated and restarted according to its restart policy. Other management of the container blocks until"
             " the hook completes. More info:"
             " https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks"
         ),
     )
     pre_stop: Optional[LifecycleHandler] = Field(
-        None,
+        default=None,
         alias="preStop",
         description=(
             "PreStop is called immediately before a container is terminated due to an API request or management event"
             " such as liveness/startup probe failure, preemption, resource contention, etc. The handler is not called"
             " if the container crashes or exits. The Pod's termination grace period countdown begins before the"
             " PreStop hook is executed. Regardless of the outcome of the handler, the container will eventually"
             " terminate within the Pod's termination grace period (unless delayed by finalizers). Other management of"
@@ -2267,172 +2306,176 @@
             " info: https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks"
         ),
     )
 
 
 class PersistentVolumeClaim(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the versioned schema of this representation of an object. Servers should convert"
             " recognized schemas to the latest internal value, and may reject unrecognized values. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources"
         ),
     )
     kind: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Kind is a string value representing the REST resource this object represents. Servers may infer this from"
             " the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
     metadata: Optional[v1.ObjectMeta] = Field(
-        None,
+        default=None,
         description=(
             "Standard object's metadata. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata"
         ),
     )
     spec: Optional[PersistentVolumeClaimSpec] = Field(
-        None,
+        default=None,
         description=(
             "Spec defines the desired characteristics of a volume requested by a pod author. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims"
         ),
     )
     status: Optional[PersistentVolumeClaimStatus] = Field(
-        None,
+        default=None,
         description=(
             "Status represents the current information/status of a persistent volume claim. Read-only. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims"
         ),
     )
 
 
 class PodAffinity(BaseModel):
     preferred_during_scheduling_ignored_during_execution: Optional[List[WeightedPodAffinityTerm]] = Field(
-        None,
+        default=None,
         alias="preferredDuringSchedulingIgnoredDuringExecution",
         description=(
             "The scheduler will prefer to schedule pods to nodes that satisfy the affinity expressions specified by"
             " this field, but it may choose a node that violates one or more of the expressions. The node that is most"
             " preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the"
             " scheduling requirements (resource request, requiredDuringScheduling affinity expressions, etc.), compute"
             ' a sum by iterating through the elements of this field and adding "weight" to the sum if the node has'
             " pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most"
             " preferred."
         ),
     )
     required_during_scheduling_ignored_during_execution: Optional[List[PodAffinityTerm]] = Field(
-        None,
+        default=None,
         alias="requiredDuringSchedulingIgnoredDuringExecution",
         description=(
             "If the affinity requirements specified by this field are not met at scheduling time, the pod will not be"
             " scheduled onto the node. If the affinity requirements specified by this field cease to be met at some"
             " point during pod execution (e.g. due to a pod label update), the system may or may not try to eventually"
             " evict the pod from its node. When there are multiple elements, the lists of nodes corresponding to each"
             " podAffinityTerm are intersected, i.e. all terms must be satisfied."
         ),
     )
 
 
 class PodAntiAffinity(BaseModel):
     preferred_during_scheduling_ignored_during_execution: Optional[List[WeightedPodAffinityTerm]] = Field(
-        None,
+        default=None,
         alias="preferredDuringSchedulingIgnoredDuringExecution",
         description=(
             "The scheduler will prefer to schedule pods to nodes that satisfy the anti-affinity expressions specified"
             " by this field, but it may choose a node that violates one or more of the expressions. The node that is"
             " most preferred is the one with the greatest sum of weights, i.e. for each node that meets all of the"
             " scheduling requirements (resource request, requiredDuringScheduling anti-affinity expressions, etc.),"
             ' compute a sum by iterating through the elements of this field and adding "weight" to the sum if the node'
             " has pods which matches the corresponding podAffinityTerm; the node(s) with the highest sum are the most"
             " preferred."
         ),
     )
     required_during_scheduling_ignored_during_execution: Optional[List[PodAffinityTerm]] = Field(
-        None,
+        default=None,
         alias="requiredDuringSchedulingIgnoredDuringExecution",
         description=(
             "If the anti-affinity requirements specified by this field are not met at scheduling time, the pod will"
             " not be scheduled onto the node. If the anti-affinity requirements specified by this field cease to be"
             " met at some point during pod execution (e.g. due to a pod label update), the system may or may not try"
             " to eventually evict the pod from its node. When there are multiple elements, the lists of nodes"
             " corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied."
         ),
     )
 
 
 class VolumeProjection(BaseModel):
     config_map: Optional[ConfigMapProjection] = Field(
-        None, alias="configMap", description="information about the configMap data to project"
+        default=None, alias="configMap", description="information about the configMap data to project"
     )
     downward_api: Optional[DownwardAPIProjection] = Field(
-        None, alias="downwardAPI", description="information about the downwardAPI data to project"
+        default=None, alias="downwardAPI", description="information about the downwardAPI data to project"
+    )
+    secret: Optional[SecretProjection] = Field(
+        default=None, description="information about the secret data to project"
     )
-    secret: Optional[SecretProjection] = Field(None, description="information about the secret data to project")
     service_account_token: Optional[ServiceAccountTokenProjection] = Field(
-        None, alias="serviceAccountToken", description="information about the serviceAccountToken data to project"
+        default=None,
+        alias="serviceAccountToken",
+        description="information about the serviceAccountToken data to project",
     )
 
 
 class Affinity(BaseModel):
     node_affinity: Optional[NodeAffinity] = Field(
-        None, alias="nodeAffinity", description="Describes node affinity scheduling rules for the pod."
+        default=None, alias="nodeAffinity", description="Describes node affinity scheduling rules for the pod."
     )
     pod_affinity: Optional[PodAffinity] = Field(
-        None,
+        default=None,
         alias="podAffinity",
         description=(
             "Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some"
             " other pod(s))."
         ),
     )
     pod_anti_affinity: Optional[PodAntiAffinity] = Field(
-        None,
+        default=None,
         alias="podAntiAffinity",
         description=(
             "Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as"
             " some other pod(s))."
         ),
     )
 
 
 class Container(BaseModel):
     args: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Arguments to the entrypoint. The docker image's CMD is used if this is not provided. Variable references"
             " $(VAR_NAME) are expanded using the container's environment. If a variable cannot be resolved, the"
             " reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for"
             ' escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string literal "$(VAR_NAME)".'
             " Escaped references will never be expanded, regardless of whether the variable exists or not. Cannot be"
             " updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     command: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Entrypoint array. Not executed within a shell. The docker image's ENTRYPOINT is used if this is not"
             " provided. Variable references $(VAR_NAME) are expanded using the container's environment. If a variable"
             " cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a"
             ' single $, which allows for escaping the $(VAR_NAME) syntax: i.e. "$$(VAR_NAME)" will produce the string'
             ' literal "$(VAR_NAME)". Escaped references will never be expanded, regardless of whether the variable'
             " exists or not. Cannot be updated. More info:"
             " https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#running-a-command-in-a-shell"
         ),
     )
     env: Optional[List[EnvVar]] = Field(
-        None, description="List of environment variables to set in the container. Cannot be updated."
+        default=None, description="List of environment variables to set in the container. Cannot be updated."
     )
     env_from: Optional[List[EnvFromSource]] = Field(
-        None,
+        default=None,
         alias="envFrom",
         description=(
             "List of sources to populate environment variables in the container. The keys defined within a source must"
             " be a C_IDENTIFIER. All invalid keys will be reported as an event when the container is starting. When a"
             " key exists in multiple sources, the value associated with the last source will take precedence. Values"
             " defined by an Env with a duplicate key will take precedence. Cannot be updated."
         ),
@@ -2442,236 +2485,238 @@
         description=(
             "Docker image name. More info: https://kubernetes.io/docs/concepts/containers/images This field is"
             " optional to allow higher level config management to default or override container images in workload"
             " controllers like Deployments and StatefulSets."
         ),
     )
     image_pull_policy: Optional[ImagePullPolicy] = Field(
-        None,
+        default=None,
         alias="imagePullPolicy",
         description=(
             "Image pull policy. One of Always, Never, IfNotPresent. Defaults to Always if :latest tag is specified, or"
             " IfNotPresent otherwise. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/containers/images#updating-images\n\nPossible enum values:\n -"
             ' `"Always"` means that kubelet always attempts to pull the latest image. Container will fail If the pull'
             ' fails.\n - `"IfNotPresent"` means that kubelet pulls if the image isn\'t present on disk. Container will'
             ' fail if the image isn\'t present and the pull fails.\n - `"Never"` means that kubelet never pulls an'
             " image, but only uses a local image. Container will fail if the image isn't present"
         ),
     )
     lifecycle: Optional[Lifecycle] = Field(
-        None,
+        default=None,
         description=(
             "Actions that the management system should take in response to container lifecycle events. Cannot be"
             " updated."
         ),
     )
     liveness_probe: Optional[Probe] = Field(
-        None,
+        default=None,
         alias="livenessProbe",
         description=(
             "Periodic probe of container liveness. Container will be restarted if the probe fails. Cannot be updated."
             " More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name"
             " (DNS_LABEL). Cannot be updated."
         ),
     )
     ports: Optional[List[ContainerPort]] = Field(
-        None,
+        default=None,
         description=(
             "List of ports to expose from the container. Exposing a port here gives the system additional information"
             " about the network connections a container uses, but is primarily informational. Not specifying a port"
             ' here DOES NOT prevent that port from being exposed. Any port which is listening on the default "0.0.0.0"'
             " address inside a container will be accessible from the network. Cannot be updated."
         ),
     )
     readiness_probe: Optional[Probe] = Field(
-        None,
+        default=None,
         alias="readinessProbe",
         description=(
             "Periodic probe of container service readiness. Container will be removed from service endpoints if the"
             " probe fails. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     resources: Optional[ResourceRequirements] = Field(
-        None,
+        default=None,
         description=(
             "Compute Resources required by this container. Cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"
         ),
     )
     security_context: Optional[SecurityContext] = Field(
-        None,
+        default=None,
         alias="securityContext",
         description=(
             "SecurityContext defines the security options the container should be run with. If set, the fields of"
             " SecurityContext override the equivalent fields of PodSecurityContext. More info:"
             " https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"
         ),
     )
     startup_probe: Optional[Probe] = Field(
-        None,
+        default=None,
         alias="startupProbe",
         description=(
             "StartupProbe indicates that the Pod has successfully initialized. If specified, no other probes are"
             " executed until this completes successfully. If this probe fails, the Pod will be restarted, just as if"
             " the livenessProbe failed. This can be used to provide different probe parameters at the beginning of a"
             " Pod's lifecycle, when it might take a long time to load data or warm a cache, than during steady-state"
             " operation. This cannot be updated. More info:"
             " https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes"
         ),
     )
     stdin: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a buffer for stdin in the container runtime. If this is not set,"
             " reads from stdin in the container will always result in EOF. Default is false."
         ),
     )
     stdin_once: Optional[bool] = Field(
-        None,
+        default=None,
         alias="stdinOnce",
         description=(
             "Whether the container runtime should close the stdin channel after it has been opened by a single attach."
             " When stdin is true the stdin stream will remain open across multiple attach sessions. If stdinOnce is"
             " set to true, stdin is opened on container start, is empty until the first client attaches to stdin, and"
             " then remains open and accepts data until the client disconnects, at which time stdin is closed and"
             " remains closed until the container is restarted. If this flag is false, a container processes that reads"
             " from stdin will never receive an EOF. Default is false"
         ),
     )
     termination_message_path: Optional[str] = Field(
-        None,
+        default=None,
         alias="terminationMessagePath",
         description=(
             "Optional: Path at which the file to which the container's termination message will be written is mounted"
             " into the container's filesystem. Message written is intended to be brief final status, such as an"
             " assertion failure message. Will be truncated by the node if greater than 4096 bytes. The total message"
             " length across all containers will be limited to 12kb. Defaults to /dev/termination-log. Cannot be"
             " updated."
         ),
     )
     termination_message_policy: Optional[TerminationMessagePolicy] = Field(
-        None,
+        default=None,
         alias="terminationMessagePolicy",
         description=(
             "Indicate how the termination message should be populated. File will use the contents of"
             " terminationMessagePath to populate the container status message on both success and failure."
             " FallbackToLogsOnError will use the last chunk of container log output if the termination message file is"
             " empty and the container exited with an error. The log output is limited to 2048 bytes or 80 lines,"
             " whichever is smaller. Defaults to File. Cannot be updated.\n\nPossible enum values:\n -"
             ' `"FallbackToLogsOnError"` will read the most recent contents of the container logs for the container'
             " status message when the container exits with an error and the terminationMessagePath has no contents.\n"
             ' - `"File"` is the default behavior and will set the container status message to the contents of the'
             " container's terminationMessagePath when the container exits."
         ),
     )
     tty: Optional[bool] = Field(
-        None,
+        default=None,
         description=(
             "Whether this container should allocate a TTY for itself, also requires 'stdin' to be true. Default is"
             " false."
         ),
     )
     volume_devices: Optional[List[VolumeDevice]] = Field(
-        None,
+        default=None,
         alias="volumeDevices",
         description="volumeDevices is the list of block devices to be used by the container.",
     )
     volume_mounts: Optional[List[VolumeMount]] = Field(
-        None,
+        default=None,
         alias="volumeMounts",
         description="Pod volumes to mount into the container's filesystem. Cannot be updated.",
     )
     working_dir: Optional[str] = Field(
-        None,
+        default=None,
         alias="workingDir",
         description=(
             "Container's working directory. If not specified, the container runtime's default will be used, which"
             " might be configured in the container image. Cannot be updated."
         ),
     )
 
 
 class ProjectedVolumeSource(BaseModel):
     default_mode: Optional[int] = Field(
-        None,
+        default=None,
         alias="defaultMode",
         description=(
             "Mode bits used to set permissions on created files by default. Must be an octal value between 0000 and"
             " 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires"
             " decimal values for mode bits. Directories within the path are not affected by this setting. This might"
             " be in conflict with other options that affect the file mode, like fsGroup, and the result can be other"
             " mode bits set."
         ),
     )
-    sources: Optional[List[VolumeProjection]] = Field(None, description="list of volume projections")
+    sources: Optional[List[VolumeProjection]] = Field(default=None, description="list of volume projections")
 
 
 class Volume(BaseModel):
     aws_elastic_block_store: Optional[AWSElasticBlockStoreVolumeSource] = Field(
-        None,
+        default=None,
         alias="awsElasticBlockStore",
         description=(
             "AWSElasticBlockStore represents an AWS Disk resource that is attached to a kubelet's host machine and"
             " then exposed to the pod. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#awselasticblockstore"
         ),
     )
     azure_disk: Optional[AzureDiskVolumeSource] = Field(
-        None,
+        default=None,
         alias="azureDisk",
         description="AzureDisk represents an Azure Data Disk mount on the host and bind mount to the pod.",
     )
     azure_file: Optional[AzureFileVolumeSource] = Field(
-        None,
+        default=None,
         alias="azureFile",
         description="AzureFile represents an Azure File Service mount on the host and bind mount to the pod.",
     )
     cephfs: Optional[CephFSVolumeSource] = Field(
-        None, description="CephFS represents a Ceph FS mount on the host that shares a pod's lifetime"
+        default=None, description="CephFS represents a Ceph FS mount on the host that shares a pod's lifetime"
     )
     cinder: Optional[CinderVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "Cinder represents a cinder volume attached and mounted on kubelets host machine. More info:"
             " https://examples.k8s.io/mysql-cinder-pd/README.md"
         ),
     )
     config_map: Optional[ConfigMapVolumeSource] = Field(
-        None, alias="configMap", description="ConfigMap represents a configMap that should populate this volume"
+        default=None,
+        alias="configMap",
+        description="ConfigMap represents a configMap that should populate this volume",
     )
     csi: Optional[CSIVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "CSI (Container Storage Interface) represents ephemeral storage that is handled by certain external CSI"
             " drivers (Beta feature)."
         ),
     )
     downward_api: Optional[DownwardAPIVolumeSource] = Field(
-        None,
+        default=None,
         alias="downwardAPI",
         description="DownwardAPI represents downward API about the pod that should populate this volume",
     )
     empty_dir: Optional[EmptyDirVolumeSource] = Field(
-        None,
+        default=None,
         alias="emptyDir",
         description=(
             "EmptyDir represents a temporary directory that shares a pod's lifetime. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#emptydir"
         ),
     )
     ephemeral: Optional[EphemeralVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "Ephemeral represents a volume that is handled by a cluster storage driver. The volume's lifecycle is tied"
             " to the pod that defines it - it will be created before the pod starts, and deleted when the pod is"
             " removed.\n\nUse this if: a) the volume is only needed while the pod runs, b) features of normal volumes"
             " like restoring from snapshot or capacity\n   tracking are needed,\nc) the storage driver is specified"
             " through a storage class, and d) the storage driver supports dynamic volume provisioning through\n   a"
             " PersistentVolumeClaim (see EphemeralVolumeSource for more\n   information on the connection between this"
@@ -2679,137 +2724,137 @@
             " APIs for volumes that persist for longer than the lifecycle of an individual pod.\n\nUse CSI for"
             " light-weight local ephemeral volumes if the CSI driver is meant to be used that way - see the"
             " documentation of the driver for more information.\n\nA pod can use both types of ephemeral volumes and"
             " persistent volumes at the same time."
         ),
     )
     fc: Optional[FCVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "FC represents a Fibre Channel resource that is attached to a kubelet's host machine and then exposed to"
             " the pod."
         ),
     )
     flex_volume: Optional[FlexVolumeSource] = Field(
-        None,
+        default=None,
         alias="flexVolume",
         description=(
             "FlexVolume represents a generic volume resource that is provisioned/attached using an exec based plugin."
         ),
     )
     flocker: Optional[FlockerVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "Flocker represents a Flocker volume attached to a kubelet's host machine. This depends on the Flocker"
             " control service being running"
         ),
     )
     gce_persistent_disk: Optional[GCEPersistentDiskVolumeSource] = Field(
-        None,
+        default=None,
         alias="gcePersistentDisk",
         description=(
             "GCEPersistentDisk represents a GCE Disk resource that is attached to a kubelet's host machine and then"
             " exposed to the pod. More info: https://kubernetes.io/docs/concepts/storage/volumes#gcepersistentdisk"
         ),
     )
     git_repo: Optional[GitRepoVolumeSource] = Field(
-        None,
+        default=None,
         alias="gitRepo",
         description=(
             "GitRepo represents a git repository at a particular revision. DEPRECATED: GitRepo is deprecated. To"
             " provision a container with a git repo, mount an EmptyDir into an InitContainer that clones the repo"
             " using git, then mount the EmptyDir into the Pod's container."
         ),
     )
     glusterfs: Optional[GlusterfsVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "Glusterfs represents a Glusterfs mount on the host that shares a pod's lifetime. More info:"
             " https://examples.k8s.io/volumes/glusterfs/README.md"
         ),
     )
     host_path: Optional[HostPathVolumeSource] = Field(
-        None,
+        default=None,
         alias="hostPath",
         description=(
             "HostPath represents a pre-existing file or directory on the host machine that is directly exposed to the"
             " container. This is generally used for system agents or other privileged things that are allowed to see"
             " the host machine. Most containers will NOT need this. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#hostpath"
         ),
     )
     iscsi: Optional[ISCSIVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "ISCSI represents an ISCSI Disk resource that is attached to a kubelet's host machine and then exposed to"
             " the pod. More info: https://examples.k8s.io/volumes/iscsi/README.md"
         ),
     )
     name: str = Field(
         ...,
         description=(
             "Volume's name. Must be a DNS_LABEL and unique within the pod. More info:"
             " https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names"
         ),
     )
     nfs: Optional[NFSVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "NFS represents an NFS mount on the host that shares a pod's lifetime More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#nfs"
         ),
     )
     persistent_volume_claim: Optional[PersistentVolumeClaimVolumeSource] = Field(
-        None,
+        default=None,
         alias="persistentVolumeClaim",
         description=(
             "PersistentVolumeClaimVolumeSource represents a reference to a PersistentVolumeClaim in the same"
             " namespace. More info:"
             " https://kubernetes.io/docs/concepts/storage/persistent-volumes#persistentvolumeclaims"
         ),
     )
     photon_persistent_disk: Optional[PhotonPersistentDiskVolumeSource] = Field(
-        None,
+        default=None,
         alias="photonPersistentDisk",
         description=(
             "PhotonPersistentDisk represents a PhotonController persistent disk attached and mounted on kubelets host"
             " machine"
         ),
     )
     portworx_volume: Optional[PortworxVolumeSource] = Field(
-        None,
+        default=None,
         alias="portworxVolume",
         description="PortworxVolume represents a portworx volume attached and mounted on kubelets host machine",
     )
     projected: Optional[ProjectedVolumeSource] = Field(
-        None, description="Items for all in one resources secrets, configmaps, and downward API"
+        default=None, description="Items for all in one resources secrets, configmaps, and downward API"
     )
     quobyte: Optional[QuobyteVolumeSource] = Field(
-        None, description="Quobyte represents a Quobyte mount on the host that shares a pod's lifetime"
+        default=None, description="Quobyte represents a Quobyte mount on the host that shares a pod's lifetime"
     )
     rbd: Optional[RBDVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "RBD represents a Rados Block Device mount on the host that shares a pod's lifetime. More info:"
             " https://examples.k8s.io/volumes/rbd/README.md"
         ),
     )
     scale_io: Optional[ScaleIOVolumeSource] = Field(
-        None,
+        default=None,
         alias="scaleIO",
         description="ScaleIO represents a ScaleIO persistent volume attached and mounted on Kubernetes nodes.",
     )
     secret: Optional[SecretVolumeSource] = Field(
-        None,
+        default=None,
         description=(
             "Secret represents a secret that should populate this volume. More info:"
             " https://kubernetes.io/docs/concepts/storage/volumes#secret"
         ),
     )
     storageos: Optional[StorageOSVolumeSource] = Field(
-        None, description="StorageOS represents a StorageOS volume attached and mounted on Kubernetes nodes."
+        default=None, description="StorageOS represents a StorageOS volume attached and mounted on Kubernetes nodes."
     )
     vsphere_volume: Optional[VsphereVirtualDiskVolumeSource] = Field(
-        None,
+        default=None,
         alias="vsphereVolume",
         description="VsphereVolume represents a vSphere volume attached and mounted on kubelets host machine",
     )
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/api/core/v1.pyi` & `hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/api/core/v1.pyi`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,13 @@
-from enum import Enum
-from typing import Dict, List, Optional
-
-from hera.shared._base_model import BaseModel as BaseModel
-
 from ...apimachinery.pkg.api import resource as resource
 from ...apimachinery.pkg.apis.meta import v1 as v1
 from ...apimachinery.pkg.util import intstr as intstr
+from enum import Enum
+from hera.shared._base_model import BaseModel as BaseModel
+from typing import Dict, List, Optional
 
 class AWSElasticBlockStoreVolumeSource(BaseModel):
     fs_type: Optional[str]
     partition: Optional[int]
     read_only: Optional[bool]
     volume_id: str
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/api/policy/v1beta1.py` & `hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/api/policy/v1beta1.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,42 +1,41 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from ...apimachinery.pkg.apis.meta import v1
 from ...apimachinery.pkg.util import intstr
 
 
 class PodDisruptionBudgetSpec(BaseModel):
     max_unavailable: Optional[intstr.IntOrString] = Field(
-        None,
+        default=None,
         alias="maxUnavailable",
         description=(
             'An eviction is allowed if at most "maxUnavailable" pods selected by "selector" are unavailable after the'
             " eviction, i.e. even in absence of the evicted pod. For example, one can prevent all voluntary evictions"
             ' by specifying 0. This is a mutually exclusive setting with "minAvailable".'
         ),
     )
     min_available: Optional[intstr.IntOrString] = Field(
-        None,
+        default=None,
         alias="minAvailable",
         description=(
             'An eviction is allowed if at least "minAvailable" pods selected by "selector" will still be available'
             " after the eviction, i.e. even in the absence of the evicted pod.  So for example you can prevent all"
             ' voluntary evictions by specifying "100%".'
         ),
     )
     selector: Optional[v1.LabelSelector] = Field(
-        None,
+        default=None,
         description=(
             "Label query over pods whose evictions are managed by the disruption budget. A null selector selects no"
             " pods. An empty selector ({}) also selects no pods, which differs from standard behavior of selecting all"
             " pods. In policy/v1, an empty selector will select all pods in the namespace."
         ),
     )
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/api/resource.py` & `hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/api/resource.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 
 class Quantity(BaseModel):
     __root__: str = Field(
         ...,
         description=(
             "Quantity is a fixed-point representation of a number. It provides convenient marshaling/unmarshaling in"
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/meta/v1.py` & `hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/meta/v1.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,40 +2,39 @@
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from datetime import datetime
 from typing import Dict, List, Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 
 class CreateOptions(BaseModel):
     dry_run: Optional[List[str]] = Field(
-        None,
+        default=None,
         alias="dryRun",
         title=(
             "When present, indicates that modifications should not be\npersisted. An invalid or unrecognized dryRun"
             " directive will\nresult in an error response and no further processing of the\nrequest. Valid values"
             " are:\n- All: all dry run stages will be processed\n+optional"
         ),
     )
     field_manager: Optional[str] = Field(
-        None,
+        default=None,
         alias="fieldManager",
         title=(
             "fieldManager is a name associated with the actor or entity\nthat is making these changes. The value must"
             " be less than or\n128 characters long, and only contain printable characters,\nas defined by"
             " https://golang.org/pkg/unicode/#IsPrint.\n+optional"
         ),
     )
     field_validation: Optional[str] = Field(
-        None,
+        default=None,
         alias="fieldValidation",
         title=(
             "fieldValidation instructs the server on how to handle\nobjects in the request (POST/PUT/PATCH) containing"
             " unknown\nor duplicate fields, provided that the `ServerSideFieldValidation`\nfeature gate is also"
             " enabled. Valid values are:\n- Ignore: This will ignore any unknown fields that are silently\ndropped"
             " from the object, and will ignore all but the last duplicate\nfield that the decoder encounters. This is"
             " the default behavior\nprior to v1.23 and is the default behavior when the\n`ServerSideFieldValidation`"
@@ -66,61 +65,61 @@
         ...,
         description=(
             "operator represents a key's relationship to a set of values. Valid operators are In, NotIn, Exists and"
             " DoesNotExist."
         ),
     )
     values: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "values is an array of string values. If the operator is In or NotIn, the values array must be non-empty."
             " If the operator is Exists or DoesNotExist, the values array must be empty. This array is replaced during"
             " a strategic merge patch."
         ),
     )
 
 
 class ListMeta(BaseModel):
     continue_: Optional[str] = Field(
-        None,
+        default=None,
         alias="continue",
         description=(
             "continue may be set if the user set a limit on the number of items returned, and indicates that the"
             " server has more data available. The value is opaque and may be used to issue another request to the"
             " endpoint that served this list to retrieve the next set of available objects. Continuing a consistent"
             " list may not be possible if the server configuration has changed or more than a few minutes have passed."
             " The resourceVersion field returned when using this continue value will be identical to the value in the"
             " first response, unless you have received this token from an error message."
         ),
     )
     remaining_item_count: Optional[int] = Field(
-        None,
+        default=None,
         alias="remainingItemCount",
         description=(
             "remainingItemCount is the number of subsequent items in the list which are not included in this list"
             " response. If the list request contained label or field selectors, then the number of remaining items is"
             " unknown and the field will be left unset and omitted during serialization. If the list is complete"
             " (either because it is not chunking or because this is the last chunk), then there are no more remaining"
             " items and this field will be left unset and omitted during serialization. Servers older than v1.15 do"
             " not set this field. The intended use of the remainingItemCount is *estimating* the size of a collection."
             " Clients should not rely on the remainingItemCount to be set or to be exact."
         ),
     )
     resource_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="resourceVersion",
         description=(
             "String that identifies the server's internal version of this object that can be used by clients to"
             " determine when objects have changed. Value must be treated as opaque by clients and passed unmodified"
             " back to the server. Populated by the system. Read-only. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"
         ),
     )
     self_link: Optional[str] = Field(
-        None,
+        default=None,
         alias="selfLink",
         description=(
             "selfLink is a URL representing this object. Populated by the system. Read-only.\n\nDEPRECATED Kubernetes"
             " will stop propagating this field in 1.20 release and the field is planned to be removed in 1.21 release."
         ),
     )
 
@@ -128,23 +127,25 @@
 class MicroTime(BaseModel):
     __root__: datetime = Field(..., description="MicroTime is version of Time with microsecond level precision.")
 
 
 class OwnerReference(BaseModel):
     api_version: str = Field(..., alias="apiVersion", description="API version of the referent.")
     block_owner_deletion: Optional[bool] = Field(
-        None,
+        default=None,
         alias="blockOwnerDeletion",
         description=(
             'If true, AND if the owner has the "foregroundDeletion" finalizer, then the owner cannot be deleted from'
             " the key-value store until this reference is removed. Defaults to false. To set this field, a user needs"
             ' "delete" permission of the owner, otherwise 422 (Unprocessable Entity) will be returned.'
         ),
     )
-    controller: Optional[bool] = Field(None, description="If true, this reference points to the managing controller.")
+    controller: Optional[bool] = Field(
+        default=None, description="If true, this reference points to the managing controller."
+    )
     kind: str = Field(
         ...,
         description=(
             "Kind of the referent. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds"
         ),
     )
@@ -154,30 +155,30 @@
     uid: str = Field(
         ..., description="UID of the referent. More info: http://kubernetes.io/docs/user-guide/identifiers#uids"
     )
 
 
 class StatusCause(BaseModel):
     field: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "The field of the resource that has caused this error, as named by its JSON serialization. May include dot"
             " and postfix notation for nested attributes. Arrays are zero-indexed.  Fields may appear more than once"
             ' in an array of causes due to fields having multiple errors. Optional.\n\nExamples:\n  "name" - the field'
             ' "name" on the current resource\n  "items[0].name" - the field "name" on the first array entry in "items"'
         ),
     )
     message: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "A human-readable description of the cause of the error.  This field may be presented as-is to a reader."
         ),
     )
     reason: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "A machine-readable description of the cause of the error. If this value is empty there is no information"
             " available."
         ),
     )
 
 
@@ -189,116 +190,118 @@
             " provided for many of the factory methods that the time package offers."
         ),
     )
 
 
 class LabelSelector(BaseModel):
     match_expressions: Optional[List[LabelSelectorRequirement]] = Field(
-        None,
+        default=None,
         alias="matchExpressions",
         description="matchExpressions is a list of label selector requirements. The requirements are ANDed.",
     )
     match_labels: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         alias="matchLabels",
         description=(
             "matchLabels is a map of {key,value} pairs. A single {key,value} in the matchLabels map is equivalent to"
             ' an element of matchExpressions, whose key field is "key", the operator is "In", and the values array'
             ' contains only "value". The requirements are ANDed.'
         ),
     )
 
 
 class ManagedFieldsEntry(BaseModel):
     api_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="apiVersion",
         description=(
             "APIVersion defines the version of this resource that this field set applies to. The format is"
             ' "group/version" just like the top-level APIVersion field. It is necessary to track the version of a'
             " field set because it cannot be automatically converted."
         ),
     )
     fields_type: Optional[str] = Field(
-        None,
+        default=None,
         alias="fieldsType",
         description=(
             "FieldsType is the discriminator for the different fields format and version. There is currently only one"
             ' possible value: "FieldsV1"'
         ),
     )
     fields_v1: Optional[FieldsV1] = Field(
-        None,
+        default=None,
         alias="fieldsV1",
         description='FieldsV1 holds the first JSON version format as described in the "FieldsV1" type.',
     )
-    manager: Optional[str] = Field(None, description="Manager is an identifier of the workflow managing these fields.")
+    manager: Optional[str] = Field(
+        default=None, description="Manager is an identifier of the workflow managing these fields."
+    )
     operation: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Operation is the type of operation which lead to this ManagedFieldsEntry being created. The only valid"
             " values for this field are 'Apply' and 'Update'."
         ),
     )
     subresource: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Subresource is the name of the subresource used to update that object, or empty string if the object was"
             " updated through the main resource. The value of this field is used to distinguish between managers, even"
             " if they share the same name. For example, a status update will be distinct from a regular update using"
             " the same manager name. Note that the APIVersion field is not related to the Subresource field and it"
             " always corresponds to the version of the main resource."
         ),
     )
     time: Optional[Time] = Field(
-        None,
+        default=None,
         description=(
             "Time is timestamp of when these fields were set. It should always be empty if Operation is 'Apply'"
         ),
     )
 
 
 class ObjectMeta(BaseModel):
     annotations: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         description=(
             "Annotations is an unstructured key value map stored with a resource that may be set by external tools to"
             " store and retrieve arbitrary metadata. They are not queryable and should be preserved when modifying"
             " objects. More info: http://kubernetes.io/docs/user-guide/annotations"
         ),
     )
     cluster_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="clusterName",
         description=(
             "The name of the cluster which the object belongs to. This is used to distinguish resources with same name"
             " and namespace in different clusters. This field is not set anywhere right now and apiserver is going to"
             " ignore it if set in create or update request."
         ),
     )
     creation_timestamp: Optional[Time] = Field(
-        None,
+        default=None,
         alias="creationTimestamp",
         description=(
             "CreationTimestamp is a timestamp representing the server time when this object was created. It is not"
             " guaranteed to be set in happens-before order across separate operations. Clients may not set this value."
             " It is represented in RFC3339 form and is in UTC.\n\nPopulated by the system. Read-only. Null for lists."
             " More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata"
         ),
     )
     deletion_grace_period_seconds: Optional[int] = Field(
-        None,
+        default=None,
         alias="deletionGracePeriodSeconds",
         description=(
             "Number of seconds allowed for this object to gracefully terminate before it will be removed from the"
             " system. Only set when deletionTimestamp is also set. May only be shortened. Read-only."
         ),
     )
     deletion_timestamp: Optional[Time] = Field(
-        None,
+        default=None,
         alias="deletionTimestamp",
         description=(
             "DeletionTimestamp is RFC 3339 date and time at which this resource will be deleted. This field is set by"
             " the server when a graceful deletion is requested by the user, and is not directly settable by a client."
             " The resource is expected to be deleted (no longer visible from resource lists, and not reachable by"
             " name) after the time in this field, once the finalizers list is empty. As long as the finalizers list"
             " contains items, deletion is blocked. Once the deletionTimestamp is set, this value may not be unset or"
@@ -310,29 +313,29 @@
             " until an administrator or automated process can determine the resource is fully terminated. If not set,"
             " graceful deletion of the object has not been requested.\n\nPopulated by the system when a graceful"
             " deletion is requested. Read-only. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata"
         ),
     )
     finalizers: Optional[List[str]] = Field(
-        None,
+        default=None,
         description=(
             "Must be empty before the object is deleted from the registry. Each entry is an identifier for the"
             " responsible component that will remove the entry from the list. If the deletionTimestamp of the object"
             " is non-nil, entries in this list can only be removed. Finalizers may be processed and removed in any"
             " order.  Order is NOT enforced because it introduces significant risk of stuck finalizers. finalizers is"
             " a shared field, any actor with permission can reorder it. If the finalizer list is processed in order,"
             " then this can lead to a situation in which the component responsible for the first finalizer in the list"
             " is waiting for a signal (field value, external system, or other) produced by a component responsible for"
             " a finalizer later in the list, resulting in a deadlock. Without enforced ordering finalizers are free to"
             " order amongst themselves and are not vulnerable to ordering changes in the list."
         ),
     )
     generate_name: Optional[str] = Field(
-        None,
+        default=None,
         alias="generateName",
         description=(
             "GenerateName is an optional prefix, used by the server, to generate a unique name ONLY IF the Name field"
             " has not been provided. If this field is used, the name returned to the client will be different than the"
             " name passed. This value will also be combined with a unique suffix. The provided value has the same"
             " validation rules as the Name field, and may be truncated by the length of the suffix required to make"
             " the value unique on the server.\n\nIf this field is specified and the generated name exists, the server"
@@ -340,89 +343,89 @@
             " indicating a unique name could not be found in the time allotted, and the client should retry"
             " (optionally after the time indicated in the Retry-After header).\n\nApplied only if Name is not"
             " specified. More info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency"
         ),
     )
     generation: Optional[int] = Field(
-        None,
+        default=None,
         description=(
             "A sequence number representing a specific generation of the desired state. Populated by the system."
             " Read-only."
         ),
     )
     labels: Optional[Dict[str, str]] = Field(
-        None,
+        default=None,
         description=(
             "Map of string keys and values that can be used to organize and categorize (scope and select) objects. May"
             " match selectors of replication controllers and services. More info:"
             " http://kubernetes.io/docs/user-guide/labels"
         ),
     )
     managed_fields: Optional[List[ManagedFieldsEntry]] = Field(
-        None,
+        default=None,
         alias="managedFields",
         description=(
             "ManagedFields maps workflow-id and version to the set of fields that are managed by that workflow. This"
             " is mostly for internal housekeeping, and users typically shouldn't need to set or understand this"
             " field. A workflow can be the user's name, a controller's name, or the name of a specific apply path"
             ' like "ci-cd". The set of fields is always in the version that the workflow used when modifying the'
             " object."
         ),
     )
     name: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Name must be unique within a namespace. Is required when creating resources, although some resources may"
             " allow a client to request the generation of an appropriate name automatically. Name is primarily"
             " intended for creation idempotence and configuration definition. Cannot be updated. More info:"
             " http://kubernetes.io/docs/user-guide/identifiers#names"
         ),
     )
     namespace: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "Namespace defines the space within which each name must be unique. An empty namespace is equivalent to"
             ' the "default" namespace, but "default" is the canonical representation. Not all objects are required to'
             " be scoped to a namespace - the value of this field for those objects will be empty.\n\nMust be a"
             " DNS_LABEL. Cannot be updated. More info: http://kubernetes.io/docs/user-guide/namespaces"
         ),
     )
     owner_references: Optional[List[OwnerReference]] = Field(
-        None,
+        default=None,
         alias="ownerReferences",
         description=(
             "List of objects depended by this object. If ALL objects in the list have been deleted, this object will"
             " be garbage collected. If this object is managed by a controller, then an entry in this list will point"
             " to this controller, with the controller field set to true. There cannot be more than one managing"
             " controller."
         ),
     )
     resource_version: Optional[str] = Field(
-        None,
+        default=None,
         alias="resourceVersion",
         description=(
             "An opaque value that represents the internal version of this object that can be used by clients to"
             " determine when objects have changed. May be used for optimistic concurrency, change detection, and the"
             " watch operation on a resource or set of resources. Clients must treat these values as opaque and passed"
             " unmodified back to the server. They may only be valid for a particular resource or set of"
             " resources.\n\nPopulated by the system. Read-only. Value must be treated as opaque by clients and . More"
             " info:"
             " https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency"
         ),
     )
     self_link: Optional[str] = Field(
-        None,
+        default=None,
         alias="selfLink",
         description=(
             "SelfLink is a URL representing this object. Populated by the system. Read-only.\n\nDEPRECATED Kubernetes"
             " will stop propagating this field in 1.20 release and the field is planned to be removed in 1.21 release."
         ),
     )
     uid: Optional[str] = Field(
-        None,
+        default=None,
         description=(
             "UID is the unique in time and space value for this object. It is typically generated by the server on"
             " successful creation of a resource and is not allowed to change on PUT operations.\n\nPopulated by the"
             " system. Read-only. More info: http://kubernetes.io/docs/user-guide/identifiers#uids"
         ),
     )
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/meta/v1.pyi` & `hera_workflows-5.6.0/src/hera/workflows/models/io/k8s/apimachinery/pkg/apis/meta/v1.pyi`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,10 @@
 from datetime import datetime
-from typing import Dict, List, Optional
-
 from hera.shared._base_model import BaseModel as BaseModel
+from typing import Dict, List, Optional
 
 class CreateOptions(BaseModel):
     dry_run: Optional[List[str]]
     field_manager: Optional[str]
     field_validation: Optional[str]
 
 class FieldsV1(BaseModel): ...
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/sensor.py` & `hera_workflows-5.6.0/src/hera/workflows/models/sensor.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,39 +1,40 @@
 # generated by datamodel-codegen:
 #   filename:  https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.4/api/openapi-spec/swagger.json
 
 from __future__ import annotations
 
 from typing import Optional
 
-from pydantic import Field
-
 from hera.shared._base_model import BaseModel
+from pydantic import Field
 
 from .io.argoproj.events import v1alpha1
 from .io.k8s.apimachinery.pkg.apis.meta import v1
 
 
 class DeleteSensorResponse(BaseModel):
     pass
 
 
 class LogEntry(BaseModel):
-    dependency_name: Optional[str] = Field(None, alias="dependencyName", title="optional - trigger dependency name")
-    event_context: Optional[str] = Field(None, alias="eventContext", title="optional - Cloud Event context")
+    dependency_name: Optional[str] = Field(
+        default=None, alias="dependencyName", title="optional - trigger dependency name"
+    )
+    event_context: Optional[str] = Field(default=None, alias="eventContext", title="optional - Cloud Event context")
     level: Optional[str] = None
     msg: Optional[str] = None
     namespace: Optional[str] = None
-    sensor_name: Optional[str] = Field(None, alias="sensorName")
+    sensor_name: Optional[str] = Field(default=None, alias="sensorName")
     time: Optional[v1.Time] = None
-    trigger_name: Optional[str] = Field(None, alias="triggerName", title="optional - any trigger name")
+    trigger_name: Optional[str] = Field(default=None, alias="triggerName", title="optional - any trigger name")
 
 
 class CreateSensorRequest(BaseModel):
-    create_options: Optional[v1.CreateOptions] = Field(None, alias="createOptions")
+    create_options: Optional[v1.CreateOptions] = Field(default=None, alias="createOptions")
     namespace: Optional[str] = None
     sensor: Optional[v1alpha1.Sensor] = None
 
 
 class SensorWatchEvent(BaseModel):
     object: Optional[v1alpha1.Sensor] = None
     type: Optional[str] = None
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/models/sensor.pyi` & `hera_workflows-5.6.0/src/hera/workflows/models/sensor.pyi`

 * *Files 9% similar despite different names*

```diff
@@ -1,13 +1,11 @@
-from typing import Optional
-
-from hera.shared._base_model import BaseModel as BaseModel
-
 from .io.argoproj.events import v1alpha1 as v1alpha1
 from .io.k8s.apimachinery.pkg.apis.meta import v1 as v1
+from hera.shared._base_model import BaseModel as BaseModel
+from typing import Optional
 
 class DeleteSensorResponse(BaseModel): ...
 
 class LogEntry(BaseModel):
     dependency_name: Optional[str]
     event_context: Optional[str]
     level: Optional[str]
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/parameter.py` & `hera_workflows-5.6.0/src/hera/workflows/parameter.py`

 * *Files 0% similar despite different names*

```diff
@@ -45,47 +45,47 @@
         Task.args=["echo", wf.get_parameter("message")].
         """
         if self.value is None:
             raise ValueError("Cannot represent `Parameter` as string as `value` is not set")
         return self.value
 
     def with_name(self, name: str) -> Parameter:
-        """Returns a copy of the parameter with the name set to the value"""
+        """Returns a copy of the parameter with the name set to the value."""
         p = self.copy(deep=True)
         p.name = name
         return p
 
     def as_input(self) -> _ModelParameter:
-        """Assembles the parameter for use as an input of a template"""
+        """Assembles the parameter for use as an input of a template."""
         return _ModelParameter(
             name=self.name,
             description=self.description,
             default=self.default,
             enum=self.enum,
             value=self.value,
             value_from=self.value_from,
         )
 
     def as_argument(self) -> _ModelParameter:
-        """Assembles the parameter for use as an argument of a step or a task"""
+        """Assembles the parameter for use as an argument of a step or a task."""
         # Setting a default value when used as an argument is a no-op so we exclude it as it would get overwritten by
         # `value` or `value_from` (one of which is required)
         # Overwrite ref: https://github.com/argoproj/argo-workflows/blob/781675ddcf6f1138d697cb9c71dae484daa0548b/workflow/common/util.go#L126-L139
         # One of value/value_from required ref: https://github.com/argoproj/argo-workflows/blob/ab178bb0b36a5ce34b4c1302cf4855879a0e8cf5/workflow/validate/validate.go#L794-L798
         return _ModelParameter(
             name=self.name,
             global_name=self.global_name,
             description=self.description,
             enum=self.enum,
             value=self.value,
             value_from=self.value_from,
         )
 
     def as_output(self) -> _ModelParameter:
-        """Assembles the parameter for use as an output of a template"""
+        """Assembles the parameter for use as an output of a template."""
         # Only `value` and `value_from` are valid here
         # see https://github.com/argoproj/argo-workflows/blob/e3254eca115c9dd358e55d16c6a3d41403c29cae/workflow/validate/validate.go#L1067
         return _ModelParameter(
             name=self.name,
             global_name=self.global_name,
             description=self.description,
             value=self.value,
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/resource.py` & `hera_workflows-5.6.0/src/hera/workflows/resource.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,21 +1,31 @@
+"""The resource module provides functionality for creating K8s resources via workflows inside task/steps."""
 from typing import List, Optional, Union
 
 from hera.workflows._mixins import CallableTemplateMixin, IOMixin, SubNodeMixin, TemplateMixin
 from hera.workflows.cron_workflow import CronWorkflow
 from hera.workflows.models import (
     ManifestFrom,
     ResourceTemplate as _ModelResourceTemplate,
     Template as _ModelTemplate,
 )
 from hera.workflows.workflow import Workflow
 from hera.workflows.workflow_template import WorkflowTemplate
 
 
 class Resource(CallableTemplateMixin, TemplateMixin, SubNodeMixin, IOMixin):
+    """`Resource` is a representation of a K8s resource that can be created by Argo.
+
+    The resource is a callable step that can be invoked in a DAG/Workflow. The resource can create any K8s resource,
+    such as other workflows, workflow templates, daemons, etc, as specified by the `manifest` field of the resource.
+    The manifest field is a canonical YAML that is submitted to K8s by Argo. Note that the manifest is a union of
+    multiple types. The manifest can be a string, in which case it is assume to be YAML. Otherwise, if it's a Hera
+    objects, it is automatically converted to the corresponding YAML representation.
+    """
+
     action: str
     failure_condition: Optional[str] = None
     flags: Optional[List[str]] = None
     manifest: Optional[Union[str, Workflow, CronWorkflow, WorkflowTemplate]] = None
     manifest_from: Optional[ManifestFrom] = None
     merge_strategy: Optional[str] = None
     set_owner_reference: Optional[bool] = None
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/resources.py` & `hera_workflows-5.6.0/src/hera/workflows/resources.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-"""Holds the resource specification"""
+"""Holds the resource specification."""
 from typing import Dict, Optional, Union
 
 from pydantic import root_validator
 
 from hera.shared._base_model import BaseModel as _BaseModel
 from hera.workflows.models import ResourceRequirements as _ModelResourceRequirements
 from hera.workflows.validators import validate_storage_units
@@ -39,30 +39,38 @@
     memory_request: Optional[str] = None
         The amount of memory to request.
     memory_limit: Optional[str] = None
         The memory limit of the pod.
     ephemeral_request: Optional[str] = None
         The amount of ephemeral storage to request.
     ephemeral_limit: Optional[str] = None
-        The emphemeral storage limit of the pod.
-    gpus: Optional[int] = None
+        The ephemeral storage limit of the pod.
+    gpus: Optional[Union[int, str]] = None
         The number of GPUs to request.
     gpu_flag: Optional[str] = "nvidia.com/gpu"
         The GPU flag to use for identifying how many GPUs to mount to a pod. This is dependent on the cloud provider.
     custom_resources: Optional[Dict] = None
         Any custom resources to request. This is dependent on the cloud provider.
+
+    Notes:
+    -----
+    Most of the fields that support a union of `int` and `str` support either specifying a number for the resource,
+    such as 1 CPU, 2 GPU, etc., a `str` representation of that numerical resource, such as '1' CPU, '2' GPU, etc., but
+    also supports specifying a *to be computed* value, such as `{{inputs.parameters.cpu_request}}`. This means tasks,
+    steps, etc., can be stitched together in a way to have a task/step that *computes* the resource requirements, and
+    then `outputs` them to the next step/task.
     """
 
     cpu_request: Optional[Union[float, int, str]] = None
     cpu_limit: Optional[Union[float, int, str]] = None
     memory_request: Optional[str] = None
     memory_limit: Optional[str] = None
     ephemeral_request: Optional[str] = None
     ephemeral_limit: Optional[str] = None
-    gpus: Optional[int] = None
+    gpus: Optional[Union[int, str]] = None
     gpu_flag: Optional[str] = "nvidia.com/gpu"
     custom_resources: Optional[Dict] = None
 
     @root_validator(pre=True)
     def _check_specs(cls, values):
         cpu_request: Optional[Union[float, int, str]] = values.get("cpu_request")
         cpu_limit: Optional[Union[float, int, str]] = values.get("cpu_limit")
@@ -88,15 +96,15 @@
             assert cpu_request >= 0, "CPU request must be positive"
             if cpu_limit is not None and isinstance(cpu_limit, int):
                 assert cpu_request <= cpu_limit, "CPU request must be smaller or equal to limit"
 
         return values
 
     def build(self) -> _ModelResourceRequirements:
-        """Builds the resource requirements of the pod"""
+        """Builds the resource requirements of the pod."""
         resources: Dict = dict()
 
         if self.cpu_limit is not None:
             resources = _merge_dicts(resources, dict(limits=dict(cpu=str(self.cpu_limit))))
 
         if self.cpu_request is not None:
             resources = _merge_dicts(resources, dict(requests=dict(cpu=str(self.cpu_request))))
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/retry_strategy.py` & `hera_workflows-5.6.0/src/hera/workflows/retry_strategy.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+"""A module that provides retry strategy functionality, along with necessary dependencies such as retry policy."""
 from enum import Enum
 from typing import Optional, Union, cast
 
 from pydantic import validator
 
 from hera.shared._base_model import BaseModel as _BaseModel
 from hera.workflows.models import (
@@ -9,14 +10,16 @@
     IntOrString,
     RetryAffinity,
     RetryStrategy as _ModelRetryStrategy,
 )
 
 
 class RetryPolicy(Enum):
+    """An enum that holds options for retry policy."""
+
     always = "Always"
     """Retry all failed steps"""
 
     on_failure = "OnFailure"
     """Retry steps whose main container is marked as failed in Kubernetes"""
 
     on_error = "OnError"
@@ -24,41 +27,56 @@
 
     on_transient_error = "OnTransientError"
     """Retry steps that encounter errors defined as transient, or errors matching the `TRANSIENT_ERROR_PATTERN`
     environment variable.
     Available in version 3.0 and later.
     """
 
-    def __str__(self):
+    def __str__(self) -> str:
+        """Assembles the `value` representation of the enum as a string."""
         return str(self.value)
 
 
 class RetryStrategy(_BaseModel):
+    """`RetryStrategy` configures how an Argo job should retry."""
+
     affinity: Optional[RetryAffinity] = None
+    """affinity dictates the affinity of the retried jobs"""
+
     backoff: Optional[Backoff] = None
+    """backoff dictates how long should a job wait for before retrying"""
+
     expression: Optional[str] = None
+    """the expression field supports the expression of complex rules regarding retry behavior"""
+
     limit: Optional[Union[str, int, IntOrString]] = None
+    """the hard numeric limit of how many times a jobs should retry"""
+
     retry_policy: Optional[Union[str, RetryPolicy]] = None
+    """the policy dictates, at a high level, under what conditions should a job retry"""
 
     @validator("retry_policy", pre=True)
     def _convert_retry_policy(cls, v):
+        """Converts the `retry_policy` field into a pure `str` from either `str` already or an enum."""
         if v is None or isinstance(v, str):
             return v
 
         v = cast(RetryPolicy, v)
         return v.value
 
     @validator("limit", pre=True)
     def _convert_limit(cls, v):
+        """Converts the `limit` field from the union specification into a `str`."""
         if v is None or isinstance(v, IntOrString):
             return v
 
         return str(v)  # int or str
 
     def build(self) -> _ModelRetryStrategy:
+        """Builds the generated `RetryStrategy` representation of the retry strategy."""
         return _ModelRetryStrategy(
             affinity=self.affinity,
             backoff=self.backoff,
             expression=self.expression,
             limit=self.limit,
             retry_policy=self.retry_policy,
         )
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/runner.py` & `hera_workflows-5.6.0/src/hera/workflows/runner.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,14 +1,15 @@
+"""The runner module contains the functionality required for the script runner."""
 import argparse
 import functools
 import importlib
 import inspect
 import json
 from pathlib import Path
-from typing import Any
+from typing import Any, Callable, cast
 
 from pydantic import validate_arguments
 
 from hera.shared.serialization import serialize
 
 
 def _ignore_unmatched_kwargs(f):
@@ -25,21 +26,23 @@
         # and transform them to the correct type
         filtered_kwargs = {key: _parse(value, key, f) for key, value in kwargs.items() if _is_kwarg_of(key, f)}
         return f(**filtered_kwargs)
 
     return inner
 
 
-def _contains_var_kwarg(f):
+def _contains_var_kwarg(f: Callable) -> bool:
+    """Tells whether the given callable contains a keyword argument."""
     return any(param.kind == inspect.Parameter.VAR_KEYWORD for param in inspect.signature(f).parameters.values())
 
 
-def _is_kwarg_of(key, f):
+def _is_kwarg_of(key: str, f: Callable) -> bool:
+    """Tells whether the given `key` identifies a keyword argument of the given callable."""
     param = inspect.signature(f).parameters.get(key)
-    return param and (
+    return param is not None and (
         param.kind is inspect.Parameter.KEYWORD_ONLY or param.kind is inspect.Parameter.POSITIONAL_OR_KEYWORD
     )
 
 
 def _parse(value, key, f):
     """Parse a value to the correct type.
 
@@ -56,16 +59,16 @@
         return value
     try:
         return json.loads(value)
     except json.JSONDecodeError:
         return value
 
 
-def _is_str_kwarg_of(key, f):
-    # check if param `key` of function `f` has a type annotation of a subclass of str
+def _is_str_kwarg_of(key: str, f: Callable):
+    """Check if param `key` of function `f` has a type annotation of a subclass of str."""
     type_ = inspect.signature(f).parameters[key].annotation
     if not type_:
         return True
     try:
         return issubclass(type_, str)
     except TypeError:
         # If this happens then it means that the annotation is complex type annotation
@@ -73,14 +76,15 @@
         return False
 
 
 def _runner(entrypoint: str, kwargs_list: Any) -> str:
     """Run a function with a list of kwargs.
 
     Args:
+        entrypoint: The path to the script within the container to execute.
         module: The module path to import the function from.
         function_name: The name of the function to run.
         kwargs_list: A list of kwargs to pass to the function.
 
     Returns:
         The result of the function as a string.
     """
@@ -93,32 +97,39 @@
         function = function.wrapped_function
     # convert the kwargs list to a dict
     kwargs = {}
     for kwarg in kwargs_list:
         if "name" not in kwarg or "value" not in kwarg:
             continue
         # sanitize the key for python
-        key = serialize(kwarg["name"]).replace("-", "_")
+        key = cast(str, serialize(kwarg["name"])).replace("-", "_")
         value = kwarg["value"]
         kwargs[key] = value
     function = validate_arguments(function)
     function = _ignore_unmatched_kwargs(function)
     return function(**kwargs)
 
 
-# write an argparse for the runner function that takes module and function name
-# as flags and a path to a json file as an argument
 def _parse_args():
+    """Creates an argparse for the runner function.
+
+    The returned argparse takes a module and function name as flags and a path to a json file as an argument.
+    """
     parser = argparse.ArgumentParser()
     parser.add_argument("--entrypoint", "-e", type=str, required=True)
     parser.add_argument("args_path", type=Path)
     return parser.parse_args()
 
 
 def _run():
+    """Runs a function from a specific path using parsed arguments from Argo.
+
+    Note that this prints the result of the function to stdout, which is the normal mode of operation for Argo. Any
+    output of a Python function submitted via a `Script.source` field results in outputs sent to stdout.
+    """
     args = _parse_args()
     kwargs_list = json.loads(args.args_path.read_text() or r"[]")
     result = _runner(args.entrypoint, kwargs_list)
     if not result:
         return
     print(serialize(result))
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/script.py` & `hera_workflows-5.6.0/src/hera/workflows/script.py`

 * *Files 4% similar despite different names*

```diff
@@ -60,15 +60,15 @@
 
     @abstractmethod
     def generate_source(self, instance: "Script") -> str:
         """A function that can inspect the Script instance and generate the source field."""
         raise NotImplementedError
 
     def transform_values(self, cls: Type["Script"], values: Any) -> Any:
-        """A function that will be inokved by the root validator of the Script class."""
+        """A function that will be invoked by the root validator of the Script class."""
         return values
 
     def transform_script_template_post_build(
         self, instance: "Script", script: _ModelScriptTemplate
     ) -> _ModelScriptTemplate:
         """A hook to transform the generated script template."""
         return script
@@ -82,16 +82,18 @@
     EnvIOMixin,
     CallableTemplateMixin,
     ContainerMixin,
     TemplateMixin,
     ResourceMixin,
     VolumeMountMixin,
 ):
-    """A Script acts as a wrapper around a container. In Hera this defaults to a "python:3.8" image
-    specified by global_config.image, which runs a python source specified by `Script.source`.
+    """A Script acts as a wrapper around a container.
+
+    In Hera this defaults to a "python:3.8" image specified by global_config.image, which runs a python source
+    specified by `Script.source`.
     """
 
     container_name: Optional[str] = None
     args: Optional[List[str]] = None
     command: Optional[List[str]] = None
     lifecycle: Optional[Lifecycle] = None
     security_context: Optional[SecurityContext] = None
@@ -287,15 +289,15 @@
     workflow context, so users do not have to worry about that.
 
     Parameters
     ----------
     **script_kwargs
         Keyword arguments to be passed to the Script object.
 
-    Returns
+    Returns:
     -------
     Callable
         Function that wraps a given function into a `Script`.
     """
 
     def script_wrapper(
         func: Callable[FuncIns, FuncR],
@@ -303,15 +305,15 @@
         """Wraps the given callable into a `Script` object that can be invoked.
 
         Parameters
         ----------
         func: Callable
             Function to wrap.
 
-        Returns
+        Returns:
         -------
         Callable
             Another callable that represents the `Script` object `__call__` method when in a Steps or DAG context,
             otherwise return the callable function unchanged.
         """
         s = Script(name=func.__name__.replace("_", "-"), source=func, **script_kwargs)
 
@@ -321,36 +323,46 @@
 
         @overload
         def task_wrapper(*args: ScriptIns.args, **kwargs: ScriptIns.kwargs) -> Union[Step, Task]:
             ...
 
         @wraps(func)
         def task_wrapper(*args, **kwargs):
-            """Invokes a `Script` object's `__call__` method using the given `task_params`"""
+            """Invokes a `Script` object's `__call__` method using the given `task_params`."""
             if _context.active:
                 return s.__call__(*args, **kwargs)
             return func(*args, **kwargs)
 
         # Set the wrapped function to the original function so that we can use it later
         task_wrapper.wrapped_function = func  # type: ignore
         return task_wrapper
 
     return script_wrapper
 
 
 class InlineScriptConstructor(ScriptConstructor):
+    """`InlineScriptConstructor` is a script constructor that submits a script as a `source` to Argo.
+
+    This script constructor is focused on taking a Python script/function "as is" for remote execution. The
+    constructor processes the script to infer what parameters it needs to deserialize so the script can execute.
+    The submitted script will contain prefixes such as new imports, e.g. `import os`, `import json`, etc. and
+    will contain the necessary `json.loads` calls to deserialize the parameters so they are usable by the script just
+    like a normal Python script/function.
+    """
+
     add_cwd_to_sys_path: Optional[bool] = None
 
     def _get_param_script_portion(self, instance: Script) -> str:
-        """Constructs and returns a script that loads the parameters of the specified arguments. Since Argo passes
-        parameters through {{input.parameters.name}} it can be very cumbersome for users to manage that. This creates a
-        script that automatically imports json and loads/adds code to interpret each independent argument into the
-        script.
+        """Constructs and returns a script that loads the parameters of the specified arguments.
+
+        Since Argo passes parameters through `{{input.parameters.name}}` it can be very cumbersome for users to
+        manage that. This creates a script that automatically imports json and loads/adds code to interpret
+        each independent argument into the script.
 
-        Returns
+        Returns:
         -------
         str
             The string representation of the script to load.
         """
         inputs = instance._build_inputs()
         assert inputs
         extract = "import json\n"
@@ -360,20 +372,22 @@
             # which is why this captures that in an except. This is only used for `InputFrom` cases as the extra
             # payload of the script is not necessary when regular input is set on the task via `func_params`
             extract += f"""try: {param.name} = json.loads(r'''{{{{inputs.parameters.{param.name}}}}}''')\n"""
             extract += f"""except: {param.name} = r'''{{{{inputs.parameters.{param.name}}}}}'''\n"""
         return textwrap.dedent(extract)
 
     def generate_source(self, instance: Script) -> str:
-        """Assembles and returns a script representation of the given function, along with the extra script material
-        prefixed to the string. The script is expected to be a callable function the client is interested in submitting
-        for execution on Argo and the script_extra material represents the parameter loading part obtained, likely,
-        through get_param_script_portion.
+        """Assembles and returns a script representation of the given function.
+
+        This also assembles any extra script material prefixed to the string source.
+        The script is expected to be a callable function the client is interested in submitting
+        for execution on Argo and the `script_extra` material represents the parameter loading part obtained, likely,
+        through `get_param_script_portion`.
 
-        Returns
+        Returns:
         -------
         str
             Final formatted script.
         """
         if not callable(instance.source):
             assert isinstance(instance.source, str)
             return instance.source
@@ -403,17 +417,28 @@
 
         s = "\n".join(content[i + 1 :])
         script += textwrap.dedent(s)
         return textwrap.dedent(script)
 
 
 class RunnerScriptConstructor(ScriptConstructor, ExperimentalMixin):
+    """`RunnerScriptConstructor` is a script constructor that runs a script in a container.
+
+    The runner script, also known as "The Hera runner", takes a script/Python function definition, inferts the path
+    to the function (module import), assembles a path to invoke the function, and passes any specified parameters
+    to the function. This helps users "save" on the `source` space required for submitting a function for remote
+    execution on Argo. Execution within the container *requires* the executing container to include the file that
+    contains the submitted script. More specifically, the container must be created in some process (e.g. CI), so that
+    it conains the script to run remotely.
+    """
+
     _flag: str = "script_runner"
 
     def transform_values(self, cls: Type[Script], values: Any) -> Any:
+        """A function that can inspect the Script instance and generate the source field."""
         if not callable(values.get("source")):
             return values
 
         if values.get("args") is not None:
             raise ValueError("Cannot specify args when callable is True")
         values["args"] = [
             "-m",
@@ -421,11 +446,12 @@
             "-e",
             f'{values["source"].__module__}:{values["source"].__name__}',
         ]
 
         return values
 
     def generate_source(self, instance: Script) -> str:
+        """A function that can inspect the Script instance and generate the source field."""
         return f"{g.inputs.parameters:$}"
 
 
 __all__ = ["Script", "script", "ScriptConstructor", "InlineScriptConstructor", "RunnerScriptConstructor"]
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/service.py` & `hera_workflows-5.6.0/src/hera/workflows/service.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+"""[DO NOT MODIFY] Auto-generated by `hera.scripts.service.py`."""
 from typing import Optional, cast
 from urllib.parse import urljoin
 
 import requests
 
 from hera.exceptions import exception_from_server_response
 from hera.shared import global_config
@@ -48,25 +49,29 @@
     WorkflowTemplateList,
     WorkflowTemplateUpdateRequest,
     WorkflowTerminateRequest,
 )
 
 
 def valid_host_scheme(host: str) -> bool:
+    """Validates the the given `host` starts with either `http` or `https`."""
     return host.startswith("http://") or host.startswith("https://")
 
 
 class WorkflowsService:
+    """The core workflows service for interacting with the Argo server."""
+
     def __init__(
         self,
         host: Optional[str] = None,
         verify_ssl: Optional[bool] = None,
         token: Optional[str] = None,
         namespace: Optional[str] = None,
-    ):
+    ) -> None:
+        """Workflows service constructor."""
         self.host = cast(str, host or global_config.host)
         self.verify_ssl = verify_ssl if verify_ssl is not None else global_config.verify_ssl
         self.token = token or global_config.token
         self.namespace = namespace or global_config.namespace
 
     def list_archived_workflows(
         self,
@@ -77,14 +82,15 @@
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
         name_prefix: Optional[str] = None,
     ) -> WorkflowList:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/archived-workflows"),
             params={
                 "listOptions.labelSelector": label_selector,
                 "listOptions.fieldSelector": field_selector,
                 "listOptions.watch": watch,
@@ -103,14 +109,15 @@
 
         if resp.ok:
             return WorkflowList(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def list_archived_workflow_label_keys(self) -> LabelKeys:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/archived-workflows-label-keys"),
             params=None,
             headers={"Authorization": f"Bearer {self.token}"},
             data=None,
             verify=self.verify_ssl,
@@ -129,14 +136,15 @@
         allow_watch_bookmarks: Optional[bool] = None,
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
     ) -> LabelValues:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/archived-workflows-label-values"),
             params={
                 "listOptions.labelSelector": label_selector,
                 "listOptions.fieldSelector": field_selector,
                 "listOptions.watch": watch,
@@ -154,14 +162,15 @@
 
         if resp.ok:
             return LabelValues(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def get_archived_workflow(self, uid: str) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/archived-workflows/{uid}").format(uid=uid),
             params=None,
             headers={"Authorization": f"Bearer {self.token}"},
             data=None,
             verify=self.verify_ssl,
@@ -169,14 +178,15 @@
 
         if resp.ok:
             return Workflow(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def delete_archived_workflow(self, uid: str) -> ArchivedWorkflowDeletedResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.delete(
             url=urljoin(self.host, "api/v1/archived-workflows/{uid}").format(uid=uid),
             params=None,
             headers={"Authorization": f"Bearer {self.token}"},
             data=None,
             verify=self.verify_ssl,
@@ -184,36 +194,38 @@
 
         if resp.ok:
             return ArchivedWorkflowDeletedResponse()
 
         raise exception_from_server_response(resp)
 
     def resubmit_archived_workflow(self, uid: str, req: ResubmitArchivedWorkflowRequest) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/archived-workflows/{uid}/resubmit").format(uid=uid),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return Workflow(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def retry_archived_workflow(self, uid: str, req: RetryArchivedWorkflowRequest) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/archived-workflows/{uid}/retry").format(uid=uid),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -229,14 +241,15 @@
         allow_watch_bookmarks: Optional[bool] = None,
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
     ) -> ClusterWorkflowTemplateList:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/cluster-workflow-templates"),
             params={
                 "listOptions.labelSelector": label_selector,
                 "listOptions.fieldSelector": field_selector,
                 "listOptions.watch": watch,
@@ -254,50 +267,53 @@
 
         if resp.ok:
             return ClusterWorkflowTemplateList(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def create_cluster_workflow_template(self, req: ClusterWorkflowTemplateCreateRequest) -> ClusterWorkflowTemplate:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/cluster-workflow-templates"),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return ClusterWorkflowTemplate(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def lint_cluster_workflow_template(self, req: ClusterWorkflowTemplateLintRequest) -> ClusterWorkflowTemplate:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/cluster-workflow-templates/lint"),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return ClusterWorkflowTemplate(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def get_cluster_workflow_template(
         self, name: str, resource_version: Optional[str] = None
     ) -> ClusterWorkflowTemplate:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/cluster-workflow-templates/{name}").format(name=name),
             params={"getOptions.resourceVersion": resource_version},
             headers={"Authorization": f"Bearer {self.token}"},
             data=None,
             verify=self.verify_ssl,
@@ -307,19 +323,20 @@
             return ClusterWorkflowTemplate(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def update_cluster_workflow_template(
         self, name: str, req: ClusterWorkflowTemplateUpdateRequest
     ) -> ClusterWorkflowTemplate:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/cluster-workflow-templates/{name}").format(name=name),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -333,14 +350,15 @@
         grace_period_seconds: Optional[str] = None,
         uid: Optional[str] = None,
         resource_version: Optional[str] = None,
         orphan_dependents: Optional[bool] = None,
         propagation_policy: Optional[str] = None,
         dry_run: Optional[list] = None,
     ) -> ClusterWorkflowTemplateDeleteResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.delete(
             url=urljoin(self.host, "api/v1/cluster-workflow-templates/{name}").format(name=name),
             params={
                 "deleteOptions.gracePeriodSeconds": grace_period_seconds,
                 "deleteOptions.preconditions.uid": uid,
                 "deleteOptions.preconditions.resourceVersion": resource_version,
@@ -367,14 +385,15 @@
         allow_watch_bookmarks: Optional[bool] = None,
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
     ) -> CronWorkflowList:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/cron-workflows/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "listOptions.labelSelector": label_selector,
@@ -405,21 +424,22 @@
                 # See `hera.scripts.service.ServiceEndpoint.__str__` for more details.
                 resp_json["status"] = None
             return CronWorkflowList(**resp_json)
 
         raise exception_from_server_response(resp)
 
     def create_cron_workflow(self, req: CreateCronWorkflowRequest, namespace: Optional[str] = None) -> CronWorkflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/cron-workflows/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -435,21 +455,22 @@
                 # See `hera.scripts.service.ServiceEndpoint.__str__` for more details.
                 resp_json["status"] = None
             return CronWorkflow(**resp_json)
 
         raise exception_from_server_response(resp)
 
     def lint_cron_workflow(self, req: LintCronWorkflowRequest, namespace: Optional[str] = None) -> CronWorkflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/cron-workflows/{namespace}/lint").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -467,14 +488,15 @@
             return CronWorkflow(**resp_json)
 
         raise exception_from_server_response(resp)
 
     def get_cron_workflow(
         self, name: str, namespace: Optional[str] = None, resource_version: Optional[str] = None
     ) -> CronWorkflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/cron-workflows/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params={"getOptions.resourceVersion": resource_version},
             headers={"Authorization": f"Bearer {self.token}"},
@@ -497,21 +519,22 @@
             return CronWorkflow(**resp_json)
 
         raise exception_from_server_response(resp)
 
     def update_cron_workflow(
         self, name: str, req: UpdateCronWorkflowRequest, namespace: Optional[str] = None
     ) -> CronWorkflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/cron-workflows/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -537,14 +560,15 @@
         grace_period_seconds: Optional[str] = None,
         uid: Optional[str] = None,
         resource_version: Optional[str] = None,
         orphan_dependents: Optional[bool] = None,
         propagation_policy: Optional[str] = None,
         dry_run: Optional[list] = None,
     ) -> CronWorkflowDeletedResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.delete(
             url=urljoin(self.host, "api/v1/cron-workflows/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "deleteOptions.gracePeriodSeconds": grace_period_seconds,
@@ -563,21 +587,22 @@
             return CronWorkflowDeletedResponse()
 
         raise exception_from_server_response(resp)
 
     def resume_cron_workflow(
         self, name: str, req: CronWorkflowResumeRequest, namespace: Optional[str] = None
     ) -> CronWorkflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/cron-workflows/{namespace}/{name}/resume").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -595,21 +620,22 @@
             return CronWorkflow(**resp_json)
 
         raise exception_from_server_response(resp)
 
     def suspend_cron_workflow(
         self, name: str, req: CronWorkflowSuspendRequest, namespace: Optional[str] = None
     ) -> CronWorkflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/cron-workflows/{namespace}/{name}/suspend").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -625,14 +651,15 @@
                 # See `hera.scripts.service.ServiceEndpoint.__str__` for more details.
                 resp_json["status"] = None
             return CronWorkflow(**resp_json)
 
         raise exception_from_server_response(resp)
 
     def get_info(self) -> InfoResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/info"),
             params=None,
             headers={"Authorization": f"Bearer {self.token}"},
             data=None,
             verify=self.verify_ssl,
@@ -640,14 +667,15 @@
 
         if resp.ok:
             return InfoResponse()
 
         raise exception_from_server_response(resp)
 
     def get_user_info(self) -> GetUserInfoResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/userinfo"),
             params=None,
             headers={"Authorization": f"Bearer {self.token}"},
             data=None,
             verify=self.verify_ssl,
@@ -655,14 +683,15 @@
 
         if resp.ok:
             return GetUserInfoResponse()
 
         raise exception_from_server_response(resp)
 
     def get_version(self) -> Version:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/version"),
             params=None,
             headers={"Authorization": f"Bearer {self.token}"},
             data=None,
             verify=self.verify_ssl,
@@ -682,14 +711,15 @@
         allow_watch_bookmarks: Optional[bool] = None,
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
     ) -> WorkflowTemplateList:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/workflow-templates/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "listOptions.labelSelector": label_selector,
@@ -711,56 +741,59 @@
             return WorkflowTemplateList(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def create_workflow_template(
         self, req: WorkflowTemplateCreateRequest, namespace: Optional[str] = None
     ) -> WorkflowTemplate:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/workflow-templates/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return WorkflowTemplate(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def lint_workflow_template(
         self, req: WorkflowTemplateLintRequest, namespace: Optional[str] = None
     ) -> WorkflowTemplate:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/workflow-templates/{namespace}/lint").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return WorkflowTemplate(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def get_workflow_template(
         self, name: str, namespace: Optional[str] = None, resource_version: Optional[str] = None
     ) -> WorkflowTemplate:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/workflow-templates/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params={"getOptions.resourceVersion": resource_version},
             headers={"Authorization": f"Bearer {self.token}"},
@@ -772,21 +805,22 @@
             return WorkflowTemplate(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def update_workflow_template(
         self, name: str, req: WorkflowTemplateUpdateRequest, namespace: Optional[str] = None
     ) -> WorkflowTemplate:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/workflow-templates/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -801,14 +835,15 @@
         grace_period_seconds: Optional[str] = None,
         uid: Optional[str] = None,
         resource_version: Optional[str] = None,
         orphan_dependents: Optional[bool] = None,
         propagation_policy: Optional[str] = None,
         dry_run: Optional[list] = None,
     ) -> WorkflowTemplateDeleteResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.delete(
             url=urljoin(self.host, "api/v1/workflow-templates/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "deleteOptions.gracePeriodSeconds": grace_period_seconds,
@@ -838,14 +873,15 @@
         resource_version: Optional[str] = None,
         resource_version_match: Optional[str] = None,
         timeout_seconds: Optional[str] = None,
         limit: Optional[str] = None,
         continue_: Optional[str] = None,
         fields: Optional[str] = None,
     ) -> WorkflowList:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/workflows/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "listOptions.labelSelector": label_selector,
@@ -866,59 +902,62 @@
 
         if resp.ok:
             return WorkflowList(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def create_workflow(self, req: WorkflowCreateRequest, namespace: Optional[str] = None) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/workflows/{namespace}").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return Workflow(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def lint_workflow(self, req: WorkflowLintRequest, namespace: Optional[str] = None) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/lint").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return Workflow(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def submit_workflow(self, req: WorkflowSubmitRequest, namespace: Optional[str] = None) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.post(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/submit").format(
                 namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
@@ -929,14 +968,15 @@
     def get_workflow(
         self,
         name: str,
         namespace: Optional[str] = None,
         resource_version: Optional[str] = None,
         fields: Optional[str] = None,
     ) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params={"getOptions.resourceVersion": resource_version, "fields": fields},
             headers={"Authorization": f"Bearer {self.token}"},
@@ -957,14 +997,15 @@
         uid: Optional[str] = None,
         resource_version: Optional[str] = None,
         orphan_dependents: Optional[bool] = None,
         propagation_policy: Optional[str] = None,
         dry_run: Optional[list] = None,
         force: Optional[bool] = None,
     ) -> WorkflowDeleteResponse:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.delete(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/{name}").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "deleteOptions.gracePeriodSeconds": grace_period_seconds,
@@ -999,14 +1040,15 @@
         timestamps: Optional[bool] = None,
         tail_lines: Optional[str] = None,
         limit_bytes: Optional[str] = None,
         insecure_skip_tls_verify_backend: Optional[bool] = None,
         grep: Optional[str] = None,
         selector: Optional[str] = None,
     ) -> V1alpha1LogEntry:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.get(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/{name}/log").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params={
                 "podName": pod_name,
@@ -1030,137 +1072,144 @@
 
         if resp.ok:
             return V1alpha1LogEntry(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def resubmit_workflow(self, name: str, req: WorkflowResubmitRequest, namespace: Optional[str] = None) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/{name}/resubmit").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return Workflow(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def resume_workflow(self, name: str, req: WorkflowResumeRequest, namespace: Optional[str] = None) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/{name}/resume").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return Workflow(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def retry_workflow(self, name: str, req: WorkflowRetryRequest, namespace: Optional[str] = None) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/{name}/retry").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return Workflow(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def set_workflow(self, name: str, req: WorkflowSetRequest, namespace: Optional[str] = None) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/{name}/set").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return Workflow(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def stop_workflow(self, name: str, req: WorkflowStopRequest, namespace: Optional[str] = None) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/{name}/stop").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return Workflow(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def suspend_workflow(self, name: str, req: WorkflowSuspendRequest, namespace: Optional[str] = None) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/{name}/suspend").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
             return Workflow(**resp.json())
 
         raise exception_from_server_response(resp)
 
     def terminate_workflow(
         self, name: str, req: WorkflowTerminateRequest, namespace: Optional[str] = None
     ) -> Workflow:
+        """API documentation."""
         assert valid_host_scheme(self.host), "The host scheme is required for service usage"
         resp = requests.put(
             url=urljoin(self.host, "api/v1/workflows/{namespace}/{name}/terminate").format(
                 name=name, namespace=namespace if namespace is not None else self.namespace
             ),
             params=None,
-            headers={"Authorization": f"Bearer {self.token}"},
+            headers={"Authorization": f"Bearer {self.token}", "Content-Type": "application/json"},
             data=req.json(
                 exclude_none=True, by_alias=True, skip_defaults=True, exclude_unset=True, exclude_defaults=True
             ),
             verify=self.verify_ssl,
         )
 
         if resp.ok:
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/steps.py` & `hera_workflows-5.6.0/src/hera/workflows/steps.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,14 +3,15 @@
 See https://argoproj.github.io/argo-workflows/walk-through/steps/
 for more on Steps.
 """
 from typing import Any, List, Optional, Union
 
 from hera.workflows._mixins import (
     ArgumentsMixin,
+    CallableTemplateMixin,
     ContextMixin,
     IOMixin,
     ItemMixin,
     ParameterMixin,
     SubNodeMixin,
     TemplateInvocatorSubNodeMixin,
     TemplateMixin,
@@ -26,17 +27,17 @@
 class Step(
     TemplateInvocatorSubNodeMixin,
     ArgumentsMixin,
     SubNodeMixin,
     ParameterMixin,
     ItemMixin,
 ):
-    """
-    Step is used to run a given template. Must be instantiated under a Steps or Parallel context, or
-    outside a Workflow.
+    """Step is used to run a given template.
+
+    Must be instantiated under a Steps or Parallel context, or outside a Workflow.
     """
 
     @property
     def _subtype(self) -> str:
         return "steps"
 
     def _build_as_workflow_step(self) -> _ModelWorkflowStep:
@@ -99,20 +100,20 @@
                 steps.append(step)
             else:
                 raise InvalidType(type(step))
         return steps
 
 
 class Steps(
-    ContextMixin,
     IOMixin,
     TemplateMixin,
+    CallableTemplateMixin,
+    ContextMixin,
 ):
-    """A Steps template invocator is used to define a sequence of steps which can run
-    sequentially or in parallel.
+    """A Steps template invocator is used to define a sequence of steps which can run sequentially or in parallel.
 
     Steps implements the contextmanager interface so allows usage of `with`, under which any
     `hera.workflows.steps.Step` objects instantiated will be added to the Steps' list of sub_steps.
 
     * Step and Parallel objects initialised within a Steps context will be added to the list of sub_steps
     in the order they are initialised.
     * All Step objects initialised within a Parallel context will run in parallel.
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/suspend.py` & `hera_workflows-5.6.0/src/hera/workflows/suspend.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,16 +1,14 @@
-"""The suspend module provides the Suspend class. The Suspend template in Hera
-provides a convenience wrapper around the "Intermediate Parameters" Argo feature.
+"""The suspend module provides the Suspend class.
 
-See https://argoproj.github.io/argo-workflows/walk-through/suspending/
-for more on suspending.
+The Suspend template in Hera provides a convenience wrapper around the "Intermediate Parameters" Argo feature.
 
-See https://argoproj.github.io/argo-workflows/intermediate-inputs/
-for more on intermediate parameters.
+See https://argoproj.github.io/argo-workflows/walk-through/suspending/ for more on suspending.
 
+See https://argoproj.github.io/argo-workflows/intermediate-inputs/ for more on intermediate parameters.
 """
 from typing import List, Optional, Union
 
 from hera.workflows._mixins import CallableTemplateMixin, TemplateMixin
 from hera.workflows.models import (
     Inputs,
     Outputs,
@@ -20,18 +18,19 @@
 from hera.workflows.parameter import Parameter
 
 
 class Suspend(
     TemplateMixin,
     CallableTemplateMixin,
 ):
-    """The Suspend template allows the user to pause a workflow for a specified length of time
-    given by `duration` or indefinitely (i.e. until manually resumed). The Suspend template also
-    allows you to specify `intermediate_parameters` which will replicate the given parameters to
-    the "inputs" and "outputs" of the template, resulting in a Suspend template that pauses and
+    """The Suspend template allows the user to pause a workflow for a specified length of time.
+
+    The workflow can pause based on the given by `duration` or indefinitely (i.e. until manually resumed).
+    The Suspend template also allows you to specify `intermediate_parameters` which will replicate the given
+    parameters to the "inputs" and "outputs" of the template, resulting in a Suspend template that pauses and
     waits for values from the user for the given list of parameters.
     """
 
     duration: Optional[Union[int, str]] = None
     intermediate_parameters: List[Parameter] = []
 
     def _build_suspend_template(self) -> _ModelSuspendTemplate:
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/task.py` & `hera_workflows-5.6.0/src/hera/workflows/task.py`

 * *Files 12% similar despite different names*

```diff
@@ -22,16 +22,18 @@
 )
 from hera.workflows.operator import Operator
 from hera.workflows.protocol import Templatable
 from hera.workflows.workflow_status import WorkflowStatus
 
 
 class TaskResult(Enum):
-    """The enumeration of Task Results specified at
-    https://argoproj.github.io/argo-workflows/enhanced-depends-logic/#depends
+    """The enumeration of Task Results.
+
+    See Also:
+        https://argoproj.github.io/argo-workflows/enhanced-depends-logic/#depends.
     """
 
     failed = "Failed"
     succeeded = "Succeeded"
     errored = "Errored"
     skipped = "Skipped"
     omitted = "Omitted"
@@ -43,98 +45,98 @@
 class Task(
     TemplateInvocatorSubNodeMixin,
     ArgumentsMixin,
     SubNodeMixin,
     ParameterMixin,
     ItemMixin,
 ):
-    """Task is used to run a given template within a DAG. Must be instantiated under a DAG context.
+    r"""Task is used to run a given template within a DAG. Must be instantiated under a DAG context.
 
-## Dependencies
+    ## Dependencies
 
-Any `Tasks` without a dependency defined will start immediately.
+    Any `Tasks` without a dependency defined will start immediately.
 
-Dependencies between Tasks can be described using the convenience syntax `>>`, for example:
+    Dependencies between Tasks can be described using the convenience syntax `>>`, for example:
 
-```py
+    ```py
     A = Task(...)
     B = Task(...)
     A >> B
-```
+    ```
 
-describes the relationships:
+    describes the relationships:
 
-* "A has no dependencies (so starts immediately)
-* "B depends on A".
+    * "A has no dependencies (so starts immediately)
+    * "B depends on A".
 
-As a diagram:
+    As a diagram:
 
-```
-A
-|
-B
-```
+    ```
+    A
+    |
+    B
+    ```
 
-`A >> B` is equivalent to `A.next(B)`.
+    `A >> B` is equivalent to `A.next(B)`.
 
-## Lists of Tasks
+    ## Lists of Tasks
 
-A list of Tasks used with the rshift syntax describes an "AND" dependency between the single Task on the left of
-`>>` and the list Tasks to the right of `>>` (or vice versa). A list of Tasks on both sides of `>>` is not supported.
+    A list of Tasks used with the rshift syntax describes an "AND" dependency between the single Task on the left of
+    `>>` and the list Tasks to the right of `>>` (or vice versa). A list of Tasks on both sides of `>>` is not supported.
 
-For example:
+    For example:
 
-```
+    ```
     A = Task(...)
     B = Task(...)
     C = Task(...)
     D = Task(...)
     A >> [B, C] >> D
-```
+    ```
 
-describes the relationships:
+    describes the relationships:
 
-* "A has no dependencies
-* "B AND C depend on A"
-* "D depends on B AND C"
+    * "A has no dependencies
+    * "B AND C depend on A"
+    * "D depends on B AND C"
 
-As a diagram:
+    As a diagram:
 
-```
-  A
- / \\
-B   C
- \ /
-  D
-```
+    ```
+     A
+    / \\
+   B   C
+    \ /
+     D
+    ```
 
-Dependencies can be described over multiple statements:
+    Dependencies can be described over multiple statements:
 
-```
+    ```
     A = Task(...)
     B = Task(...)
     C = Task(...)
     D = Task(...)
     A >> [C, D]
     B >> [C, D]
-```
+    ```
 
-describes the relationships:
+    describes the relationships:
 
-* "A and B have no dependencies
-* "C depends on A AND B"
-* "D depends on A AND B"
+    * "A and B have no dependencies
+    * "C depends on A AND B"
+    * "D depends on A AND B"
 
-As a diagram:
+    As a diagram:
 
-```
-A   B
-| X |
-C   D
-```
+    ```
+    A   B
+    | X |
+    C   D
+    ```
     """
 
     dependencies: Optional[List[str]] = None
     depends: Optional[str] = None
 
     def _get_dependency_tasks(self) -> List[str]:
         if self.depends is None:
@@ -194,54 +196,61 @@
                     o, Task
                 ), f"Unknown list item type {type(o)} specified using right bitshift operator `>>`"
                 self.next(o)
             return other
         raise ValueError(f"Unknown type {type(other)} provided to `__rshift__`")
 
     def __or__(self, other: Union[Task, str]) -> str:
-        """Adds a condition of"""
+        """Adds a condition of."""
         if isinstance(other, Task):
             return f"({self.name} || {other.name})"
         assert isinstance(other, str), f"Unknown type {type(other)} specified using `|` operator"
         return f"{self.name} || {other}"
 
     def on_workflow_status(self, status: WorkflowStatus, op: Operator = Operator.equals) -> Task:
+        """Sets the current task to run when the workflow finishes with the specified status."""
         expression = f"{{{{workflow.status}}}} {op} {status}"
         if self.when:
             self.when += f" {Operator.and_} {expression}"
         else:
             self.when = expression
         return self
 
     def on_success(self, other: Task) -> Task:
+        """Sets the current task to run when the given `other` task succeeds."""
         return self.next(other, on=TaskResult.succeeded)
 
     def on_failure(self, other: Task) -> Task:
+        """Sets the current task to run when the given `other` task fails."""
         return self.next(other, on=TaskResult.failed)
 
     def on_error(self, other: Task) -> Task:
+        """Sets the current task to run when the given `other` task errors."""
         return self.next(other, on=TaskResult.errored)
 
     def on_other_result(self, other: Task, value: str, operator: Operator = Operator.equals) -> Task:
+        """Sets the current task to run when the given `other` task results in the specified `value` result."""
         expression = f"{other.result} {operator} {value}"
         if self.when:
             self.when += f" {Operator.and_} {expression}"
         else:
             self.when = expression
         other.next(self)
         return self
 
     def when_any_succeeded(self, other: Task) -> Task:
+        """Sets the current task to run when the given `other` task succeedds."""
         assert (self.with_param is not None) or (
             self.with_sequence is not None
         ), "Can only use `when_all_failed` when using `with_param` or `with_sequence`"
 
         return self.next(other, on=TaskResult.any_succeeded)
 
     def when_all_failed(self, other: Task) -> Task:
+        """Sets the current task to run when the given `other` task has failed."""
         assert (self.with_param is not None) or (
             self.with_sequence is not None
         ), "Can only use `when_all_failed` when using `with_param` or `with_sequence`"
 
         return self.next(other, on=TaskResult.all_failed)
 
     def _build_dag_task(self) -> _ModelDAGTask:
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/validators.py` & `hera_workflows-5.6.0/src/hera/workflows/validators.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,32 +1,32 @@
-"""Holds a collection of validators that are shared in V1"""
+"""Holds a collection of validators that are shared in V1."""
 import re
 from typing import Optional
 
 
 def validate_name(name: str, max_length: Optional[int] = None, generate_name: bool = False) -> str:
-    """Validates a name according to standard argo/kubernetes limitations
+    """Validates a name according to standard argo/kubernetes limitations.
 
     Parameters
     ----------
     name: str
         The name which should be validated.
     max_length: Optional[int] = None
         Specify a maximum length of the name.
         Example: Kubernetes labels have a maximum length of 63 characters.
     generate_name: bool = False
         Whether the provided name is to be used as a prefix for name generation.
         If set, name is allowed to end in a single dot (.) or any number of hyphens (-).
 
-    Raises
+    Raises:
     ------
     ValueError
         When the name is invalid according to specifications.
 
-    Notes
+    Notes:
     -----
     Official doc on object names in Kubernetes:
     https://kubernetes.io/docs/concepts/overview/working-with-objects/names/
     """
     if max_length and len(name) > max_length:
         raise ValueError(f"Name is too long. Max length: {max_length}, found: {len(name)}")
     if "_" in name:
@@ -50,15 +50,15 @@
     1Gi, etc.
 
     Parameters
     ----------
     value: str
         The value to validate the units of.
 
-    Raises
+    Raises:
     ------
     ValueError
         When the units cannot be extracted from the given value.
     AssertionError
         When the identified unit is not a supported one. The supported units are ['Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei'].
     """
     supported_units = ["Ki", "Mi", "Gi", "Ti", "Pi", "Ei"]
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/volume.py` & `hera_workflows-5.6.0/src/hera/workflows/volume.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+"""Volume module provides all Argo volume types that can be used via Hera."""
 import uuid
 from enum import Enum
 from typing import List, Optional, Union, cast
 
 from pydantic import root_validator, validator
 
 from hera.workflows.models import (
@@ -43,15 +44,15 @@
 )
 from hera.workflows.validators import validate_storage_units
 
 
 class AccessMode(Enum):
     """A representations of the volume access modes for Kubernetes.
 
-    Notes
+    Notes:
     -----
     See: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes for more information.
     """
 
     read_write_once = "ReadWriteOnce"
     """
     The volume can be mounted as read-write by a single node. ReadWriteOnce access mode still can allow multiple
@@ -67,19 +68,22 @@
     read_write_once_pod = "ReadWriteOncePod"
     """
     The volume can be mounted as read-write by a single Pod. Use ReadWriteOncePod access mode if you want to
     ensure that only one pod across whole cluster can read that PVC or write to it. This is only supported for CSI
     volumes and Kubernetes version 1.22+.
     """
 
-    def __str__(self):
+    def __str__(self) -> str:
+        """Returns the value representation of the enum in the form of a string."""
         return str(self.value)
 
 
 class _BaseVolume(_ModelVolumeMount):
+    """Base volume representation."""
+
     name: Optional[str] = None  # type: ignore
     mount_path: Optional[str] = None  # type: ignore
 
     @validator("name", pre=True)
     def _check_name(cls, v):
         if v is None:
             return str(uuid.uuid4())
@@ -99,24 +103,28 @@
             read_only=self.read_only,
             sub_path=self.sub_path,
             sub_path_expr=self.sub_path_expr,
         )
 
 
 class AWSElasticBlockStoreVolumeVolume(_BaseVolume, _ModelAWSElasticBlockStoreVolumeSource):
+    """Representation of AWS elastic block store volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             aws_elastic_block_store=_ModelAWSElasticBlockStoreVolumeSource(
                 fs_type=self.fs_type, partition=self.partition, read_only=self.read_only, volume_id=self.volume_id
             ),
         )
 
 
 class AzureDiskVolumeVolume(_BaseVolume, _ModelAzureDiskVolumeSource):
+    """Representation of an Azure disk volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             azure_disk=_ModelAzureDiskVolumeSource(
                 caching_mode=self.caching_mode,
                 disk_name=self.disk_name,
                 disk_uri=self.disk_uri,
@@ -124,24 +132,28 @@
                 kind=self.kind,
                 read_only=self.read_only,
             ),
         )
 
 
 class AzureFileVolumeVolume(_BaseVolume, _ModelAzureFileVolumeSource):
+    """Representation of an Azure file that can be mounted as a volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             azure_file=_ModelAzureFileVolumeSource(
                 read_only=self.read_only, secret_name=self.secret_name, share_name=self.share_name
             ),
         )
 
 
 class CephFSVolumeVolume(_BaseVolume, _ModelCephFSVolumeSource):
+    """Representation of a Ceph file system volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             cephfs=_ModelCephFSVolumeSource(
                 monitors=self.monitors,
                 path=self.path,
                 read_only=self.read_only,
@@ -149,142 +161,174 @@
                 secret_ref=self.secret_ref,
                 user=self.user,
             ),
         )
 
 
 class CinderVolume(_BaseVolume, _ModelCinderVolumeSource):
+    """Representation of a Cinder volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             cinder=_ModelCinderVolumeSource(
                 fs_type=self.fs_type,
                 read_only=self.read_only,
                 secret_ref=self.secret_ref,
                 volume_id=self.volume_id,
             ),
         )
 
 
 class ConfigMapVolume(_BaseVolume, _ModelConfigMapVolumeSource):  # type: ignore
+    """Representation of a config map volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             config_map=_ModelConfigMapVolumeSource(
                 default_mode=self.default_mode, items=self.items, name=self.name, optional=self.optional
             ),
         )
 
 
 class CSIVolume(_BaseVolume, _ModelCSIVolumeSource):
+    """Representation of a container service interface volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             csi=_ModelCSIVolumeSource(
                 driver=self.driver,
                 fs_type=self.fs_type,
                 node_publish_secret_ref=self.node_publish_secret_ref,
                 read_only=self.read_only,
                 volume_attributes=self.volume_attributes,
             ),
         )
 
 
 class DownwardAPIVolume(_BaseVolume, _ModelDownwardAPIVolumeSource):
+    """Representation of a volume passed via the downward API."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             downward_api=_ModelDownwardAPIVolumeSource(default_mode=self.default_mode, items=self.items),
         )
 
 
 class EmptyDirVolume(_BaseVolume, _ModelEmptyDirVolumeSource):
+    """Representation of an empty dir volume from K8s."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name, empty_dir=_ModelEmptyDirVolumeSource(medium=self.medium, size_limit=self.size_limit)
         )
 
 
 class EphemeralVolume(_BaseVolume, _ModelEphemeralVolumeSource):
+    """Representation of a volume that uses ephemeral storage shared with the K8s node a pod is scheduled on."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name, ephemeral=_ModelEphemeralVolumeSource(volume_claim_template=self.volume_claim_template)
         )
 
 
 class FCVolume(_BaseVolume, _ModelFCVolumeSource):
+    """An FV volume representation."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             fc=_ModelFCVolumeSource(
                 fs_type=self.fs_type,
                 lun=self.lun,
                 read_only=self.read_only,
                 target_ww_ns=self.target_ww_ns,
                 wwids=self.wwids,
             ),
         )
 
 
 class FlexVolume(_BaseVolume, _ModelFlexVolumeSource):
+    """A Flex volume representation."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             flex_volume=_ModelFlexVolumeSource(
                 driver=self.driver,
                 fs_type=self.fs_type,
                 options=self.options,
                 read_only=self.read_only,
                 secret_ref=self.secret_ref,
             ),
         )
 
 
 class FlockerVolume(_BaseVolume, _ModelFlockerVolumeSource):
+    """A Flocker volume representation."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             flocker=_ModelFlockerVolumeSource(dataset_name=self.dataset_name, dataset_uuid=self.dataset_uuid),
         )
 
 
 class GCEPersistentDiskVolume(_BaseVolume, _ModelGCEPersistentDiskVolumeSource):
+    """A representation of a Google Cloud Compute Enginer persistent disk.
+
+    Notes:
+        The volume must exist on GCE before a request to mount it to a pod is performed.
+    """
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             gce_persistent_disk=_ModelGCEPersistentDiskVolumeSource(
                 fs_type=self.fs_type, partition=self.partition, pd_name=self.pd_name, read_only=self.read_only
             ),
         )
 
 
 class GitRepoVolume(_BaseVolume, _ModelGitRepoVolumeSource):
+    """A representation of a Git repo that can be mounted as a volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             git_repo=_ModelGitRepoVolumeSource(
                 directory=self.directory, repository=self.repository, revision=self.revision
             ),
         )
 
 
 class GlusterfsVolume(_BaseVolume, _ModelGlusterfsVolumeSource):
+    """A representation for a Gluster filesystem volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             glusterfs=_ModelGlusterfsVolumeSource(endpoints=self.endpoints, path=self.path, read_only=self.read_only),
         )
 
 
 class HostPathVolume(_BaseVolume, _ModelHostPathVolumeSource):
+    """Representation for a volume that can be mounted from a host path/node location."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(name=self.name, host_path=_ModelHostPathVolumeSource(path=self.path, type=self.type))
 
 
 class ISCSIVolume(_BaseVolume, _ModelISCSIVolumeSource):
+    """Representation of ISCSI volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             iscsi=_ModelISCSIVolumeSource(
                 chap_auth_discovery=self.chap_auth_discovery,
                 chap_auth_session=self.chap_auth_discovery,
                 fs_type=self.fs_type,
@@ -297,44 +341,54 @@
                 secret_ref=self.secret_ref,
                 target_portal=self.target_portal,
             ),
         )
 
 
 class NFSVolume(_BaseVolume, _ModelNFSVolumeSource):
+    """A network file system volume representation."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(name=self.name, nfs=_ModelNFSVolumeSource(path=self.path, read_only=self.read_only))
 
 
 class PhotonPersistentDiskVolume(_BaseVolume, _ModelPhotonPersistentDiskVolumeSource):
+    """A Photon Persisten Disk representation."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             photon_persistent_disk=_ModelPhotonPersistentDiskVolumeSource(fs_type=self.fs_type, pd_id=self.pd_id),
         )
 
 
 class PortworxVolume(_BaseVolume, _ModelPortworxVolumeSource):
+    """`PortworxVolume` represents a Portworx volume to mount to a container."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             portworx_volume=_ModelPortworxVolumeSource(
                 fs_type=self.fs_type, read_only=self.read_only, volume_id=self.volume_id
             ),
         )
 
 
 class ProjectedVolume(_BaseVolume, _ModelProjectedVolumeSource):
+    """`ProjectedVolume` represents a projected volume to mount to a container."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name, projected=_ModelProjectedVolumeSource(default_mode=self.default_mode, sources=self.sources)
         )
 
 
 class QuobyteVolume(_BaseVolume, _ModelQuobyteVolumeSource):
+    """`QuobyteVolume` represents a Quobyte volume to mount to a container."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             quobyte=_ModelQuobyteVolumeSource(
                 group=self.group,
                 read_only=self.read_only,
                 registry=self.registry,
@@ -342,14 +396,16 @@
                 user=self.user,
                 volume=self.volume,
             ),
         )
 
 
 class RBDVolume(_BaseVolume, _ModelRBDVolumeSource):
+    """An RDB volume representation."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             rbd=_ModelRBDVolumeSource(
                 fs_type=self.fs_type,
                 image=self.image,
                 keyring=self.keyring,
@@ -359,14 +415,16 @@
                 secret_ref=self.secret_ref,
                 user=self.user,
             ),
         )
 
 
 class ScaleIOVolume(_BaseVolume, _ModelScaleIOVolumeSource):
+    """`ScaleIOVolume` represents a ScaleIO volume to mount to the container."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             scale_io=_ModelScaleIOVolumeSource(
                 fs_type=self.fs_type,
                 gateway=self.gateway,
                 protection_domain=self.protection_domain,
@@ -378,61 +436,80 @@
                 system=self.system,
                 volume_name=self.volume_name,
             ),
         )
 
 
 class SecretVolume(_BaseVolume, _ModelSecretVolumeSource):
+    """`SecretVolume` supports mounting a K8s secret as a container volume."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             secret=_ModelSecretVolumeSource(
                 default_mode=self.default_mode, items=self.items, optional=self.optional, secret_name=self.secret_name
             ),
         )
 
 
 class StorageOSVolume(_BaseVolume, _ModelStorageOSVolumeSource):
+    """`StorageOSVolume` represents a Storage OS volume to mount."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             storageos=_ModelStorageOSVolumeSource(
                 fs_type=self.fs_type,
                 read_only=self.read_only,
                 secret_ref=self.secret_ref,
                 volume_name=self.volume_name,
                 volume_namespace=self.volume_namespace,
             ),
         )
 
 
 class VsphereVirtualDiskVolume(_BaseVolume, _ModelVsphereVirtualDiskVolumeSource):
+    """`VsphereVirtualDiskVolume` represents a vSphere virtual disk volume to mount."""
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             vsphere_volume=_ModelVsphereVirtualDiskVolumeSource(
                 fs_type=self.fs_type,
                 storage_policy_id=self.storage_policy_id,
                 storage_policy_name=self.storage_policy_name,
                 volume_path=self.volume_path,
             ),
         )
 
 
 class ExistingVolume(_BaseVolume, _ModelPersistentVolumeClaimVolumeSource):
+    """`ExistingVolume` is a representation of an existing volume in K8s.
+
+    The existing volume is mounted based on the supplied claim name. This tells K8s that the specified persistent
+    volume claim should be used to mount a volume to a pod.
+    """
+
     def _build_volume(self) -> _ModelVolume:
         return _ModelVolume(
             name=self.name,
             persistent_volume_claim=_ModelPersistentVolumeClaimVolumeSource(
                 claim_name=self.claim_name, read_only=self.read_only
             ),
         )
 
 
 class Volume(_BaseVolume, _ModelPersistentVolumeClaimSpec):
+    """Volume represents a basic, dynamic, volume representation.
+
+    This `Volume` cannot only be instantiated to be used for mounting purposes but also for dynamically privisioning
+    volumes in K8s. When the volume is used a corresponding persistent volume claim is also created on workflow
+    submission.
+    """
+
     size: Optional[str] = None  # type: ignore
     resources: Optional[ResourceRequirements] = None
     metadata: Optional[ObjectMeta] = None
     access_modes: Optional[List[Union[str, AccessMode]]] = [AccessMode.read_write_once]  # type: ignore
     storage_class_name: Optional[str] = None
 
     @validator("access_modes", pre=True, always=True)
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/workflow.py` & `hera_workflows-5.6.0/src/hera/workflows/workflow.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-"""The workflow module provides the Workflow class
+"""The workflow module provides the Workflow class.
 
 See https://argoproj.github.io/argo-workflows/workflow-concepts/#the-workflow
 for more on Workflows.
 """
 import time
 from pathlib import Path
 from types import ModuleType
@@ -71,15 +71,15 @@
 try:
     import yaml
 
     _yaml = yaml
 except ImportError:
     _yaml = None
 
-ImagePullSecrets = Optional[Union[LocalObjectReference, List[LocalObjectReference], str, List[str]]]
+ImagePullSecretsT = Optional[Union[LocalObjectReference, List[LocalObjectReference], str, List[str]]]
 
 NAME_LIMIT = 63
 
 # The length of the random suffix used for generate_name
 # length (5) from https://github.com/kubernetes/kubernetes/blob/6195f96e/staging/src/k8s.io/apiserver/pkg/storage/names/generate.go#L45
 _SUFFIX_LEN = 5
 # The max name length comes from https://github.com/kubernetes/kubernetes/blob/6195f96e/staging/src/k8s.io/apiserver/pkg/storage/names/generate.go#L44
@@ -207,15 +207,15 @@
     dns_config: Annotated[Optional[PodDNSConfig], _WorkflowModelMapper("spec.dns_config")] = None
     dns_policy: Annotated[Optional[str], _WorkflowModelMapper("spec.dns_policy")] = None
     entrypoint: Annotated[Optional[str], _WorkflowModelMapper("spec.entrypoint")] = None
     executor: Annotated[Optional[ExecutorConfig], _WorkflowModelMapper("spec.executor")] = None
     hooks: Annotated[Optional[Dict[str, LifecycleHook]], _WorkflowModelMapper("spec.hooks")] = None
     host_aliases: Annotated[Optional[List[HostAlias]], _WorkflowModelMapper("spec.host_aliases")] = None
     host_network: Annotated[Optional[bool], _WorkflowModelMapper("spec.host_network")] = None
-    image_pull_secrets: Annotated[ImagePullSecrets, _WorkflowModelMapper("spec.image_pull_secrets")] = None
+    image_pull_secrets: Annotated[ImagePullSecretsT, _WorkflowModelMapper("spec.image_pull_secrets")] = None
     node_selector: Annotated[Optional[Dict[str, str]], _WorkflowModelMapper("spec.node_selector")] = None
     on_exit: Annotated[Optional[Union[str, Templatable]], _WorkflowModelMapper("spec.on_exit", _build_on_exit)] = None
     parallelism: Annotated[Optional[int], _WorkflowModelMapper("spec.parallelism")] = None
     pod_disruption_budget: Annotated[
         Optional[PodDisruptionBudgetSpec], _WorkflowModelMapper("spec.pod_disruption_budget")
     ] = None
     pod_gc: Annotated[Optional[PodGC], _WorkflowModelMapper("spec.pod_gc")] = None
@@ -322,14 +322,15 @@
             if isinstance(secret, str):
                 result.append(LocalObjectReference(name=secret))
             elif isinstance(secret, LocalObjectReference):
                 result.append(secret)
         return result
 
     def get_parameter(self, name: str) -> Parameter:
+        """Attempts to find and return a `Parameter` of the specified name."""
         arguments = self._build_arguments()
         if arguments is None:
             raise KeyError("Workflow has no arguments set")
         if arguments.parameters is None:
             raise KeyError("Workflow has no argument parameters set")
 
         parameters = arguments.parameters
@@ -348,14 +349,15 @@
         return _WorkflowModelMapper.build_model(Workflow, self, model_workflow)
 
     def to_dict(self) -> Any:
         """Builds the Workflow as an Argo schema Workflow object and returns it as a dictionary."""
         return self.build().dict(exclude_none=True, by_alias=True)
 
     def __eq__(self, other) -> bool:
+        """Verifies equality of `self` with the specified `other`."""
         if other.__class__ is self.__class__:
             return self.to_dict() == other.to_dict()
 
         return False
 
     def to_yaml(self, *args, **kwargs) -> str:
         """Builds the Workflow as an Argo schema Workflow object and returns it as yaml string."""
@@ -424,27 +426,26 @@
         assert self.workflows_service, "workflow service not initialized"
         assert self.namespace, "workflow namespace not defined"
         return self.workflows_service.lint_workflow(
             WorkflowLintRequest(workflow=self.build()), namespace=self.namespace
         )
 
     def _add_sub(self, node: Any):
-        """Adds any objects instantiated under the Workflow context manager that conform to the `Templatable` protocol
-        or are Argo schema Template objects to the Workflow's list of templates.
-        """
+        """Adds the given node (expected to satisfy the `Templatable` protocol) to the context."""
         if not isinstance(node, (Templatable, _ModelTemplate)):
             raise InvalidType(type(node))
         self.templates.append(node)
 
     def to_file(self, output_directory: Union[Path, str] = ".", name: str = "", *args, **kwargs) -> Path:
         """Writes the Workflow as an Argo schema Workflow object to a YAML file and returns the path to the file.
 
         Args:
             output_directory: The directory to write the file to. Defaults to the current working directory.
-            name: The name of the file to write without the file extension. Defaults to the Workflow's name or a generated name.
+            name: The name of the file to write without the file extension.  Defaults to the Workflow's name or a
+            generated name.
             *args: Additional arguments to pass to `yaml.dump`.
             **kwargs: Additional keyword arguments to pass to `yaml.dump`.
         """
         workflow_name = self.name or (self.generate_name or "workflow").rstrip("-")
         name = name or workflow_name
         output_directory = Path(output_directory)
         output_path = Path(output_directory) / f"{name}.yaml"
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/workflow_status.py` & `hera_workflows-5.6.0/src/hera/workflows/workflow_status.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,25 +1,27 @@
+"""Workflow status is a module that provides functionality for representing and interacting with workflow status."""
 from enum import Enum
 
 
 class WorkflowStatus(str, Enum):
-    """Placeholder for workflow statuses"""
+    """Placeholder for workflow statuses."""
 
     running = "Running"
     succeeded = "Succeeded"
     failed = "Failed"
     error = "Error"
     terminated = "Terminated"
 
-    def __str__(self):
+    def __str__(self) -> str:
+        """Returns the value representation of the workflow status enum."""
         return str(self.value)
 
     @classmethod
     def from_argo_status(cls, s: str) -> "WorkflowStatus":
-        """Turns an Argo status into a Hera workflow status representation"""
+        """Turns an Argo status into a Hera workflow status representation."""
         switch = {
             "Running": WorkflowStatus.running,
             "Succeeded": WorkflowStatus.succeeded,
             "Failed": WorkflowStatus.failed,
             "Error": WorkflowStatus.error,
             "Terminated": WorkflowStatus.terminated,
         }
```

### Comparing `hera_workflows-5.5.2/src/hera/workflows/workflow_template.py` & `hera_workflows-5.6.0/src/hera/workflows/workflow_template.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-"""The workflow_template module provides the WorkflowTemplate class
+"""The workflow_template module provides the WorkflowTemplate class.
 
 See https://argoproj.github.io/argo-workflows/workflow-templates/
 for more on WorkflowTemplates.
 """
 from pathlib import Path
 from typing import Dict, Optional, Type, Union, cast
 
@@ -33,14 +33,15 @@
     @classmethod
     def _get_model_class(cls) -> Type[BaseModel]:
         return _ModelWorkflowTemplate  # type: ignore
 
 
 class WorkflowTemplate(Workflow):
     """WorkflowTemplates are definitions of Workflows that live in your namespace in your cluster.
+
     This allows you to create a library of frequently-used templates and reuse them by referencing
     them from your Workflows.
     """
 
     # Removes status mapping
     status: Annotated[Optional[_ModelWorkflowStatus], _WorkflowTemplateModelMapper("")] = None
 
@@ -56,24 +57,24 @@
         assert self.workflows_service, "workflow service not initialized"
         assert self.namespace, "workflow namespace not defined"
         return self.workflows_service.create_workflow_template(
             WorkflowTemplateCreateRequest(template=self.build()), namespace=self.namespace
         )
 
     def get(self) -> TWorkflow:
-        """Attempts to get a workflow template based on the parameters of this template e.g. name + namespace"""
+        """Attempts to get a workflow template based on the parameters of this template e.g. name + namespace."""
         assert self.workflows_service, "workflow service not initialized"
         assert self.namespace, "workflow namespace not defined"
         assert self.name, "workflow name not defined"
         return self.workflows_service.get_workflow_template(name=self.name, namespace=self.namespace)
 
     def update(self) -> TWorkflow:
-        """
-        Attempts to perform a workflow template update based on the parameters of this template
-        e.g. name, namespace. Note that this creates the template if it does not exist. In addition, this performs
+        """Attempts to perform a template update based on the parameters of this template.
+
+        This creates the template if it does not exist. In addition, this performs
         a get prior to updating to get the resource version to update in the first place. If you know the template
         does not exist ahead of time, it is more efficient to use `create()` directly to avoid one round trip.
         """
         assert self.workflows_service, "workflow service not initialized"
         assert self.namespace, "workflow namespace not defined"
         assert self.name, "workflow name not defined"
         # we always need to do a get prior to updating to get the resource version to update in the first place
@@ -165,13 +166,12 @@
 
         If generate_name is given, the workflow created uses generate_name as a prefix, as per the usual for
         hera.workflows.Workflow.generate_name. If not given, the WorkflowTemplate's name will be used, truncated to 57
         chars and appended with a hyphen.
 
         Note: this function does not require the WorkflowTemplate to already exist on the cluster
         """
-
         workflow = self._get_as_workflow(generate_name)
         return workflow.create(wait=wait, poll_interval=poll_interval)
 
 
 __all__ = ["WorkflowTemplate"]
```

### Comparing `hera_workflows-5.5.2/PKG-INFO` & `hera_workflows-5.6.0/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: hera-workflows
-Version: 5.5.2
+Version: 5.6.0
 Summary: Hera is a Python framework for constructing and submitting Argo Workflows. The main goal of Hera is to make Argo Workflows more accessible by abstracting away some setup that is typically necessary for constructing Argo workflows.
 Home-page: https://github.com/argoproj-labs/hera
 License: MIT
 Author: Flaviu Vadan
 Author-email: flaviu.vadan@dynotx.com
 Maintainer: Flaviu Vadan
 Maintainer-email: flaviu.vadan@dynotx.com
@@ -61,26 +61,24 @@
 [![Downloads](https://pepy.tech/badge/hera-workflows)](https://pepy.tech/project/hera-workflows)
 [![Downloads/month](https://pepy.tech/badge/hera-workflows/month)](https://pepy.tech/project/hera-workflows)
 [![Downloads/week](https://pepy.tech/badge/hera-workflows/week)](https://pepy.tech/project/hera-workflows)
 
 Hera is a Python framework for constructing and submitting Argo Workflows. The main goal of Hera is to make the Argo
 ecosystem accessible by simplifying workflow construction and submission.
 
-You can watch the introductory Hera presentation at the "Argo Workflows and Events Community Meeting 20 Oct
-2021" [here](https://www.youtube.com/watch?v=QETfzfVV-GY&t=181s)!
-
 # Table of content
 
 - [Hera](#hera)
 - [Table of content](#table-of-content)
 - [Requirements](#requirements)
 - [Installation](#installation)
 - [Examples](#examples)
     - [Single step script](#single-step-script)
     - [DAG diamond](#dag-diamond)
+- [Presentations](#presentations)
 - [Contributing](#contributing)
 - [Comparison](#comparison)
 
 # Requirements
 
 Hera requires an Argo server to be deployed to a Kubernetes cluster. Currently, Hera assumes that the Argo server sits
 behind an authentication layer that can authenticate workflow submission requests by using the Bearer token on the
@@ -167,14 +165,26 @@
         A >> [B, C] >> D
 
 w.create()
 ```
 
 See the [examples](./examples/) directory for a collection of Argo workflow construction and submission via Hera!
 
+# Presentations
+
+- [Argo Workflows and Events Community Meeting 20 Oct 2021 - Hera introductory presentation](https://youtu.be/QETfzfVV-GY?t=181)
+- [Argo Workflows and Events Community Meeting 15 June 2022 - Hera project update](https://youtu.be/sdkBDPOdQ-g?t=231)
+- [KubeCon/ArgoCon EU 2023 - Scaling gene therapy with Argo Workflows and Hera](https://www.youtube.com/watch?v=h2TEw8kd1Ds)
+- [Unsticking ourselves from Glue - Migrating PayIt's Data Pipelines to Argo Workflows and Hera](https://youtu.be/sSLFVIIEKcE?t=2088)
+
+# Blogs
+
+- [Hera introduction and motivation](https://www.dynotx.com/hera-the-missing-argo-workflows-python-sdk/)
+- [Dyno is scaling gene therapy research with cloud-native tools like Argo Workflows and Hera](https://www.dynotx.com/argo-workflows-hera/)
+
 # Contributing
 
 If you plan to submit contributions to Hera you can install Hera in a virtual environment managed by `poetry`:
 
 ```shell
 poetry install
 ```
```

